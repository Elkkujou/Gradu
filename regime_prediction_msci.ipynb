{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPHLU55IJpm3iPWFKX2cwWZ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jPPizlV3oIUh",
        "outputId": "23060c3e-0781-4133-808e-228419b78046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Gradu'...\n",
            "remote: Enumerating objects: 294, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 294 (delta 52), reused 43 (delta 43), pack-reused 232 (from 2)\u001b[K\n",
            "Receiving objects: 100% (294/294), 38.47 MiB | 22.27 MiB/s, done.\n",
            "Resolving deltas: 100% (133/133), done.\n",
            "/content/Gradu\n",
            " chatti_RF.ipynb\t       MSCI_XGBOOST.ipynb\t\t    RF_Gradu.ipynb\n",
            " Fama_french_XGBOOST.ipynb     Regiimi_prediction.ipynb\t\t   'RF REGIIMI HYVÃ„ TRAINING.ipynb'\n",
            "'Financial turbulence.ipynb'   regime_prediction_famafrench.ipynb  'RF_regime (3).ipynb'\n",
            " FT_source.xlsx\t\t       regime_prediction_msci.ipynb\t    THE_ONE.xlsx\n",
            " Gradient_boost_malli.ipynb    regime_pred.txt\n"
          ]
        }
      ],
      "source": [
        "%cd /content\n",
        "!rm -rf Gradu\n",
        "!git clone https://github.com/Elkkujou/Gradu.git\n",
        "%cd /content/Gradu\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "calc_drawdowns = True\n",
        "drawdowns_from = 'USA MOMENTUM Standard (Large+Mid Cap)'\n",
        "cluster_n = 2\n",
        "\n",
        "normmalize = True\n",
        "ML_reg = True\n",
        "ML_long = False\n",
        "DL = False"
      ],
      "metadata": {
        "id": "nQWZcw65odSb"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from tabulate import tabulate\n",
        "\n",
        "from IPython.display import display, HTML\n",
        "from sklearn.cluster import KMeans"
      ],
      "metadata": {
        "id": "FhY_-l5aofPM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_ONE.xlsx\")\n",
        "excel_df = xls_file.parse(\"returns non-log\")\n",
        "\n",
        "excel_df.columns = excel_df.columns.get_level_values(0)\n",
        "excel_df['Date'] = pd.to_datetime(excel_df['Date'])\n",
        "excel_df.sort_values('Date', inplace=True)\n",
        "\n",
        "print(\"Headers in the 'returns non-log' sheet:\")\n",
        "print(excel_df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vS4jmrFtoiJa",
        "outputId": "ea182230-fb6b-463d-cf75-7250f73dc9a4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers in the 'returns non-log' sheet:\n",
            "Index(['Date', 'USA Standard (Large+Mid Cap)',\n",
            "       'USA MOMENTUM Standard (Large+Mid Cap)',\n",
            "       'USA MINIMUM VOLATILITY (USD) Standard (Large+Mid Cap)',\n",
            "       'USA RISK WEIGHTED Standard (Large+Mid Cap)',\n",
            "       'USA SECTOR NEUTRAL QUALITY Standard (Large+Mid Cap)',\n",
            "       'USA ENHANCED VALUE Standard (Large+Mid Cap)', 'Winning Factor',\n",
            "       'VIXCLS', 'GDPC1', 'CPI', 'LEI', '10yr', 'Financial Turbulance',\n",
            "       '(Regime)', 'Financial condition', 'Drawdown', 'HY Spread', 'IG Spread',\n",
            "       '10y-2y', 'Vix-regimes2', 'Vix-regimes3', 'FT_regimes2', 'FT_regimes3',\n",
            "       'DD_regimes2', 'DD_regimes3', 'cluster_0 (minimal drawdown',\n",
            "       'cluster_1 (moderate drawdown)', 'cluster_2 (severe drawdown)', 'SMB',\n",
            "       'HML', 'RMW', 'CMA', 'MOM', '(Regimes)', 'Regimes'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if calc_drawdowns:\n",
        "    # Calculate the cumulative portfolio values using excel_df and the specified drawdowns_from column\n",
        "    portfolio = (1 + excel_df[drawdowns_from]).cumprod()\n",
        "\n",
        "    # Define a function to compute the maximum drawdown over a rolling window\n",
        "    def max_dd(s):\n",
        "        return (s / s.cummax() - 1).min()\n",
        "\n",
        "    # Calculate the 3-month rolling drawdown using the portfolio series\n",
        "    dd_series = portfolio.rolling(window=3, min_periods=3).apply(max_dd, raw=False)\n",
        "\n",
        "    # Create a temporary DataFrame to store dates and the calculated drawdowns\n",
        "    dd_temp = pd.DataFrame({\n",
        "        'Date': excel_df['Date'],\n",
        "        '3M_Max_Drawdown': dd_series\n",
        "    })\n",
        "\n",
        "    # Drop rows with missing drawdown data to ensure complete observations\n",
        "    dd_temp = dd_temp.dropna()\n",
        "\n",
        "    print(\"Drawdown calculation (temporary DataFrame):\")\n",
        "    print(dd_temp.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRsD7p1TQc6o",
        "outputId": "57b51c57-db90-4127-804b-6c1bd5fdf991"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drawdown calculation (temporary DataFrame):\n",
            "        Date  3M_Max_Drawdown\n",
            "2 1990-05-31        -0.014564\n",
            "3 1990-06-29         0.000000\n",
            "4 1990-07-31         0.000000\n",
            "5 1990-08-31        -0.086261\n",
            "6 1990-09-28        -0.128945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit KMeans clustering on the temporary DataFrame's '3M_Max_Drawdown' column\n",
        "kmeans = KMeans(n_clusters=cluster_n, random_state=42)\n",
        "dd_temp['DD_cluster'] = kmeans.fit_predict(dd_temp[['3M_Max_Drawdown']])\n",
        "\n",
        "# Merge the cluster labels from dd_temp into excel_df by matching on the 'Date' column.\n",
        "# This adds a new column 'DD_cluster' to excel_df.\n",
        "excel_df = excel_df.merge(dd_temp[['Date', 'DD_cluster']], on='Date', how='left')\n",
        "\n",
        "print(\"excel_df with DD_cluster added:\")\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(excel_df.head().to_html(index=False)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 868
        },
        "id": "ND-2VF-SS187",
        "outputId": "f6e95fd2-383b-48a3-9111-c73ca050ca50"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "excel_df with DD_cluster added:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-ae5c710a1ef9>:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  dd_temp['DD_cluster'] = kmeans.fit_predict(dd_temp[['3M_Max_Drawdown']])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Date</th>\n",
              "      <th>USA Standard (Large+Mid Cap)</th>\n",
              "      <th>USA MOMENTUM Standard (Large+Mid Cap)</th>\n",
              "      <th>USA MINIMUM VOLATILITY (USD) Standard (Large+Mid Cap)</th>\n",
              "      <th>USA RISK WEIGHTED Standard (Large+Mid Cap)</th>\n",
              "      <th>USA SECTOR NEUTRAL QUALITY Standard (Large+Mid Cap)</th>\n",
              "      <th>USA ENHANCED VALUE Standard (Large+Mid Cap)</th>\n",
              "      <th>Winning Factor</th>\n",
              "      <th>VIXCLS</th>\n",
              "      <th>GDPC1</th>\n",
              "      <th>CPI</th>\n",
              "      <th>LEI</th>\n",
              "      <th>10yr</th>\n",
              "      <th>Financial Turbulance</th>\n",
              "      <th>(Regime)</th>\n",
              "      <th>Financial condition</th>\n",
              "      <th>Drawdown</th>\n",
              "      <th>HY Spread</th>\n",
              "      <th>IG Spread</th>\n",
              "      <th>10y-2y</th>\n",
              "      <th>Vix-regimes2</th>\n",
              "      <th>Vix-regimes3</th>\n",
              "      <th>FT_regimes2</th>\n",
              "      <th>FT_regimes3</th>\n",
              "      <th>DD_regimes2</th>\n",
              "      <th>DD_regimes3</th>\n",
              "      <th>cluster_0 (minimal drawdown</th>\n",
              "      <th>cluster_1 (moderate drawdown)</th>\n",
              "      <th>cluster_2 (severe drawdown)</th>\n",
              "      <th>SMB</th>\n",
              "      <th>HML</th>\n",
              "      <th>RMW</th>\n",
              "      <th>CMA</th>\n",
              "      <th>MOM</th>\n",
              "      <th>(Regimes)</th>\n",
              "      <th>Regimes</th>\n",
              "      <th>DD_cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1990-03-30</td>\n",
              "      <td>0.020702</td>\n",
              "      <td>0.028022</td>\n",
              "      <td>0.012313</td>\n",
              "      <td>0.013039</td>\n",
              "      <td>0.036524</td>\n",
              "      <td>0.013845</td>\n",
              "      <td>USA SECTOR NEUTRAL QUALITY Standard (Large+Mid Cap)</td>\n",
              "      <td>21.40</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.004687</td>\n",
              "      <td>63.5</td>\n",
              "      <td>8.59</td>\n",
              "      <td>9.383220</td>\n",
              "      <td>Recovery</td>\n",
              "      <td>0.04713</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.08</td>\n",
              "      <td>High</td>\n",
              "      <td>Low</td>\n",
              "      <td>Normal Turbulence</td>\n",
              "      <td>Low Turbulence</td>\n",
              "      <td>Normal Drawdown</td>\n",
              "      <td>Moderate Drawdown</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0162</td>\n",
              "      <td>-0.0292</td>\n",
              "      <td>0.0204</td>\n",
              "      <td>-0.0102</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990-04-30</td>\n",
              "      <td>-0.023292</td>\n",
              "      <td>-0.014564</td>\n",
              "      <td>-0.031345</td>\n",
              "      <td>-0.042294</td>\n",
              "      <td>-0.007806</td>\n",
              "      <td>-0.032085</td>\n",
              "      <td>USA SECTOR NEUTRAL QUALITY Standard (Large+Mid Cap)</td>\n",
              "      <td>18.10</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>63.6</td>\n",
              "      <td>8.79</td>\n",
              "      <td>4.756642</td>\n",
              "      <td>Slowdown</td>\n",
              "      <td>-0.13602</td>\n",
              "      <td>-0.023292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.10</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Normal Turbulence</td>\n",
              "      <td>Low Turbulence</td>\n",
              "      <td>Normal Drawdown</td>\n",
              "      <td>Moderate Drawdown</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>-0.0259</td>\n",
              "      <td>0.0178</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>0.0239</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990-05-31</td>\n",
              "      <td>0.089444</td>\n",
              "      <td>0.112070</td>\n",
              "      <td>0.079487</td>\n",
              "      <td>0.080355</td>\n",
              "      <td>0.101743</td>\n",
              "      <td>0.081134</td>\n",
              "      <td>USA MOMENTUM Standard (Large+Mid Cap)</td>\n",
              "      <td>16.82</td>\n",
              "      <td>0.003630</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>63.3</td>\n",
              "      <td>8.76</td>\n",
              "      <td>14.686352</td>\n",
              "      <td>Slowdown</td>\n",
              "      <td>-0.15632</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.19</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Normal Turbulence</td>\n",
              "      <td>High Turbulence</td>\n",
              "      <td>Normal Drawdown</td>\n",
              "      <td>Moderate Drawdown</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0235</td>\n",
              "      <td>-0.0383</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>-0.0168</td>\n",
              "      <td>0.0303</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990-06-29</td>\n",
              "      <td>-0.006432</td>\n",
              "      <td>0.022340</td>\n",
              "      <td>-0.010553</td>\n",
              "      <td>-0.010138</td>\n",
              "      <td>0.011909</td>\n",
              "      <td>-0.016679</td>\n",
              "      <td>USA MOMENTUM Standard (Large+Mid Cap)</td>\n",
              "      <td>18.39</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.006197</td>\n",
              "      <td>63.3</td>\n",
              "      <td>8.48</td>\n",
              "      <td>8.020190</td>\n",
              "      <td>Slowdown</td>\n",
              "      <td>0.08472</td>\n",
              "      <td>-0.006432</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.45</td>\n",
              "      <td>High</td>\n",
              "      <td>High</td>\n",
              "      <td>Normal Turbulence</td>\n",
              "      <td>Low Turbulence</td>\n",
              "      <td>Normal Drawdown</td>\n",
              "      <td>Moderate Drawdown</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>-0.0193</td>\n",
              "      <td>-0.0103</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1990-07-31</td>\n",
              "      <td>-0.004276</td>\n",
              "      <td>0.004471</td>\n",
              "      <td>-0.001134</td>\n",
              "      <td>-0.011361</td>\n",
              "      <td>-0.000470</td>\n",
              "      <td>-0.008691</td>\n",
              "      <td>USA MOMENTUM Standard (Large+Mid Cap)</td>\n",
              "      <td>28.18</td>\n",
              "      <td>0.000666</td>\n",
              "      <td>0.004619</td>\n",
              "      <td>63.3</td>\n",
              "      <td>8.47</td>\n",
              "      <td>25.637526</td>\n",
              "      <td>Slowdown</td>\n",
              "      <td>0.24339</td>\n",
              "      <td>-0.010680</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.79</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Low</td>\n",
              "      <td>High Turbulence</td>\n",
              "      <td>High Turbulence</td>\n",
              "      <td>Normal Drawdown</td>\n",
              "      <td>Moderate Drawdown</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.0316</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>-0.0018</td>\n",
              "      <td>0.0321</td>\n",
              "      <td>0.0610</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Normal</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#FEATURES = ['VIXCLS', 'CPI', 'LEI', '10yr', 'Financial condition', '10y-2y', 'Financial Turbulance']\n",
        "\n",
        "FEATURES = [\n",
        "    'VIXCLS',\n",
        "    'GDPC1',\n",
        "    'CPI',\n",
        "    'LEI',\n",
        "    '10yr',\n",
        "    'Financial Turbulance',\n",
        "    'Financial condition',\n",
        "    'HY Spread',\n",
        "    'IG Spread',\n",
        "    '10y-2y'\n",
        "]\n",
        "\n",
        "\n",
        "TRUE_REGIME = 'DD_cluster'\n",
        "DATE = 'Date'\n",
        "\n",
        "selected_columns =  [DATE] + [TRUE_REGIME] +FEATURES\n"
      ],
      "metadata": {
        "id": "r9Cg8WsZqU34"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing = [col for col in selected_columns if col not in excel_df.columns]\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns in df: {missing}\")\n",
        "\n",
        "# Subset and drop rows with empty values\n",
        "input_df = excel_df[selected_columns]\n",
        "orig_rows = len(input_df)\n",
        "input_df = input_df.dropna()\n",
        "print(f\"Original rows: {orig_rows}, Dropped rows: {orig_rows - len(input_df)}\")\n",
        "print(f\"First obs: {input_df[DATE].min()}, Last obs: {input_df[DATE].max()}\")\n",
        "\n",
        "# Display as neat HTML table (adjust 'rows_to_show' as needed)\n",
        "rows_to_show = 10\n",
        "from IPython.display import display, HTML\n",
        "display(HTML(input_df.head(rows_to_show).to_html(index=False)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "7yQYY08pMCVR",
        "outputId": "b16ebc21-4cf7-42b8-e068-8dddbff97187"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original rows: 416, Dropped rows: 81\n",
            "First obs: 1996-12-31 00:00:00, Last obs: 2024-10-31 00:00:00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Date</th>\n",
              "      <th>DD_cluster</th>\n",
              "      <th>VIXCLS</th>\n",
              "      <th>GDPC1</th>\n",
              "      <th>CPI</th>\n",
              "      <th>LEI</th>\n",
              "      <th>10yr</th>\n",
              "      <th>Financial Turbulance</th>\n",
              "      <th>Financial condition</th>\n",
              "      <th>HY Spread</th>\n",
              "      <th>IG Spread</th>\n",
              "      <th>10y-2y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1996-12-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>19.47</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.002520</td>\n",
              "      <td>77.7</td>\n",
              "      <td>6.30</td>\n",
              "      <td>7.860576</td>\n",
              "      <td>0.00043</td>\n",
              "      <td>3.04</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-01-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.14</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.001886</td>\n",
              "      <td>77.8</td>\n",
              "      <td>6.58</td>\n",
              "      <td>14.956959</td>\n",
              "      <td>-0.03500</td>\n",
              "      <td>2.73</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-02-28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.17</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>78.3</td>\n",
              "      <td>6.42</td>\n",
              "      <td>13.043063</td>\n",
              "      <td>0.05391</td>\n",
              "      <td>2.84</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.47</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-03-31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.66</td>\n",
              "      <td>0.016652</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>79.3</td>\n",
              "      <td>6.69</td>\n",
              "      <td>10.821978</td>\n",
              "      <td>0.05726</td>\n",
              "      <td>2.94</td>\n",
              "      <td>0.59</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-04-30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>19.92</td>\n",
              "      <td>0.016652</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>79.7</td>\n",
              "      <td>6.89</td>\n",
              "      <td>10.938933</td>\n",
              "      <td>0.00925</td>\n",
              "      <td>2.68</td>\n",
              "      <td>0.61</td>\n",
              "      <td>0.45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-05-30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.19</td>\n",
              "      <td>0.016652</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>79.9</td>\n",
              "      <td>6.71</td>\n",
              "      <td>4.038058</td>\n",
              "      <td>-0.01056</td>\n",
              "      <td>2.67</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.43</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-06-30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.53</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>0.001876</td>\n",
              "      <td>80.4</td>\n",
              "      <td>6.49</td>\n",
              "      <td>7.517614</td>\n",
              "      <td>-0.02150</td>\n",
              "      <td>2.71</td>\n",
              "      <td>0.56</td>\n",
              "      <td>0.28</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-07-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>23.08</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>0.001248</td>\n",
              "      <td>81.1</td>\n",
              "      <td>6.22</td>\n",
              "      <td>14.684361</td>\n",
              "      <td>0.01571</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.57</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-08-29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.81</td>\n",
              "      <td>0.012486</td>\n",
              "      <td>0.002494</td>\n",
              "      <td>82.0</td>\n",
              "      <td>6.30</td>\n",
              "      <td>10.252527</td>\n",
              "      <td>0.03394</td>\n",
              "      <td>2.59</td>\n",
              "      <td>0.54</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-09-30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>23.87</td>\n",
              "      <td>0.008539</td>\n",
              "      <td>0.002488</td>\n",
              "      <td>82.3</td>\n",
              "      <td>6.21</td>\n",
              "      <td>6.511195</td>\n",
              "      <td>0.10651</td>\n",
              "      <td>2.99</td>\n",
              "      <td>0.67</td>\n",
              "      <td>0.21</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jY0ai22WK_kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Calculate the number of observations for each regime in the data\n",
        "regime_counts = input_df[TRUE_REGIME].value_counts().sort_index().reset_index()\n",
        "regime_counts.columns = [TRUE_REGIME, 'Count']\n",
        "\n",
        "print(\"Number of observations per regime:\")\n",
        "print(tabulate(regime_counts, headers='keys', tablefmt='psql', showindex=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeP-z42p8ae2",
        "outputId": "423e61a9-4cbe-4c33-843c-b201096c599b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of observations per regime:\n",
            "+--------------+---------+\n",
            "|   DD_cluster |   Count |\n",
            "|--------------+---------|\n",
            "|            0 |     258 |\n",
            "|            1 |      77 |\n",
            "+--------------+---------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if normmalize == True:\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "    # Instantiate the scaler and apply it in-place to the FEATURES columns\n",
        "    scaler = StandardScaler()\n",
        "    input_df[FEATURES] = scaler.fit_transform(input_df[FEATURES])\n",
        "\n",
        "    print(\"Normalization applied. First 10 observations:\")\n",
        "    display(HTML(input_df.head(10).to_html(index=False)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "dlkbkE3YYiEG",
        "outputId": "eb9eec8c-81f4-4099-e627-9db8f2657f85"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalization applied. First 10 observations:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Date</th>\n",
              "      <th>DD_cluster</th>\n",
              "      <th>VIXCLS</th>\n",
              "      <th>GDPC1</th>\n",
              "      <th>CPI</th>\n",
              "      <th>LEI</th>\n",
              "      <th>10yr</th>\n",
              "      <th>Financial Turbulance</th>\n",
              "      <th>Financial condition</th>\n",
              "      <th>HY Spread</th>\n",
              "      <th>IG Spread</th>\n",
              "      <th>10y-2y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1996-12-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.110862</td>\n",
              "      <td>0.029919</td>\n",
              "      <td>0.158638</td>\n",
              "      <td>-1.496475</td>\n",
              "      <td>1.871825</td>\n",
              "      <td>-0.411545</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>-0.909269</td>\n",
              "      <td>-1.082181</td>\n",
              "      <td>-0.415568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-01-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.025812</td>\n",
              "      <td>0.029919</td>\n",
              "      <td>-0.058714</td>\n",
              "      <td>-1.487664</td>\n",
              "      <td>2.062500</td>\n",
              "      <td>0.235650</td>\n",
              "      <td>-0.253834</td>\n",
              "      <td>-1.032720</td>\n",
              "      <td>-1.106567</td>\n",
              "      <td>-0.539840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-02-28</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.022004</td>\n",
              "      <td>0.029919</td>\n",
              "      <td>-0.059929</td>\n",
              "      <td>-1.443610</td>\n",
              "      <td>1.953543</td>\n",
              "      <td>0.061101</td>\n",
              "      <td>0.383025</td>\n",
              "      <td>-0.988915</td>\n",
              "      <td>-1.130954</td>\n",
              "      <td>-0.539840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-03-31</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.086743</td>\n",
              "      <td>0.867842</td>\n",
              "      <td>-0.489887</td>\n",
              "      <td>-1.355504</td>\n",
              "      <td>2.137408</td>\n",
              "      <td>-0.141463</td>\n",
              "      <td>0.407021</td>\n",
              "      <td>-0.949092</td>\n",
              "      <td>-1.106567</td>\n",
              "      <td>-0.581264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-04-30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.053739</td>\n",
              "      <td>0.867842</td>\n",
              "      <td>-0.490021</td>\n",
              "      <td>-1.320261</td>\n",
              "      <td>2.273604</td>\n",
              "      <td>-0.130797</td>\n",
              "      <td>0.063127</td>\n",
              "      <td>-1.052631</td>\n",
              "      <td>-1.082181</td>\n",
              "      <td>-0.560552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-05-30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.019465</td>\n",
              "      <td>0.867842</td>\n",
              "      <td>-0.704261</td>\n",
              "      <td>-1.302640</td>\n",
              "      <td>2.151027</td>\n",
              "      <td>-0.760162</td>\n",
              "      <td>-0.078771</td>\n",
              "      <td>-1.056614</td>\n",
              "      <td>-1.118761</td>\n",
              "      <td>-0.581264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-06-30</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.023694</td>\n",
              "      <td>0.525492</td>\n",
              "      <td>-0.061944</td>\n",
              "      <td>-1.258586</td>\n",
              "      <td>2.001211</td>\n",
              "      <td>-0.442824</td>\n",
              "      <td>-0.157134</td>\n",
              "      <td>-1.040684</td>\n",
              "      <td>-1.143147</td>\n",
              "      <td>-0.736604</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-07-31</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.347390</td>\n",
              "      <td>0.525492</td>\n",
              "      <td>-0.276851</td>\n",
              "      <td>-1.196911</td>\n",
              "      <td>1.817346</td>\n",
              "      <td>0.210789</td>\n",
              "      <td>0.109400</td>\n",
              "      <td>-1.088472</td>\n",
              "      <td>-1.130954</td>\n",
              "      <td>-0.643400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-08-29</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.440056</td>\n",
              "      <td>0.525492</td>\n",
              "      <td>0.149492</td>\n",
              "      <td>-1.117615</td>\n",
              "      <td>1.871825</td>\n",
              "      <td>-0.193398</td>\n",
              "      <td>0.239981</td>\n",
              "      <td>-1.088472</td>\n",
              "      <td>-1.167534</td>\n",
              "      <td>-0.695180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1997-09-30</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.447673</td>\n",
              "      <td>0.201170</td>\n",
              "      <td>0.147369</td>\n",
              "      <td>-1.091183</td>\n",
              "      <td>1.810537</td>\n",
              "      <td>-0.534610</td>\n",
              "      <td>0.759796</td>\n",
              "      <td>-0.929180</td>\n",
              "      <td>-1.009021</td>\n",
              "      <td>-0.809096</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ML_reg == True:\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "    # Define predictors and target using the given column lists\n",
        "    predictors = FEATURES\n",
        "    target = TRUE_REGIME\n",
        "\n",
        "    # Drop rows with missing values in predictors or target\n",
        "    data = input_df.dropna(subset=predictors + [target]).reset_index(drop=True)\n",
        "    X = data[predictors]\n",
        "    y = data[target]\n",
        "    # Also grab the DATE column for printing prediction timestamps\n",
        "    dates = data[DATE]\n",
        "\n",
        "    # Define the base models with high parameters, now including Gradient Boosting\n",
        "    base_models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "        'GradientBoosting': GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, random_state=42),\n",
        "        'SVC': SVC(probability=True, C=1000, kernel='rbf', gamma='scale'),\n",
        "        'GaussianNB': GaussianNB(var_smoothing=1e-8)\n",
        "    }\n",
        "\n",
        "    # Define the meta-learner\n",
        "    meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    window_size = 60  # fixed window length once reached\n",
        "    min_data_pct = 0.2  # start base model predictions when available data >= 80% of window_size\n",
        "    base_threshold = int(window_size * min_data_pct)\n",
        "\n",
        "    # Initialize dictionaries/lists to store rolling accuracies and meta weights\n",
        "    rolling_accuracies_base = {name: [] for name in base_models}\n",
        "    rolling_accuracies_meta = []\n",
        "    meta_weights_list = []  # record meta-learner coefficients\n",
        "\n",
        "    # Initialize expanding meta training set (meta features and outcomes)\n",
        "    meta_train_features = []\n",
        "    meta_train_targets = []\n",
        "\n",
        "    print(\"Starting rolling window evaluation with stacking ensemble and expanding meta learner:\")\n",
        "\n",
        "    for i in range(len(X)):\n",
        "        # Do not make any prediction until we have at least base_threshold observations\n",
        "        if i < base_threshold:\n",
        "            print(f\"Skipping prediction for timestamp {dates.iloc[i]} \"\n",
        "                  f\"(insufficient data: {i} observations, need {base_threshold}).\")\n",
        "            continue\n",
        "\n",
        "        # For base models, if i is less than full window_size, use all data from start to i.\n",
        "        # Once i >= window_size, use a fixed rolling window of size window_size.\n",
        "        if i < window_size:\n",
        "            X_train_window = X.iloc[:i]\n",
        "            y_train_window = y.iloc[:i]\n",
        "        else:\n",
        "            X_train_window = X.iloc[i-window_size:i]\n",
        "            y_train_window = y.iloc[i-window_size:i]\n",
        "\n",
        "        X_test_window = X.iloc[[i]]\n",
        "        y_test_window = y.iloc[[i]]\n",
        "        test_date = dates.iloc[i]\n",
        "\n",
        "        # --- Step 1: Generate meta features from base model predictions ---\n",
        "        meta_features_test = np.zeros((1, len(base_models)))\n",
        "        for j, (name, model) in enumerate(base_models.items()):\n",
        "            model.fit(X_train_window, y_train_window)\n",
        "            probs = model.predict_proba(X_test_window)\n",
        "            if probs.shape[1] == 1:\n",
        "                # Edge-case: model returns only one class probability\n",
        "                y_pred_prob = 1.0 if model.classes_[0] == 1 else 0.0\n",
        "            else:\n",
        "                y_pred_prob = probs[0, 1]\n",
        "            meta_features_test[0, j] = y_pred_prob\n",
        "\n",
        "            # Record accuracy for each base model on this test observation\n",
        "            y_pred = model.predict(X_test_window)[0]\n",
        "            acc = accuracy_score(y_test_window, [y_pred])\n",
        "            rolling_accuracies_base[name].append(acc)\n",
        "\n",
        "        # --- Step 2: Use meta training set to make meta prediction once full window is reached ---\n",
        "        if i >= window_size:\n",
        "            meta_learner.fit(np.array(meta_train_features), np.array(meta_train_targets))\n",
        "            meta_pred = meta_learner.predict(meta_features_test)[0]\n",
        "            meta_acc = accuracy_score(y_test_window, [meta_pred])\n",
        "            rolling_accuracies_meta.append(meta_acc)\n",
        "            meta_weights_list.append(meta_learner.coef_[0])\n",
        "            prediction_source = \"Meta Learner\"\n",
        "            print(f\"Test date {test_date}: {prediction_source} Prediction = {meta_pred}, Accuracy = {meta_acc:.2f}\")\n",
        "        else:\n",
        "            # For i in [base_threshold, window_size), only base model predictions are available\n",
        "            print(f\"Test date {test_date}: Base Model predictions only (growing training window).\")\n",
        "\n",
        "        # --- Step 3: Append current meta features and true outcome to the expanding meta training set ---\n",
        "        meta_train_features.append(meta_features_test[0])\n",
        "        meta_train_targets.append(y_test_window.iloc[0])\n",
        "\n",
        "    # --- Aggregation of Results ---\n",
        "    avg_base_acc = {name: np.mean(acc_list) if acc_list else 0 for name, acc_list in rolling_accuracies_base.items()}\n",
        "    avg_meta_acc = np.mean(rolling_accuracies_meta) if rolling_accuracies_meta else 0\n",
        "    avg_meta_weights = np.mean(meta_weights_list, axis=0) if meta_weights_list else np.array([])\n",
        "\n",
        "    results_table = pd.DataFrame({\n",
        "        'Model': list(avg_base_acc.keys()) + ['Meta Learner'],\n",
        "        'Average Accuracy': list(avg_base_acc.values()) + [avg_meta_acc]\n",
        "    })\n",
        "    print(\"\\nRolling Window Average Accuracies:\")\n",
        "    print(results_table)\n",
        "\n",
        "    if avg_meta_weights.size > 0:\n",
        "        weights_table = pd.DataFrame({\n",
        "            'Base Model': list(base_models.keys()),\n",
        "            'Average Weight': avg_meta_weights\n",
        "        })\n",
        "        print(\"\\nAggregate Meta-Learner Weights (Average Coefficients):\")\n",
        "        print(weights_table)\n"
      ],
      "metadata": {
        "id": "sivK9Crtiv-a",
        "outputId": "20ef9239-7333-49c7-88ea-4ad9bd70ffdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting rolling window evaluation with stacking ensemble and expanding meta learner:\n",
            "Skipping prediction for timestamp 1996-12-31 00:00:00 (insufficient data: 0 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-01-31 00:00:00 (insufficient data: 1 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-02-28 00:00:00 (insufficient data: 2 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-03-31 00:00:00 (insufficient data: 3 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-04-30 00:00:00 (insufficient data: 4 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-05-30 00:00:00 (insufficient data: 5 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-06-30 00:00:00 (insufficient data: 6 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-07-31 00:00:00 (insufficient data: 7 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-08-29 00:00:00 (insufficient data: 8 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-09-30 00:00:00 (insufficient data: 9 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-10-31 00:00:00 (insufficient data: 10 observations, need 12).\n",
            "Skipping prediction for timestamp 1997-11-28 00:00:00 (insufficient data: 11 observations, need 12).\n",
            "Test date 1997-12-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-01-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-02-27 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-03-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-04-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-05-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-06-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-07-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-08-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-09-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-10-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-11-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1998-12-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-01-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-02-26 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-03-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-04-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-05-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-06-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-07-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-08-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-09-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-10-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-11-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 1999-12-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-01-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-02-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-03-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-04-28 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-05-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-06-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-07-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-08-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-09-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-10-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-11-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2000-12-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-01-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-02-28 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-03-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-04-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-05-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-06-29 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-07-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-08-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-09-28 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-10-31 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-11-30 00:00:00: Base Model predictions only (growing training window).\n",
            "Test date 2001-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-03-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2002-06-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2002-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2002-08-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2002-09-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2002-10-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2002-11-29 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2002-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-05-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-08-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-11-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2003-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-01-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-02-27 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2004-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-07-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2004-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2004-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-10-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2004-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-04-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2005-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-07-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2005-12-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-04-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-09-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2006-12-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-03-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-06-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2007-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-09-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2007-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2008-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2008-02-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2008-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2008-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2008-05-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2008-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2008-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2008-08-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2008-09-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2008-10-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2008-11-28 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2008-12-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2009-01-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2009-02-27 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2009-03-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2009-04-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2009-05-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-10-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2009-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-01-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2010-02-26 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2010-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2010-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2010-07-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2010-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-10-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2010-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-04-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-07-29 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2011-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2011-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2011-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2011-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2011-12-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-02-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-03-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2012-06-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2012-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-09-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2012-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-03-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-06-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-08-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-11-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2013-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-05-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-08-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-11-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2014-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-01-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-02-27 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-05-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2015-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2015-10-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2015-12-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2016-01-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-02-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2016-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-04-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-07-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2016-12-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-04-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-09-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2017-12-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-03-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2018-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-06-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-09-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2018-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2018-11-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2018-12-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2019-01-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2019-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-03-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-06-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-08-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-11-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2019-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2020-03-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2020-04-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2020-05-29 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2020-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-10-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2020-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2020-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-01-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-02-26 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-07-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-10-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2021-12-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2022-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2022-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2022-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2022-04-29 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2022-05-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2022-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2022-07-29 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2022-08-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2022-09-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2022-10-31 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 1.00\n",
            "Test date 2022-11-30 00:00:00: Meta Learner Prediction = 1.0, Accuracy = 0.00\n",
            "Test date 2022-12-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-02-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2023-03-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-04-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2023-06-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2023-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-08-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-09-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2023-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2023-11-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2023-12-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-01-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-02-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-03-29 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-04-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2024-05-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 0.00\n",
            "Test date 2024-06-28 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-07-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-08-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-09-30 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "Test date 2024-10-31 00:00:00: Meta Learner Prediction = 0.0, Accuracy = 1.00\n",
            "\n",
            "Rolling Window Average Accuracies:\n",
            "              Model  Average Accuracy\n",
            "0      RandomForest          0.804954\n",
            "1  GradientBoosting          0.758514\n",
            "2               SVC          0.780186\n",
            "3        GaussianNB          0.777090\n",
            "4      Meta Learner          0.832727\n",
            "\n",
            "Aggregate Meta-Learner Weights (Average Coefficients):\n",
            "         Base Model  Average Weight\n",
            "0      RandomForest        1.782195\n",
            "1  GradientBoosting        0.033369\n",
            "2               SVC        0.479850\n",
            "3        GaussianNB        1.111256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ML == True:\n",
        "\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import pandas as pd\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from sklearn.svm import SVC\n",
        "    from sklearn.naive_bayes import GaussianNB\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "    # Define predictors and target using the given column lists\n",
        "    predictors = FEATURES\n",
        "    target = TRUE_REGIME\n",
        "\n",
        "    # Drop rows with missing values in predictors or target\n",
        "    data = input_df.dropna(subset=predictors + [target]).reset_index(drop=True)\n",
        "    X = data[predictors]\n",
        "    y = data[target]\n",
        "    # Also grab the DATE column for printing prediction timestamps\n",
        "    dates = data[DATE]\n",
        "\n",
        "    # Define the base models with high parameters\n",
        "    base_models = {\n",
        "        'RandomForest': RandomForestClassifier(n_estimators=200, random_state=42),\n",
        "        'SVC': SVC(probability=True, C=1000, kernel='rbf', gamma='scale'),\n",
        "        'GaussianNB': GaussianNB(var_smoothing=1e-8)\n",
        "    }\n",
        "\n",
        "    # Define the meta-learner\n",
        "    meta_learner = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "    window_size = 60  # e.g., 60 observations for training\n",
        "    meta_start_threshold = window_size  # start meta predictions after 60 meta samples\n",
        "\n",
        "    # Initialize dictionaries/lists to store rolling accuracies\n",
        "    rolling_accuracies_base = {name: [] for name in base_models}\n",
        "    rolling_accuracies_meta = []\n",
        "    meta_weights_list = []  # store meta-learner coefficients when trained\n",
        "\n",
        "    # Initialize expanding meta training set (meta features and outcomes)\n",
        "    meta_train_features = []\n",
        "    meta_train_targets = []\n",
        "\n",
        "    print(\"Starting rolling window evaluation with stacking ensemble and expanding meta learner:\")\n",
        "\n",
        "    # Loop over the dataset in a rolling window manner\n",
        "    for i in range(len(X)):\n",
        "        if i < window_size:\n",
        "            print(f\"Skipping prediction for timestamp {dates.iloc[i]} \"\n",
        "                  f\"(insufficient data: {i} observations, need {window_size}).\")\n",
        "            continue\n",
        "\n",
        "        # Define training set as the previous window_size observations and test as the next point\n",
        "        X_train_window = X.iloc[i-window_size:i]\n",
        "        y_train_window = y.iloc[i-window_size:i]\n",
        "        X_test_window = X.iloc[[i]]\n",
        "        y_test_window = y.iloc[[i]]\n",
        "        test_date = dates.iloc[i]\n",
        "\n",
        "        # --- Step 1: Get meta features from base models for the test observation ---\n",
        "        meta_features_test = np.zeros((1, len(base_models)))\n",
        "        for j, (name, model) in enumerate(base_models.items()):\n",
        "            model.fit(X_train_window, y_train_window)\n",
        "            probs = model.predict_proba(X_test_window)\n",
        "            if probs.shape[1] == 1:\n",
        "                # Handle edge-case when model predicts only one class\n",
        "                y_pred_prob = 1.0 if model.classes_[0] == 1 else 0.0\n",
        "            else:\n",
        "                y_pred_prob = probs[0, 1]\n",
        "            meta_features_test[0, j] = y_pred_prob\n",
        "\n",
        "            # Record base model accuracy on this test observation\n",
        "            y_pred = model.predict(X_test_window)[0]\n",
        "            acc = accuracy_score(y_test_window, [y_pred])\n",
        "            rolling_accuracies_base[name].append(acc)\n",
        "\n",
        "        # --- Step 2: Use expanding meta training set to predict final outcome ---\n",
        "        if len(meta_train_features) >= meta_start_threshold:\n",
        "            # Train the meta-learner on all previously accumulated meta samples\n",
        "            meta_learner.fit(np.array(meta_train_features), np.array(meta_train_targets))\n",
        "            meta_pred = meta_learner.predict(meta_features_test)[0]\n",
        "            meta_acc = accuracy_score(y_test_window, [meta_pred])\n",
        "            rolling_accuracies_meta.append(meta_acc)\n",
        "            meta_weights_list.append(meta_learner.coef_[0])\n",
        "            print(f\"Test date {test_date}: Meta Learner Prediction = {meta_pred}, Accuracy = {meta_acc:.2f}\")\n",
        "        else:\n",
        "            print(f\"Test date {test_date}: Not enough meta training samples for meta learner prediction (have {len(meta_train_features)}).\")\n",
        "\n",
        "        # --- Step 3: Append current meta features and true outcome to the expanding meta training set ---\n",
        "        meta_train_features.append(meta_features_test[0])\n",
        "        meta_train_targets.append(y_test_window.iloc[0])\n",
        "\n",
        "    # --- Aggregation of results ---\n",
        "    # Compute average accuracy for each base model and meta-learner\n",
        "    avg_base_acc = {name: np.mean(acc_list) if acc_list else 0 for name, acc_list in rolling_accuracies_base.items()}\n",
        "    avg_meta_acc = np.mean(rolling_accuracies_meta) if rolling_accuracies_meta else 0\n",
        "    avg_meta_weights = np.mean(meta_weights_list, axis=0) if meta_weights_list else np.array([])\n",
        "\n",
        "    # Create a table for accuracies\n",
        "    results_table = pd.DataFrame({\n",
        "        'Model': list(avg_base_acc.keys()) + ['Meta Learner'],\n",
        "        'Average Accuracy': list(avg_base_acc.values()) + [avg_meta_acc]\n",
        "    })\n",
        "    print(\"\\nRolling Window Average Accuracies:\")\n",
        "    print(results_table)\n",
        "\n",
        "    # Create a table for aggregate meta-learner weights (if available)\n",
        "    if avg_meta_weights.size > 0:\n",
        "        weights_table = pd.DataFrame({\n",
        "            'Base Model': list(base_models.keys()),\n",
        "            'Average Weight': avg_meta_weights\n",
        "        })\n",
        "        print(\"\\nAggregate Meta-Learner Weights (Average Coefficients):\")\n",
        "        print(weights_table)\n"
      ],
      "metadata": {
        "id": "vp7FA1IRumLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if DL == True:\n",
        "    import warnings\n",
        "    warnings.filterwarnings(\"ignore\")  # Suppress warnings\n",
        "\n",
        "    import tensorflow as tf\n",
        "    from tensorflow.keras.models import Sequential, Model\n",
        "    from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "    import matplotlib.pyplot as plt\n",
        "    import seaborn as sns\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # Define the experiment function with remapping of training labels if needed\n",
        "    def run_experiment(training_window, seq_len,\n",
        "                       lstm_units_first=128, lstm_units_second=32, dropout_rate=0.3,\n",
        "                       xgb_n_estimators=100, xgb_max_depth=4, xgb_learning_rate=0.1):\n",
        "        \"\"\"\n",
        "        Runs the rolling-window hybrid (LSTM + XGBoost) experiment using a given training_window (in months)\n",
        "        and a sequence length (seq_len). If the training labels do not form a contiguous set starting at 0,\n",
        "        they are remapped. Returns overall accuracy, total number of predictions, and regime-specific accuracies.\n",
        "        \"\"\"\n",
        "        all_preds = []   # To store predictions\n",
        "        all_true = []    # To store true regime labels\n",
        "\n",
        "        # Ensure data is sorted by date\n",
        "        data = input_df.sort_values(DATE).reset_index(drop=True)\n",
        "\n",
        "        # Rolling window: iterate from index=training_window to end of data\n",
        "        for i in range(training_window, len(data)):\n",
        "            # Use the previous 'training_window' months as training data\n",
        "            train_window = data.iloc[i - training_window:i].reset_index(drop=True)\n",
        "            if len(train_window) < training_window:\n",
        "                continue\n",
        "\n",
        "            # Generate overlapping sequences of length 'seq_len' within the training window\n",
        "            X_train_sequences = []\n",
        "            y_train = []\n",
        "            for j in range(len(train_window) - seq_len + 1):\n",
        "                seq = train_window[FEATURES].iloc[j : j + seq_len].values\n",
        "                target = train_window[TRUE_REGIME].iloc[j + seq_len - 1]  # regime at end of sequence\n",
        "                X_train_sequences.append(seq)\n",
        "                y_train.append(target)\n",
        "            X_train_sequences = np.array(X_train_sequences)  # shape: (num_samples, seq_len, num_features)\n",
        "            y_train = np.array(y_train)\n",
        "\n",
        "            # Remap training labels to contiguous integers starting at 0 if needed.\n",
        "            unique_classes = np.unique(y_train)\n",
        "            mapping = {old: new for new, old in enumerate(unique_classes)}\n",
        "            inverse_mapping = {v: k for k, v in mapping.items()}\n",
        "            y_train_mapped = np.array([mapping[val] for val in y_train])\n",
        "\n",
        "            # Build the LSTM feature extractor using an explicit Input layer\n",
        "            input_layer = Input(shape=(seq_len, len(FEATURES)))\n",
        "            lstm_out = LSTM(lstm_units_first, return_sequences=True)(input_layer)\n",
        "            lstm_out = LSTM(lstm_units_second, return_sequences=False)(lstm_out)\n",
        "            dropout_out = Dropout(dropout_rate)(lstm_out)\n",
        "            feature_extractor = Model(inputs=input_layer, outputs=dropout_out)\n",
        "\n",
        "            # Extract features from training sequences\n",
        "            X_train_features = feature_extractor.predict(X_train_sequences, verbose=0)\n",
        "\n",
        "            # Train an XGBoost classifier on these features using the remapped labels.\n",
        "            clf = xgb.XGBClassifier(\n",
        "                n_estimators=xgb_n_estimators,\n",
        "                max_depth=xgb_max_depth,\n",
        "                learning_rate=xgb_learning_rate,\n",
        "                use_label_encoder=False,\n",
        "                eval_metric='mlogloss'\n",
        "            )\n",
        "            clf.fit(X_train_features, y_train_mapped)\n",
        "\n",
        "            # For prediction, use the last 'seq_len'-month sequence from the training window\n",
        "            X_pred_seq = train_window[FEATURES].iloc[-seq_len:].values.reshape(1, seq_len, len(FEATURES))\n",
        "            X_pred_feature = feature_extractor.predict(X_pred_seq, verbose=0)\n",
        "            pred_mapped = clf.predict(X_pred_feature)[0]\n",
        "            # Map back to original label\n",
        "            pred_regime = inverse_mapping[pred_mapped]\n",
        "\n",
        "            # The true regime for the next month (at time index i)\n",
        "            true_regime = data[TRUE_REGIME].iloc[i]\n",
        "\n",
        "            all_preds.append(pred_regime)\n",
        "            all_true.append(true_regime)\n",
        "\n",
        "        # Compute overall accuracy and total number of predictions\n",
        "        overall_acc = accuracy_score(all_true, all_preds)\n",
        "        total_obs = len(all_preds)\n",
        "\n",
        "        # Compute regime-specific accuracies\n",
        "        regimes = np.unique(all_true)\n",
        "        regime_accuracies = {}\n",
        "        for r in regimes:\n",
        "            indices = [idx for idx, val in enumerate(all_true) if val == r]\n",
        "            if len(indices) > 0:\n",
        "                correct = sum(1 for idx in indices if all_preds[idx] == all_true[idx])\n",
        "                regime_acc = correct / len(indices)\n",
        "                regime_accuracies[r] = regime_acc\n",
        "            else:\n",
        "                regime_accuracies[r] = np.nan\n",
        "\n",
        "        return {\n",
        "            \"training_window\": training_window,\n",
        "            \"seq_len\": seq_len,\n",
        "            \"total_obs\": total_obs,\n",
        "            \"overall_accuracy\": overall_acc,\n",
        "            \"regime_accuracies\": regime_accuracies\n",
        "        }\n",
        "\n",
        "    # Prepare to store experiment results in a list of dictionaries\n",
        "    results = []\n",
        "\n",
        "    # Define training windows from 1 to 10 years (in months) â€“ assume 1 year = 12 months\n",
        "    for tw in range(12, 121, 12):  # 12, 24, ..., 120 months\n",
        "        # Define sequence length percentages: 25%, 50%, 75%, and 100% of training window\n",
        "        for frac in [0.25, 0.5, 0.75, 1.0]:\n",
        "            seq = int(round(tw * frac))\n",
        "            print(\"=\"*50)\n",
        "            print(f\"Running experiment with training_window = {tw} months and seq_len = {seq} months\")\n",
        "            res = run_experiment(training_window=tw, seq_len=seq)\n",
        "            # Build a result row that includes run parameters and accuracies\n",
        "            row = {\n",
        "                \"Training Window (months)\": tw,\n",
        "                \"Seq Length (months)\": seq,\n",
        "                \"Total Observations\": res[\"total_obs\"],\n",
        "                \"Overall Accuracy\": f\"{res['overall_accuracy']*100:.2f}%\",\n",
        "                \"Accuracy Regime 0\": f\"{res['regime_accuracies'].get(0, np.nan)*100:.2f}%\" if 0 in res[\"regime_accuracies\"] else \"N/A\",\n",
        "                \"Accuracy Regime 1\": f\"{res['regime_accuracies'].get(1, np.nan)*100:.2f}%\" if 1 in res[\"regime_accuracies\"] else \"N/A\",\n",
        "                \"Accuracy Regime 2\": f\"{res['regime_accuracies'].get(2, np.nan)*100:.2f}%\" if 2 in res[\"regime_accuracies\"] else \"N/A\"\n",
        "            }\n",
        "            results.append(row)\n",
        "            print(\"=\"*50)\n",
        "\n",
        "    # Convert results to a DataFrame and display as an HTML table\n",
        "    df_results = pd.DataFrame(results)\n",
        "    display(HTML(df_results.to_html(index=False)))\n"
      ],
      "metadata": {
        "id": "DoyxwlWcFY1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert results to a DataFrame and display as an HTML table\n",
        "df_results = pd.DataFrame(results)\n",
        "display(HTML(df_results.to_html(index=False)))\n"
      ],
      "metadata": {
        "id": "aFjKoZBkmOQb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}