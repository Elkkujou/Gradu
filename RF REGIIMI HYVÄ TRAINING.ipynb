{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IDEAT: Vix takas? Joku momentum indikaattori? Sentimentti?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mqpPtrCOkXAO"
      },
      "id": "mqpPtrCOkXAO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model settings"
      ],
      "metadata": {
        "id": "k1mEzhhWXup6"
      },
      "id": "k1mEzhhWXup6"
    },
    {
      "cell_type": "code",
      "source": [
        "use_regime_split = False\n",
        "\n",
        "#Default models\n",
        "RF = True # perus random forest\n",
        "RF2 = True\n",
        "GB = False # perus gradient boost\n",
        "Hybrid = True\n",
        "\n",
        "#Looping models\n",
        "RF_feature_seek = False # random forest all combinations\n",
        "seek_all = False\n",
        "gb_loop = False\n",
        "\n",
        "#DATA\n",
        "FF5 = True\n",
        "FF5_long = False\n",
        "MSCI = False\n",
        "\n",
        "RSI = True\n",
        "\n",
        "local = False #ajetaanko colab vai oma kone\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-AF3FdwvaLPp"
      },
      "id": "-AF3FdwvaLPp",
      "execution_count": 186,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify active model flags\n",
        "active_modes = [name for name, flag in zip(\n",
        "    ['RF', 'GB', 'RF_feature_seek', 'Hybrid', 'seek_all', 'gb_loop'],\n",
        "    [RF, GB, RF_feature_seek, Hybrid, seek_all, gb_loop]\n",
        ") if flag]\n",
        "\n",
        "if active_modes:\n",
        "    print(\"‚úÖ Active model modes:\", \", \".join(active_modes))\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No active model mode selected.\")\n",
        "\n",
        "# Check dataset toggles: exactly one must be True\n",
        "datasets = {\n",
        "    'FF5': FF5,\n",
        "    'FF5_long': FF5_long,\n",
        "    'MSCI': MSCI\n",
        "}\n",
        "active_datasets = [name for name, flag in datasets.items() if flag]\n",
        "\n",
        "if len(active_datasets) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [FF5, FF5_long, MSCI] must be True.\")\n",
        "else:\n",
        "    print(f\"üìä Using dataset: {active_datasets[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ktRzXgrQHYX",
        "outputId": "38c63e03-2820-4517-c335-4d547c76d5ec"
      },
      "id": "7ktRzXgrQHYX",
      "execution_count": 187,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Active model modes: RF, Hybrid\n",
            "üìä Using dataset: FF5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "id": "4085d55c-e568-465c-9bcc-6013281c105d",
      "metadata": {
        "tags": [],
        "id": "4085d55c-e568-465c-9bcc-6013281c105d"
      },
      "outputs": [],
      "source": [
        "# # Import Required Libraries\n",
        "#\n",
        "# Import all necessary libraries for data manipulation, visualization,\n",
        "# machine learning, and regression analysis.\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from tabulate import tabulate\n",
        "\n",
        "from IPython.display import display, HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not local:\n",
        "\n",
        "  %cd /content\n",
        "  !rm -rf Gradu\n",
        "  !git clone https://github.com/Elkkujou/Gradu.git\n",
        "  %cd /content/Gradu\n",
        "  !ls\n",
        "  xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "\n",
        "else:\n",
        "\n",
        "\n",
        "\n",
        "    repo_url = \"https://github.com/Elkkujou/Gradu.git\"\n",
        "    repo_name = \"Gradu\"  # Name of the cloned folder\n",
        "\n",
        "    # Check if the directory already exists\n",
        "    if os.path.exists(repo_name):\n",
        "        print(f\"Folder '{repo_name}' already exists. Pulling latest changes...\")\n",
        "        # Change to the existing repo folder and pull the latest updates\n",
        "        subprocess.run([\"git\", \"-C\", repo_name, \"pull\"], check=True)\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{repo_name}'...\")\n",
        "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "    # List contents of the cloned repository\n",
        "    subprocess.run([\"ls\", repo_name], check=True)\n",
        "    xls_file = pd.ExcelFile(\"Gradu/THE_2ND_latest.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j2fmaZCMluYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8e06daa-61c8-4465-f7c3-86b1bf861c13"
      },
      "id": "j2fmaZCMluYf",
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Gradu'...\n",
            "remote: Enumerating objects: 865, done.\u001b[K\n",
            "remote: Counting objects: 100% (166/166), done.\u001b[K\n",
            "remote: Compressing objects: 100% (38/38), done.\u001b[K\n",
            "remote: Total 865 (delta 153), reused 128 (delta 128), pack-reused 699 (from 1)\u001b[K\n",
            "Receiving objects: 100% (865/865), 199.88 MiB | 20.93 MiB/s, done.\n",
            "Resolving deltas: 100% (420/420), done.\n",
            "/content/Gradu\n",
            " chatti_RF.ipynb\t\t      regime_prediction_msci.ipynb\n",
            " data+regimes.xlsx\t\t      regime_pred.txt\n",
            " Fama_french_XGBOOST.ipynb\t      RF_Gradu.ipynb\n",
            "'Financial turbulence.ipynb'\t     'RF REGIIMI HYV√Ñ TRAINING.ipynb'\n",
            " FT_source.xlsx\t\t\t     'RF_REGIIMI_HYV√Ñ_TRAINING (MSCI).ipynb'\n",
            " Gradient_boost_malli.ipynb\t     'RF_regime (3).ipynb'\n",
            " MSCI_XGBOOST.ipynb\t\t      THE_2ND_latest.xlsx\n",
            " Regiimi_prediction.ipynb\t      THE_2ND.xlsx\n",
            " regime_prediction_famafrench.ipynb   THE_ONE.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5:\n",
        "  SHEET_NAME = \"ajodata_FF5\"\n",
        "  FEATURES = ['CPI%','T10YFF', 'CFNAI', 'Cape', 'GARCH_1M']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = False"
      ],
      "metadata": {
        "id": "R_dU9PNF-Puv"
      },
      "id": "R_dU9PNF-Puv",
      "execution_count": 190,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5_long:\n",
        "  SHEET_NAME = \"ajodata_FF5_long\"\n",
        "  FEATURES = ['CPI%', 'T10YFF', 'CFNAI','Cape']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "wJo52ErY-v0Q"
      },
      "id": "wJo52ErY-v0Q",
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MSCI:\n",
        "  SHEET_NAME = \"ajodata_MSCI\"\n",
        "  FEATURES = ['CPI%', 'T10YFF', 'CFNAI','Cape']\n",
        "  FACTORS = [\n",
        "    'Size',\n",
        "    'value',\n",
        "    'Quality',\n",
        "    'min_vola']\n",
        "  BENCHMARK = ['Us_standard']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "AT6Cf_ge_ApN"
      },
      "id": "AT6Cf_ge_ApN",
      "execution_count": 192,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data"
      ],
      "metadata": {
        "id": "ZwuAM8venlx5"
      },
      "id": "ZwuAM8venlx5"
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
      "metadata": {
        "tags": [],
        "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
        "outputId": "8d450b33-1bf5-44f8-bcc1-1fa9bc5b74f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers in the 'ajodata_FF5' sheet:\n",
            "Index(['Date', 'SMB', 'HML', 'RMW', 'CMA', 'Mkt', 'RF', 'Mkt-RF', 'GARCH_1M',\n",
            "       'CPI%', 'T10YFF', 'Amihud', 'LEI%', 'Cape', 'Cape %', 'GDP', 'TED',\n",
            "       'T10Y3', 'LEI', 'CFNAI', 'HV', 'VIX', 'EWMA', 'AvgShock', 'AR_Shock',\n",
            "       'RealizedVol', 'RelVol_12m', 'VolBucket'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table class=\"dataframe table table-striped\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>First observation date</td>\n",
              "      <td>1963-07-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Last observation date</td>\n",
              "      <td>2024-11-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Total number of observations</td>\n",
              "      <td>737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = xls_file.parse(SHEET_NAME)\n",
        "df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "# Print headers dynamically\n",
        "print(f\"Headers in the '{SHEET_NAME}' sheet:\")\n",
        "print(df.columns)\n",
        "\n",
        "REGIMES_COLUMN = 'Predicted_reg'\n",
        "\n",
        "# Convert the leftmost column (assumed to be the date column) to datetime\n",
        "date_column = df.columns[0]\n",
        "df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "# Retrieve first and last observation dates and count observations\n",
        "first_date = df[date_column].iloc[0]\n",
        "last_date = df[date_column].iloc[-1]\n",
        "n_observations = len(df)\n",
        "\n",
        "# Create a DataFrame with the information\n",
        "info_df = pd.DataFrame({\n",
        "    \"Description\": [\"First observation date\", \"Last observation date\", \"Total number of observations\"],\n",
        "    \"Value\": [first_date, last_date, n_observations]\n",
        "})\n",
        "\n",
        "# Display the results as a neat HTML table\n",
        "display(HTML(info_df.to_html(index=False, classes=\"table table-striped\", border=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if RSI:\n",
        "\n",
        "  # --- Add lagged 12‚Äëmonth moving average for each factor return ---\n",
        "  for f in FACTORS:\n",
        "      # shift by 1 so that MA at time t uses returns t-12‚Ä¶t-1\n",
        "      df[f + '_MA12'] = (\n",
        "          df[f]\n",
        "          .shift(1)                          # drop ‚Äútoday‚Äù\n",
        "          .rolling(window=12, min_periods=12)\n",
        "          .mean()\n",
        "      )\n",
        "\n",
        "  # update your FEATURES list\n",
        "  FEATURES += [f + '_MA12' for f in FACTORS]\n",
        "\n",
        "  print(\"‚ú® Added lagged 12‚Äëmonth MA columns:\", [f + '_MA12' for f in FACTORS])\n",
        "  print(\"üß© New FEATURES list:\", FEATURES)"
      ],
      "metadata": {
        "id": "3bGFK-QXeeDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc4c8c4a-4e9d-4822-8b89-deb09303d0b1"
      },
      "id": "3bGFK-QXeeDR",
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ú® Added lagged 12‚Äëmonth MA columns: ['SMB_MA12', 'HML_MA12', 'CMA_MA12', 'RMW_MA12']\n",
            "üß© New FEATURES list: ['CPI%', 'T10YFF', 'CFNAI', 'Cape', 'GARCH_1M', 'SMB_MA12', 'HML_MA12', 'CMA_MA12', 'RMW_MA12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Define Helper Functions ---\n",
        "def annualized_return(returns):\n",
        "    \"\"\"Compute the compounded annualized return (assuming monthly returns).\"\"\"\n",
        "    return np.prod(1 + returns)**(12 / len(returns)) - 1\n",
        "\n",
        "def compute_metrics(returns):\n",
        "    \"\"\"\n",
        "    Compute key metrics for a returns series:\n",
        "      - Annualized Return\n",
        "      - Annualized Volatility (assuming monthly returns)\n",
        "      - Total Cumulative Return\n",
        "    \"\"\"\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    total_cum_return = cumulative_returns.iloc[-1] - 1\n",
        "    ann_ret = annualized_return(returns)\n",
        "    ann_vol = np.std(returns) * np.sqrt(12)\n",
        "    return ann_ret, ann_vol, total_cum_return\n",
        "\n",
        "# --- Compute Metrics for Benchmark and Each Factor ---\n",
        "metrics = []\n",
        "\n",
        "# Compute metrics for the benchmark.\n",
        "benchmark_returns = df[BENCHMARK[0]]\n",
        "bench_ann_ret, bench_ann_vol, bench_cum_return = compute_metrics(benchmark_returns)\n",
        "metrics.append({\n",
        "    \"Strategy\": \"Benchmark\",\n",
        "    \"Annualized Return\": f\"{bench_ann_ret*100:.2f}%\",\n",
        "    \"Annualized Volatility\": f\"{bench_ann_vol*100:.2f}%\",\n",
        "    \"Total Cumulative Return\": f\"{bench_cum_return*100:.2f}%\"\n",
        "})\n",
        "\n",
        "# Compute metrics for each factor in FACTORS.\n",
        "for factor in FACTORS:\n",
        "    factor_returns = df[factor]\n",
        "    factor_ann_ret, factor_ann_vol, factor_cum_return = compute_metrics(factor_returns)\n",
        "    metrics.append({\n",
        "        \"Strategy\": factor,\n",
        "        \"Annualized Return\": f\"{factor_ann_ret*100:.2f}%\",\n",
        "        \"Annualized Volatility\": f\"{factor_ann_vol*100:.2f}%\",\n",
        "        \"Total Cumulative Return\": f\"{factor_cum_return*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the metrics.\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# --- Display the Results as an HTML Table ---\n",
        "display(HTML(metrics_df.to_html(index=False)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "phBSNpYXviHe",
        "outputId": "63fcbdd0-0ec0-44c2-9810-abc0a70dc77c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "id": "phBSNpYXviHe",
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Strategy</th>\n",
              "      <th>Annualized Return</th>\n",
              "      <th>Annualized Volatility</th>\n",
              "      <th>Total Cumulative Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Benchmark</td>\n",
              "      <td>10.71%</td>\n",
              "      <td>15.46%</td>\n",
              "      <td>51691.71%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>SMB</td>\n",
              "      <td>1.89%</td>\n",
              "      <td>10.55%</td>\n",
              "      <td>216.75%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>HML</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>10.37%</td>\n",
              "      <td>473.19%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CMA</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>7.18%</td>\n",
              "      <td>472.94%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>RMW</td>\n",
              "      <td>3.14%</td>\n",
              "      <td>7.67%</td>\n",
              "      <td>568.93%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
      "metadata": {
        "tags": [],
        "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
        "outputId": "9822a6ac-9ae6-45d0-8774-ecb09fe4b822",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows before cleaning: 737\n",
            "\n",
            "Missing values in FEATURES before cleaning:\n",
            "CPI%         0\n",
            "T10YFF       0\n",
            "CFNAI       45\n",
            "Cape         0\n",
            "GARCH_1M     0\n",
            "SMB_MA12    12\n",
            "HML_MA12    12\n",
            "CMA_MA12    12\n",
            "RMW_MA12    12\n",
            "dtype: int64\n",
            "\n",
            "Dropped 45 rows due to missing FEATURES.\n",
            "\n",
            "Missing values in FEATURES after cleaning:\n",
            "CPI%        0\n",
            "T10YFF      0\n",
            "CFNAI       0\n",
            "Cape        0\n",
            "GARCH_1M    0\n",
            "SMB_MA12    0\n",
            "HML_MA12    0\n",
            "CMA_MA12    0\n",
            "RMW_MA12    0\n",
            "dtype: int64\n",
            "\n",
            "Final dataset now has 692 rows.\n",
            "Target value counts:\n",
            "Winning Factor\n",
            "SMB    210\n",
            "RMW    209\n",
            "HML    164\n",
            "CMA    109\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Toggle for dropping rows with missing values in the FEATURES columns.\n",
        "drop_empty = True\n",
        "\n",
        "# 1) Print the initial number of rows.\n",
        "initial_rows = len(df)\n",
        "print(f\"Total number of rows before cleaning: {initial_rows}\")\n",
        "\n",
        "# 2) Show missing‚Äêvalue counts in FEATURES.\n",
        "missing_counts = df[FEATURES].isna().sum()\n",
        "print(\"\\nMissing values in FEATURES before cleaning:\")\n",
        "print(missing_counts)\n",
        "\n",
        "# 3) Drop any rows with NA in FEATURES, if requested.\n",
        "if drop_empty:\n",
        "    before = len(df)\n",
        "    df.dropna(subset=FEATURES, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    dropped = before - len(df)\n",
        "    print(f\"\\nDropped {dropped} rows due to missing FEATURES.\")\n",
        "else:\n",
        "    print(\"\\nKeeping all rows, including those with missing FEATURES.\")\n",
        "\n",
        "# 4) Re‚Äêcheck that FEATURES are now complete:\n",
        "print(\"\\nMissing values in FEATURES after cleaning:\")\n",
        "print(df[FEATURES].isna().sum())\n",
        "\n",
        "# 5) Compute your target column in‚Äêplace.\n",
        "df['Winning Factor'] = df[FACTORS].idxmax(axis=1).astype('category')\n",
        "\n",
        "# 6) (Optionally) create a numeric code column\n",
        "df['Winning Factor Code'] = df['Winning Factor'].cat.codes\n",
        "\n",
        "# 7) Quick summary:\n",
        "print(f\"\\nFinal dataset now has {len(df)} rows.\")\n",
        "print(\"Target value counts:\")\n",
        "print(df['Winning Factor'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_regime_split:\n",
        "\n",
        "    # --- Regime Mapping & Conversion to Numeric Codes (Dynamic) ---\n",
        "\n",
        "    # Dynamically extract the unique values in the REGIMES_COLUMN.\n",
        "    unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "\n",
        "    # Convert the Regimes column to a categorical type with the unique values, ordered alphabetically.\n",
        "    df[REGIMES_COLUMN] = pd.Categorical(df[REGIMES_COLUMN], categories=sorted(unique_regimes), ordered=True)\n",
        "\n",
        "    # Create a dictionary mapping numeric codes to the regime names based on the unique values.\n",
        "    regime_mapping = {i: cat for i, cat in enumerate(df[REGIMES_COLUMN].cat.categories)}\n",
        "\n",
        "    # Now encode the Regimes column as numeric codes.\n",
        "    df[REGIMES_COLUMN] = df[REGIMES_COLUMN].cat.codes\n",
        "\n",
        "    # Create a mapping from numeric codes to original regime names.\n",
        "    regime_short_mapping = {code: name for code, name in regime_mapping.items()}\n",
        "\n",
        "    # Calculate the number of observations for each regime using value_counts (without reindexing).\n",
        "    obs_counts = df[REGIMES_COLUMN].value_counts(sort=False)\n",
        "\n",
        "    # Create a DataFrame preview of the regime mapping, including observation counts.\n",
        "    mapping_table_data = []\n",
        "    for code in regime_mapping.keys():\n",
        "        mapping_table_data.append({\n",
        "            \"Numeric Code\": code,\n",
        "            \"Original Name\": regime_mapping.get(code, \"N/A\"),\n",
        "            \"Observations\": obs_counts.get(code, 0)\n",
        "        })\n",
        "\n",
        "    # Append a row with the total observations.\n",
        "    total_obs = obs_counts.sum()\n",
        "    mapping_table_data.append({\n",
        "        \"Numeric Code\": \"\",\n",
        "        \"Original Name\": \"Total\",\n",
        "        \"Observations\": total_obs\n",
        "    })\n",
        "\n",
        "    # Create the DataFrame for regime mapping preview and print.\n",
        "    regime_mapping_df = pd.DataFrame(mapping_table_data)\n",
        "\n",
        "    from tabulate import tabulate\n",
        "    print(\"Preview of Dynamic Regime Mapping:\")\n",
        "    print(tabulate(regime_mapping_df, headers=\"keys\", tablefmt=\"psql\", showindex=False))\n"
      ],
      "metadata": {
        "id": "wYgvlvGRUUG4"
      },
      "id": "wYgvlvGRUUG4",
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "0wYzowb5Xdau"
      },
      "id": "0wYzowb5Xdau"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature seek"
      ],
      "metadata": {
        "id": "KkFWbyO6XlRZ"
      },
      "id": "KkFWbyO6XlRZ"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF_feature_seek:\n",
        "    import itertools\n",
        "    import os\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # --------------------------\n",
        "    # Parameters for Feature & Training Window Search\n",
        "    # --------------------------\n",
        "    min_features = 2                  # minimum number of features in a subset\n",
        "    max_features = len(FEATURES)      # maximum number of features (or set to a smaller number if desired)\n",
        "\n",
        "    # Define fixed rolling window sizes (in years) to test (assuming monthly data)\n",
        "    training_window_years = [5, 10, 15, 20]\n",
        "\n",
        "    # Also run an expanding window experiment\n",
        "    run_expanding_window = True\n",
        "\n",
        "    # Independent variable: minimum number of observations required for making a prediction.\n",
        "    # This is now decoupled from the training window calculation.\n",
        "    min_obs_for_prediction = 60  # adjust this value as desired\n",
        "\n",
        "    output_filename = \"feature_subset_results.csv\"\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    # Ensure the data is sorted by date.\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # --------------------------\n",
        "    # Outer Loop: Fixed Rolling Window Modes\n",
        "    # --------------------------\n",
        "    for years in training_window_years:\n",
        "        # Convert years to number of observations (assume 12 obs per year)\n",
        "        rolling_window_size = years * 12\n",
        "        # Ensure predictions start only after both the rolling window and the independent minimum are met.\n",
        "        start_index = max(min_obs_for_prediction, rolling_window_size)\n",
        "        print(f\"\\n--- Testing fixed rolling window of {years} years \"\n",
        "              f\"({rolling_window_size} observations, starting predictions at index {start_index}) ---\")\n",
        "        outer_start_time = time.time()\n",
        "\n",
        "        # Inner loop over feature subset sizes\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            # Loop over all combinations of size r\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination: {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # Loop over test rows, starting when we have enough training data\n",
        "                for i in range(start_index, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    # Build fixed rolling training window (most recent rolling_window_size observations)\n",
        "                    train_window = df_sorted.iloc[i - rolling_window_size : i].copy()\n",
        "\n",
        "                    # Ensure the last training observation is strictly before test row date\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    # (Optional) Regime check if use_regime_split is True:\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                        if insufficient_regimes:\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "\n",
        "                    # Prepare training data for the current feature subset.\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    # Train the RandomForest model.\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    # Use the last row of the training window as test data.\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    # Map the predicted probabilities to the full set of FACTORS.\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    # months_ahead: how many months ahead the prediction is (optional usage)\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    # Collect feature levels for logging\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    # Create a result row\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": years,   # <--- Record the training window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                # End of inner test row loop for this feature combination.\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Results for combination {current_features} appended to CSV. \"\n",
        "                          f\"Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed fixed rolling window of {years} years in {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Expanding Window Mode\n",
        "    # --------------------------\n",
        "    if run_expanding_window:\n",
        "        print(\"\\n--- Testing Expanding Window Mode ---\")\n",
        "        outer_start_time = time.time()\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination (expanding): {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # In expanding mode, the training window goes from the start until the test row.\n",
        "                # Start predictions only after the minimum observation threshold is met.\n",
        "                for i in range(min_obs_for_prediction, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "                    train_window = df_sorted.iloc[:i].copy()\n",
        "                    if train_window.empty:\n",
        "                        continue\n",
        "\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": \"expanding\",  # <--- Indicate expanding window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Expanding window: Results for combination {current_features} \"\n",
        "                          f\"appended to CSV. Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed Expanding Window Mode in {minutes:02d}:{seconds:02d}\")\n"
      ],
      "metadata": {
        "id": "kraj1YkNhEq4"
      },
      "id": "kraj1YkNhEq4",
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seek all"
      ],
      "metadata": {
        "id": "QkeGs9s9dExF"
      },
      "id": "QkeGs9s9dExF"
    },
    {
      "cell_type": "code",
      "source": [
        "if seek_all: #updated\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import time\n",
        "    from itertools import product, combinations\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from datetime import datetime\n",
        "\n",
        "    use_regime_split = False\n",
        "    min_months_train = 60\n",
        "    min_obs_regime = 50\n",
        "    min_obs_train = 0\n",
        "    use_fixed_window = True\n",
        "\n",
        "    min_features = len(FEATURES)\n",
        "    max_features = len(FEATURES)\n",
        "\n",
        "    training_windows_list = [60]\n",
        "\n",
        "    hyperparameter_grid = {\n",
        "        \"n_estimators\": [100],\n",
        "        \"max_depth\": [None],\n",
        "        \"max_features\": ['sqrt'],\n",
        "        \"min_samples_split\": [2],\n",
        "        \"min_samples_leaf\": [1],\n",
        "        \"bootstrap\": [False]\n",
        "    }\n",
        "\n",
        "    hyperparameter_combinations = [\n",
        "        dict(zip(hyperparameter_grid.keys(), values))\n",
        "        for values in product(*hyperparameter_grid.values())\n",
        "    ]\n",
        "\n",
        "    feature_combinations = [\n",
        "        list(combo)\n",
        "        for r in range(min_features, max_features + 1)\n",
        "        for combo in combinations(FEATURES, r)\n",
        "    ]\n",
        "\n",
        "    total_iterations = len(training_windows_list) * len(feature_combinations) * len(hyperparameter_combinations)\n",
        "    print(f\"Total iterations to run: {total_iterations}\")\n",
        "\n",
        "    log_entries = []\n",
        "    iteration_results_log = []\n",
        "    iteration_number = 0\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    for train_window_size in training_windows_list:\n",
        "        for current_features in feature_combinations:\n",
        "            for rf_params in hyperparameter_combinations:\n",
        "                iteration_number += 1\n",
        "                iter_start_time = time.time()\n",
        "                results = []\n",
        "\n",
        "                for i in range(1, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    if use_fixed_window:\n",
        "                        start_idx = max(0, i - train_window_size)\n",
        "                        train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "                    else:\n",
        "                        train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "                    if len(train_window) < min_months_train:\n",
        "                        continue\n",
        "\n",
        "                    train_start_date = train_window['Date'].iloc[0]\n",
        "                    train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        if any(regime_counts < min_obs_regime):\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "                        regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                    else:\n",
        "                        regime_used = 'NoRegime'\n",
        "\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[current_features].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < min_obs_train:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "                        rf_model.fit(X_train, y_train)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    X_test = train_window[current_features].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "                    equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "                    results.append({\n",
        "                        'Predicted_month': Predicted_month,\n",
        "                        'Allocated_Return': allocated_return,\n",
        "                        'Equal_Weight_Return': equal_weight_return,\n",
        "                        'Train_Start_Date': train_start_date,\n",
        "                        'Train_End_Date': train_end_date\n",
        "                    })\n",
        "\n",
        "                results_df = pd.DataFrame(results)\n",
        "                if results_df.empty:\n",
        "                    cum_return_allocated = np.nan\n",
        "                    cum_return_equal = np.nan\n",
        "                    cum_return_allocated_total = np.nan\n",
        "                    cum_return_equal_total = np.nan\n",
        "                    first_pred_date = None\n",
        "                    last_pred_date = None\n",
        "                    winning_months_str = None\n",
        "                    sharpe_ratio = np.nan\n",
        "                    win_months = np.nan\n",
        "                    total_months = np.nan\n",
        "                else:\n",
        "                    # NEW: Actual prediction date range\n",
        "                    first_pred_date = results_df.iloc[0]['Predicted_month']\n",
        "                    last_pred_date = results_df.iloc[-1]['Predicted_month']\n",
        "\n",
        "                    # Performance from 2000 onward\n",
        "                    filtered = results_df[results_df['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "                    if not filtered.empty:\n",
        "                        cum_return_allocated = (1 + filtered['Allocated_Return']).prod() - 1\n",
        "                        cum_return_equal = (1 + filtered['Equal_Weight_Return']).prod() - 1\n",
        "                        win_months = (filtered['Allocated_Return'] > filtered['Equal_Weight_Return']).sum()\n",
        "                        total_months = len(filtered)\n",
        "                        winning_months_str = f\"{win_months}/{total_months}\"\n",
        "                        std_alloc = filtered['Allocated_Return'].std()\n",
        "                        sharpe_ratio = filtered['Allocated_Return'].mean() / std_alloc if std_alloc != 0 else np.nan\n",
        "                    else:\n",
        "                        cum_return_allocated = np.nan\n",
        "                        cum_return_equal = np.nan\n",
        "                        winning_months_str = None\n",
        "                        sharpe_ratio = np.nan\n",
        "                        win_months = np.nan\n",
        "                        total_months = np.nan\n",
        "\n",
        "                    cum_return_allocated_total = (1 + results_df['Allocated_Return']).prod() - 1\n",
        "                    cum_return_equal_total = (1 + results_df['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "                iter_time = time.time() - iter_start_time\n",
        "                time_str = f\"{int(iter_time // 60):02d}:{int(iter_time % 60):02d}\"\n",
        "\n",
        "                iteration_results_log.append({\n",
        "                    'Iteration': iteration_number,\n",
        "                    'Cum_Return_Total': cum_return_allocated_total,\n",
        "                    'Cum_Return_Post2000': cum_return_allocated,\n",
        "                    'Sharpe_Ratio_Post2000': sharpe_ratio,\n",
        "                    'Win_Months_Count': win_months\n",
        "                })\n",
        "\n",
        "                valid_total = [x['Cum_Return_Total'] for x in iteration_results_log if pd.notna(x['Cum_Return_Total'])]\n",
        "                valid_post2000 = [x['Cum_Return_Post2000'] for x in iteration_results_log if pd.notna(x['Cum_Return_Post2000'])]\n",
        "                valid_sharpe = [x['Sharpe_Ratio_Post2000'] for x in iteration_results_log if pd.notna(x['Sharpe_Ratio_Post2000'])]\n",
        "                valid_win = [x['Win_Months_Count'] for x in iteration_results_log if pd.notna(x.get('Win_Months_Count'))]\n",
        "\n",
        "                total_rank = sorted(valid_total, reverse=True).index(cum_return_allocated_total) + 1 if pd.notna(cum_return_allocated_total) else None\n",
        "                post2000_rank = sorted(valid_post2000, reverse=True).index(cum_return_allocated) + 1 if pd.notna(cum_return_allocated) else None\n",
        "                sharpe_rank = sorted(valid_sharpe, reverse=True).index(sharpe_ratio) + 1 if pd.notna(sharpe_ratio) else None\n",
        "                win_months_rank = sorted(valid_win, reverse=True).index(win_months) + 1 if pd.notna(win_months) else None\n",
        "\n",
        "                log_entry = {\n",
        "                    'Iteration': iteration_number,\n",
        "                    'Training_Window': train_window_size,\n",
        "                    'Features': ','.join(current_features),\n",
        "                    'Hyperparameters': ','.join(f\"{k}={v}\" for k, v in rf_params.items()),\n",
        "                    'First_Prediction_Date': first_pred_date,\n",
        "                    'Last_Prediction_Date': last_pred_date,\n",
        "                    'Cumulative_Return_Post2000': cum_return_allocated,\n",
        "                    'Cumulative_Return_Total': cum_return_allocated_total,\n",
        "                    'Cumulative_Return_Equal_Post2000': cum_return_equal,\n",
        "                    'Cumulative_Return_Equal_Total': cum_return_equal_total,\n",
        "                    'Winning_Months': winning_months_str,\n",
        "                    'Winning_Months_Rank': win_months_rank,\n",
        "                    'Sharpe_Ratio_Post2000': sharpe_ratio\n",
        "                }\n",
        "                log_entries.append(log_entry)\n",
        "\n",
        "                # UPDATED: Print first and last prediction dates\n",
        "                print(f\"Iteration {iteration_number}/{total_iterations} | Duration: {time_str}\")\n",
        "                print(f\"Prediction Dates: {first_pred_date.date() if first_pred_date else 'NA'} ‚Üí {last_pred_date.date() if last_pred_date else 'NA'}\")\n",
        "                print(f\"Total Cum Return (Allocated): {cum_return_allocated_total:.4f} | Post-2000: {cum_return_allocated:.4f}\")\n",
        "                print(f\"Total Cum Return (Equal): {cum_return_equal_total:.4f} | Post-2000: {cum_return_equal:.4f}\")\n",
        "                print(f\"Winning Months (Allocated > Equal): {winning_months_str} | Winning Months Rank: {win_months_rank}\")\n",
        "                print(f\"Sharpe Ratio (Post-2000): {sharpe_ratio:.4f} | Sharpe Rank: {sharpe_rank}\")\n",
        "                print(f\"Rank (Total): {total_rank} | Rank (Post-2000): {post2000_rank}\\n\")\n",
        "\n",
        "    log_df = pd.DataFrame(log_entries)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    filename = f\"seek_all_results({timestamp}).csv\"\n",
        "    log_df.to_csv(filename, sep=\";\", index=False)\n",
        "    print(f\"Logged all results to '{filename}'.\")"
      ],
      "metadata": {
        "id": "b_ac9qZ7ir9I"
      },
      "id": "b_ac9qZ7ir9I",
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "MrSJ4xhmDuzE"
      },
      "id": "MrSJ4xhmDuzE"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    RF1_FEATURES = [f for f in FEATURES if f not in ['Cape', 'CPI']]\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Min obs per regime if splitting\n",
        "    min_obs_train = 0         # Min total obs after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # New toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    # -------------------\n",
        "    # New: Tunable hyperparameter for parallel jobs\n",
        "    # -------------------\n",
        "    n_jobs = -1  # Set to -1 to use all available cores; adjust as needed\n",
        "\n",
        "    # -------------------\n",
        "    # Hyperparameter Settings for Random Forest\n",
        "    # -------------------\n",
        "    if default_hyperparameters:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': 'sqrt',\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 1,\n",
        "            'bootstrap': True,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "    else:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': None,\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 5,\n",
        "            'bootstrap': False,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check that we have enough training rows\n",
        "        if len(train_window) < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training rows ({len(train_window)} rows). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  üî¥ Regime split active. Insufficient data in: {', '.join(regime_str_list)}. Skipping.\\n\")\n",
        "                continue\n",
        "\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  üî¥ Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\\n\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure last training date < test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train using RF1_FEATURES\n",
        "        X_train = train_window[RF1_FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Fit RandomForest\n",
        "        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict using the last row in training window\n",
        "        X_test = train_window[RF1_FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\\n\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "        predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "        # Map probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "            try:\n",
        "                idx = FACTORS.index(cls)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # Calculate returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Gather additional info\n",
        "        tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "        avg_depth = np.mean(tree_depths)\n",
        "        max_depth_val = np.max(tree_depths)\n",
        "        months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month))\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in RF1_FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth_val,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build results DataFrame\n",
        "    # -------------------\n",
        "    results_df_rf = pd.DataFrame(results)\n",
        "    print(\"Final results_df_rf columns:\", results_df_rf.columns.tolist())\n",
        "    display(results_df_rf.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # 4) Cumulative Returns from 2000-01-01\n",
        "    # -------------------\n",
        "    filtered_results = results_df_rf[results_df_rf['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered_results.empty:\n",
        "        cum_return_allocated = (1 + filtered_results['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal = (1 + filtered_results['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_pred_month = filtered_results.iloc[0]['Predicted_month']\n",
        "        last_pred_month = filtered_results.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_pred_month.date(), last_pred_month.date(),\n",
        "            cum_return_allocated, cum_return_equal))\n",
        "    else:\n",
        "        print(\"No predictions from 1 Jan 2000 onwards.\")\n",
        "\n",
        "    # -------------------\n",
        "    # 5) Total Time Cumulative Returns\n",
        "    # -------------------\n",
        "    if not results_df_rf.empty:\n",
        "        cum_return_allocated_total = (1 + results_df_rf['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal_total = (1 + results_df_rf['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_total_month = results_df_rf.iloc[0]['Predicted_month']\n",
        "        last_total_month = results_df_rf.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_total_month.date(), last_total_month.date(),\n",
        "            cum_return_allocated_total, cum_return_equal_total))\n",
        "    else:\n",
        "        print(\"No predictions available for total time.\")\n"
      ],
      "metadata": {
        "id": "YjHj_tiKSCYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "23060941-5324-4230-eb8a-42211558fc01"
      },
      "id": "YjHj_tiKSCYZ",
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test row date: 1967-05-30 - Insufficient training rows (1 rows). Skipping.\n",
            "Test row date: 1967-06-30 - Insufficient training rows (2 rows). Skipping.\n",
            "Test row date: 1967-07-30 - Insufficient training rows (3 rows). Skipping.\n",
            "Test row date: 1967-08-30 - Insufficient training rows (4 rows). Skipping.\n",
            "Test row date: 1967-09-30 - Insufficient training rows (5 rows). Skipping.\n",
            "Test row date: 1967-10-30 - Insufficient training rows (6 rows). Skipping.\n",
            "Test row date: 1967-11-30 - Insufficient training rows (7 rows). Skipping.\n",
            "Test row date: 1967-12-30 - Insufficient training rows (8 rows). Skipping.\n",
            "Test row date: 1968-01-30 - Insufficient training rows (9 rows). Skipping.\n",
            "Test row date: 1968-02-29 - Insufficient training rows (10 rows). Skipping.\n",
            "Test row date: 1968-03-30 - Insufficient training rows (11 rows). Skipping.\n",
            "Test row date: 1968-04-30 - Insufficient training rows (12 rows). Skipping.\n",
            "Test row date: 1968-05-30 - Insufficient training rows (13 rows). Skipping.\n",
            "Test row date: 1968-06-30 - Insufficient training rows (14 rows). Skipping.\n",
            "Test row date: 1968-07-30 - Insufficient training rows (15 rows). Skipping.\n",
            "Test row date: 1968-08-30 - Insufficient training rows (16 rows). Skipping.\n",
            "Test row date: 1968-09-30 - Insufficient training rows (17 rows). Skipping.\n",
            "Test row date: 1968-10-30 - Insufficient training rows (18 rows). Skipping.\n",
            "Test row date: 1968-11-30 - Insufficient training rows (19 rows). Skipping.\n",
            "Test row date: 1968-12-30 - Insufficient training rows (20 rows). Skipping.\n",
            "Test row date: 1969-01-30 - Insufficient training rows (21 rows). Skipping.\n",
            "Test row date: 1969-02-28 - Insufficient training rows (22 rows). Skipping.\n",
            "Test row date: 1969-03-30 - Insufficient training rows (23 rows). Skipping.\n",
            "Test row date: 1969-04-30 - Insufficient training rows (24 rows). Skipping.\n",
            "Test row date: 1969-05-30 - Insufficient training rows (25 rows). Skipping.\n",
            "Test row date: 1969-06-30 - Insufficient training rows (26 rows). Skipping.\n",
            "Test row date: 1969-07-30 - Insufficient training rows (27 rows). Skipping.\n",
            "Test row date: 1969-08-30 - Insufficient training rows (28 rows). Skipping.\n",
            "Test row date: 1969-09-30 - Insufficient training rows (29 rows). Skipping.\n",
            "Test row date: 1969-10-30 - Insufficient training rows (30 rows). Skipping.\n",
            "Test row date: 1969-11-30 - Insufficient training rows (31 rows). Skipping.\n",
            "Test row date: 1969-12-30 - Insufficient training rows (32 rows). Skipping.\n",
            "Test row date: 1970-01-30 - Insufficient training rows (33 rows). Skipping.\n",
            "Test row date: 1970-02-28 - Insufficient training rows (34 rows). Skipping.\n",
            "Test row date: 1970-03-30 - Insufficient training rows (35 rows). Skipping.\n",
            "Test row date: 1970-04-30 - Insufficient training rows (36 rows). Skipping.\n",
            "Test row date: 1970-05-30 - Insufficient training rows (37 rows). Skipping.\n",
            "Test row date: 1970-06-30 - Insufficient training rows (38 rows). Skipping.\n",
            "Test row date: 1970-07-30 - Insufficient training rows (39 rows). Skipping.\n",
            "Test row date: 1970-08-30 - Insufficient training rows (40 rows). Skipping.\n",
            "Test row date: 1970-09-30 - Insufficient training rows (41 rows). Skipping.\n",
            "Test row date: 1970-10-30 - Insufficient training rows (42 rows). Skipping.\n",
            "Test row date: 1970-11-30 - Insufficient training rows (43 rows). Skipping.\n",
            "Test row date: 1970-12-30 - Insufficient training rows (44 rows). Skipping.\n",
            "Test row date: 1971-01-30 - Insufficient training rows (45 rows). Skipping.\n",
            "Test row date: 1971-02-28 - Insufficient training rows (46 rows). Skipping.\n",
            "Test row date: 1971-03-30 - Insufficient training rows (47 rows). Skipping.\n",
            "Test row date: 1971-04-30 - Insufficient training rows (48 rows). Skipping.\n",
            "Test row date: 1971-05-30 - Insufficient training rows (49 rows). Skipping.\n",
            "Test row date: 1971-06-30 - Insufficient training rows (50 rows). Skipping.\n",
            "Test row date: 1971-07-30 - Insufficient training rows (51 rows). Skipping.\n",
            "Test row date: 1971-08-30 - Insufficient training rows (52 rows). Skipping.\n",
            "Test row date: 1971-09-30 - Insufficient training rows (53 rows). Skipping.\n",
            "Test row date: 1971-10-30 - Insufficient training rows (54 rows). Skipping.\n",
            "Test row date: 1971-11-30 - Insufficient training rows (55 rows). Skipping.\n",
            "Test row date: 1971-12-30 - Insufficient training rows (56 rows). Skipping.\n",
            "Test row date: 1972-01-30 - Insufficient training rows (57 rows). Skipping.\n",
            "Test row date: 1972-02-29 - Insufficient training rows (58 rows). Skipping.\n",
            "Test row date: 1972-03-30 - Insufficient training rows (59 rows). Skipping.\n",
            "Test row date: 1972-04-30 -> Model trained, prediction made (using: 1967-04-30 - 1972-03-30)\n",
            "Test row date: 1972-05-30 -> Model trained, prediction made (using: 1967-05-30 - 1972-04-30)\n",
            "Test row date: 1972-06-30 -> Model trained, prediction made (using: 1967-06-30 - 1972-05-30)\n",
            "Test row date: 1972-07-30 -> Model trained, prediction made (using: 1967-07-30 - 1972-06-30)\n",
            "Test row date: 1972-08-30 -> Model trained, prediction made (using: 1967-08-30 - 1972-07-30)\n",
            "Test row date: 1972-09-30 -> Model trained, prediction made (using: 1967-09-30 - 1972-08-30)\n",
            "Test row date: 1972-10-30 -> Model trained, prediction made (using: 1967-10-30 - 1972-09-30)\n",
            "Test row date: 1972-11-30 -> Model trained, prediction made (using: 1967-11-30 - 1972-10-30)\n",
            "Test row date: 1972-12-30 -> Model trained, prediction made (using: 1967-12-30 - 1972-11-30)\n",
            "Test row date: 1973-01-30 -> Model trained, prediction made (using: 1968-01-30 - 1972-12-30)\n",
            "Test row date: 1973-02-28 -> Model trained, prediction made (using: 1968-02-29 - 1973-01-30)\n",
            "Test row date: 1973-03-30 -> Model trained, prediction made (using: 1968-03-30 - 1973-02-28)\n",
            "Test row date: 1973-04-30 -> Model trained, prediction made (using: 1968-04-30 - 1973-03-30)\n",
            "Test row date: 1973-05-30 -> Model trained, prediction made (using: 1968-05-30 - 1973-04-30)\n",
            "Test row date: 1973-06-30 -> Model trained, prediction made (using: 1968-06-30 - 1973-05-30)\n",
            "Test row date: 1973-07-30 -> Model trained, prediction made (using: 1968-07-30 - 1973-06-30)\n",
            "Test row date: 1973-08-30 -> Model trained, prediction made (using: 1968-08-30 - 1973-07-30)\n",
            "Test row date: 1973-09-30 -> Model trained, prediction made (using: 1968-09-30 - 1973-08-30)\n",
            "Test row date: 1973-10-30 -> Model trained, prediction made (using: 1968-10-30 - 1973-09-30)\n",
            "Test row date: 1973-11-30 -> Model trained, prediction made (using: 1968-11-30 - 1973-10-30)\n",
            "Test row date: 1973-12-30 -> Model trained, prediction made (using: 1968-12-30 - 1973-11-30)\n",
            "Test row date: 1974-01-30 -> Model trained, prediction made (using: 1969-01-30 - 1973-12-30)\n",
            "Test row date: 1974-02-28 -> Model trained, prediction made (using: 1969-02-28 - 1974-01-30)\n",
            "Test row date: 1974-03-30 -> Model trained, prediction made (using: 1969-03-30 - 1974-02-28)\n",
            "Test row date: 1974-04-30 -> Model trained, prediction made (using: 1969-04-30 - 1974-03-30)\n",
            "Test row date: 1974-05-30 -> Model trained, prediction made (using: 1969-05-30 - 1974-04-30)\n",
            "Test row date: 1974-06-30 -> Model trained, prediction made (using: 1969-06-30 - 1974-05-30)\n",
            "Test row date: 1974-07-30 -> Model trained, prediction made (using: 1969-07-30 - 1974-06-30)\n",
            "Test row date: 1974-08-30 -> Model trained, prediction made (using: 1969-08-30 - 1974-07-30)\n",
            "Test row date: 1974-09-30 -> Model trained, prediction made (using: 1969-09-30 - 1974-08-30)\n",
            "Test row date: 1974-10-30 -> Model trained, prediction made (using: 1969-10-30 - 1974-09-30)\n",
            "Test row date: 1974-11-30 -> Model trained, prediction made (using: 1969-11-30 - 1974-10-30)\n",
            "Test row date: 1974-12-30 -> Model trained, prediction made (using: 1969-12-30 - 1974-11-30)\n",
            "Test row date: 1975-01-30 -> Model trained, prediction made (using: 1970-01-30 - 1974-12-30)\n",
            "Test row date: 1975-02-28 -> Model trained, prediction made (using: 1970-02-28 - 1975-01-30)\n",
            "Test row date: 1975-03-30 -> Model trained, prediction made (using: 1970-03-30 - 1975-02-28)\n",
            "Test row date: 1975-04-30 -> Model trained, prediction made (using: 1970-04-30 - 1975-03-30)\n",
            "Test row date: 1975-05-30 -> Model trained, prediction made (using: 1970-05-30 - 1975-04-30)\n",
            "Test row date: 1975-06-30 -> Model trained, prediction made (using: 1970-06-30 - 1975-05-30)\n",
            "Test row date: 1975-07-30 -> Model trained, prediction made (using: 1970-07-30 - 1975-06-30)\n",
            "Test row date: 1975-08-30 -> Model trained, prediction made (using: 1970-08-30 - 1975-07-30)\n",
            "Test row date: 1975-09-30 -> Model trained, prediction made (using: 1970-09-30 - 1975-08-30)\n",
            "Test row date: 1975-10-30 -> Model trained, prediction made (using: 1970-10-30 - 1975-09-30)\n",
            "Test row date: 1975-11-30 -> Model trained, prediction made (using: 1970-11-30 - 1975-10-30)\n",
            "Test row date: 1975-12-30 -> Model trained, prediction made (using: 1970-12-30 - 1975-11-30)\n",
            "Test row date: 1976-01-30 -> Model trained, prediction made (using: 1971-01-30 - 1975-12-30)\n",
            "Test row date: 1976-02-29 -> Model trained, prediction made (using: 1971-02-28 - 1976-01-30)\n",
            "Test row date: 1976-03-30 -> Model trained, prediction made (using: 1971-03-30 - 1976-02-29)\n",
            "Test row date: 1976-04-30 -> Model trained, prediction made (using: 1971-04-30 - 1976-03-30)\n",
            "Test row date: 1976-05-30 -> Model trained, prediction made (using: 1971-05-30 - 1976-04-30)\n",
            "Test row date: 1976-06-30 -> Model trained, prediction made (using: 1971-06-30 - 1976-05-30)\n",
            "Test row date: 1976-07-30 -> Model trained, prediction made (using: 1971-07-30 - 1976-06-30)\n",
            "Test row date: 1976-08-30 -> Model trained, prediction made (using: 1971-08-30 - 1976-07-30)\n",
            "Test row date: 1976-09-30 -> Model trained, prediction made (using: 1971-09-30 - 1976-08-30)\n",
            "Test row date: 1976-10-30 -> Model trained, prediction made (using: 1971-10-30 - 1976-09-30)\n",
            "Test row date: 1976-11-30 -> Model trained, prediction made (using: 1971-11-30 - 1976-10-30)\n",
            "Test row date: 1976-12-30 -> Model trained, prediction made (using: 1971-12-30 - 1976-11-30)\n",
            "Test row date: 1977-01-30 -> Model trained, prediction made (using: 1972-01-30 - 1976-12-30)\n",
            "Test row date: 1977-02-28 -> Model trained, prediction made (using: 1972-02-29 - 1977-01-30)\n",
            "Test row date: 1977-03-30 -> Model trained, prediction made (using: 1972-03-30 - 1977-02-28)\n",
            "Test row date: 1977-04-30 -> Model trained, prediction made (using: 1972-04-30 - 1977-03-30)\n",
            "Test row date: 1977-05-30 -> Model trained, prediction made (using: 1972-05-30 - 1977-04-30)\n",
            "Test row date: 1977-06-30 -> Model trained, prediction made (using: 1972-06-30 - 1977-05-30)\n",
            "Test row date: 1977-07-30 -> Model trained, prediction made (using: 1972-07-30 - 1977-06-30)\n",
            "Test row date: 1977-08-30 -> Model trained, prediction made (using: 1972-08-30 - 1977-07-30)\n",
            "Test row date: 1977-09-30 -> Model trained, prediction made (using: 1972-09-30 - 1977-08-30)\n",
            "Test row date: 1977-10-30 -> Model trained, prediction made (using: 1972-10-30 - 1977-09-30)\n",
            "Test row date: 1977-11-30 -> Model trained, prediction made (using: 1972-11-30 - 1977-10-30)\n",
            "Test row date: 1977-12-30 -> Model trained, prediction made (using: 1972-12-30 - 1977-11-30)\n",
            "Test row date: 1978-01-30 -> Model trained, prediction made (using: 1973-01-30 - 1977-12-30)\n",
            "Test row date: 1978-02-28 -> Model trained, prediction made (using: 1973-02-28 - 1978-01-30)\n",
            "Test row date: 1978-03-30 -> Model trained, prediction made (using: 1973-03-30 - 1978-02-28)\n",
            "Test row date: 1978-04-30 -> Model trained, prediction made (using: 1973-04-30 - 1978-03-30)\n",
            "Test row date: 1978-05-30 -> Model trained, prediction made (using: 1973-05-30 - 1978-04-30)\n",
            "Test row date: 1978-06-30 -> Model trained, prediction made (using: 1973-06-30 - 1978-05-30)\n",
            "Test row date: 1978-07-30 -> Model trained, prediction made (using: 1973-07-30 - 1978-06-30)\n",
            "Test row date: 1978-08-30 -> Model trained, prediction made (using: 1973-08-30 - 1978-07-30)\n",
            "Test row date: 1978-09-30 -> Model trained, prediction made (using: 1973-09-30 - 1978-08-30)\n",
            "Test row date: 1978-10-30 -> Model trained, prediction made (using: 1973-10-30 - 1978-09-30)\n",
            "Test row date: 1978-11-30 -> Model trained, prediction made (using: 1973-11-30 - 1978-10-30)\n",
            "Test row date: 1978-12-30 -> Model trained, prediction made (using: 1973-12-30 - 1978-11-30)\n",
            "Test row date: 1979-01-30 -> Model trained, prediction made (using: 1974-01-30 - 1978-12-30)\n",
            "Test row date: 1979-02-28 -> Model trained, prediction made (using: 1974-02-28 - 1979-01-30)\n",
            "Test row date: 1979-03-30 -> Model trained, prediction made (using: 1974-03-30 - 1979-02-28)\n",
            "Test row date: 1979-04-30 -> Model trained, prediction made (using: 1974-04-30 - 1979-03-30)\n",
            "Test row date: 1979-05-30 -> Model trained, prediction made (using: 1974-05-30 - 1979-04-30)\n",
            "Test row date: 1979-06-30 -> Model trained, prediction made (using: 1974-06-30 - 1979-05-30)\n",
            "Test row date: 1979-07-30 -> Model trained, prediction made (using: 1974-07-30 - 1979-06-30)\n",
            "Test row date: 1979-08-30 -> Model trained, prediction made (using: 1974-08-30 - 1979-07-30)\n",
            "Test row date: 1979-09-30 -> Model trained, prediction made (using: 1974-09-30 - 1979-08-30)\n",
            "Test row date: 1979-10-30 -> Model trained, prediction made (using: 1974-10-30 - 1979-09-30)\n",
            "Test row date: 1979-11-30 -> Model trained, prediction made (using: 1974-11-30 - 1979-10-30)\n",
            "Test row date: 1979-12-30 -> Model trained, prediction made (using: 1974-12-30 - 1979-11-30)\n",
            "Test row date: 1980-01-30 -> Model trained, prediction made (using: 1975-01-30 - 1979-12-30)\n",
            "Test row date: 1980-02-29 -> Model trained, prediction made (using: 1975-02-28 - 1980-01-30)\n",
            "Test row date: 1980-03-30 -> Model trained, prediction made (using: 1975-03-30 - 1980-02-29)\n",
            "Test row date: 1980-04-30 -> Model trained, prediction made (using: 1975-04-30 - 1980-03-30)\n",
            "Test row date: 1980-05-30 -> Model trained, prediction made (using: 1975-05-30 - 1980-04-30)\n",
            "Test row date: 1980-06-30 -> Model trained, prediction made (using: 1975-06-30 - 1980-05-30)\n",
            "Test row date: 1980-07-30 -> Model trained, prediction made (using: 1975-07-30 - 1980-06-30)\n",
            "Test row date: 1980-08-30 -> Model trained, prediction made (using: 1975-08-30 - 1980-07-30)\n",
            "Test row date: 1980-09-30 -> Model trained, prediction made (using: 1975-09-30 - 1980-08-30)\n",
            "Test row date: 1980-10-30 -> Model trained, prediction made (using: 1975-10-30 - 1980-09-30)\n",
            "Test row date: 1980-11-30 -> Model trained, prediction made (using: 1975-11-30 - 1980-10-30)\n",
            "Test row date: 1980-12-30 -> Model trained, prediction made (using: 1975-12-30 - 1980-11-30)\n",
            "Test row date: 1981-01-30 -> Model trained, prediction made (using: 1976-01-30 - 1980-12-30)\n",
            "Test row date: 1981-02-28 -> Model trained, prediction made (using: 1976-02-29 - 1981-01-30)\n",
            "Test row date: 1981-03-30 -> Model trained, prediction made (using: 1976-03-30 - 1981-02-28)\n",
            "Test row date: 1981-04-30 -> Model trained, prediction made (using: 1976-04-30 - 1981-03-30)\n",
            "Test row date: 1981-05-30 -> Model trained, prediction made (using: 1976-05-30 - 1981-04-30)\n",
            "Test row date: 1981-06-30 -> Model trained, prediction made (using: 1976-06-30 - 1981-05-30)\n",
            "Test row date: 1981-07-30 -> Model trained, prediction made (using: 1976-07-30 - 1981-06-30)\n",
            "Test row date: 1981-08-30 -> Model trained, prediction made (using: 1976-08-30 - 1981-07-30)\n",
            "Test row date: 1981-09-30 -> Model trained, prediction made (using: 1976-09-30 - 1981-08-30)\n",
            "Test row date: 1981-10-30 -> Model trained, prediction made (using: 1976-10-30 - 1981-09-30)\n",
            "Test row date: 1981-11-30 -> Model trained, prediction made (using: 1976-11-30 - 1981-10-30)\n",
            "Test row date: 1981-12-30 -> Model trained, prediction made (using: 1976-12-30 - 1981-11-30)\n",
            "Test row date: 1982-01-30 -> Model trained, prediction made (using: 1977-01-30 - 1981-12-30)\n",
            "Test row date: 1982-02-28 -> Model trained, prediction made (using: 1977-02-28 - 1982-01-30)\n",
            "Test row date: 1982-03-30 -> Model trained, prediction made (using: 1977-03-30 - 1982-02-28)\n",
            "Test row date: 1982-04-30 -> Model trained, prediction made (using: 1977-04-30 - 1982-03-30)\n",
            "Test row date: 1982-05-30 -> Model trained, prediction made (using: 1977-05-30 - 1982-04-30)\n",
            "Test row date: 1982-06-30 -> Model trained, prediction made (using: 1977-06-30 - 1982-05-30)\n",
            "Test row date: 1982-07-30 -> Model trained, prediction made (using: 1977-07-30 - 1982-06-30)\n",
            "Test row date: 1982-08-30 -> Model trained, prediction made (using: 1977-08-30 - 1982-07-30)\n",
            "Test row date: 1982-09-30 -> Model trained, prediction made (using: 1977-09-30 - 1982-08-30)\n",
            "Test row date: 1982-10-30 -> Model trained, prediction made (using: 1977-10-30 - 1982-09-30)\n",
            "Test row date: 1982-11-30 -> Model trained, prediction made (using: 1977-11-30 - 1982-10-30)\n",
            "Test row date: 1982-12-30 -> Model trained, prediction made (using: 1977-12-30 - 1982-11-30)\n",
            "Test row date: 1983-01-30 -> Model trained, prediction made (using: 1978-01-30 - 1982-12-30)\n",
            "Test row date: 1983-02-28 -> Model trained, prediction made (using: 1978-02-28 - 1983-01-30)\n",
            "Test row date: 1983-03-30 -> Model trained, prediction made (using: 1978-03-30 - 1983-02-28)\n",
            "Test row date: 1983-04-30 -> Model trained, prediction made (using: 1978-04-30 - 1983-03-30)\n",
            "Test row date: 1983-05-30 -> Model trained, prediction made (using: 1978-05-30 - 1983-04-30)\n",
            "Test row date: 1983-06-30 -> Model trained, prediction made (using: 1978-06-30 - 1983-05-30)\n",
            "Test row date: 1983-07-30 -> Model trained, prediction made (using: 1978-07-30 - 1983-06-30)\n",
            "Test row date: 1983-08-30 -> Model trained, prediction made (using: 1978-08-30 - 1983-07-30)\n",
            "Test row date: 1983-09-30 -> Model trained, prediction made (using: 1978-09-30 - 1983-08-30)\n",
            "Test row date: 1983-10-30 -> Model trained, prediction made (using: 1978-10-30 - 1983-09-30)\n",
            "Test row date: 1983-11-30 -> Model trained, prediction made (using: 1978-11-30 - 1983-10-30)\n",
            "Test row date: 1983-12-30 -> Model trained, prediction made (using: 1978-12-30 - 1983-11-30)\n",
            "Test row date: 1984-01-30 -> Model trained, prediction made (using: 1979-01-30 - 1983-12-30)\n",
            "Test row date: 1984-02-29 -> Model trained, prediction made (using: 1979-02-28 - 1984-01-30)\n",
            "Test row date: 1984-03-30 -> Model trained, prediction made (using: 1979-03-30 - 1984-02-29)\n",
            "Test row date: 1984-04-30 -> Model trained, prediction made (using: 1979-04-30 - 1984-03-30)\n",
            "Test row date: 1984-05-30 -> Model trained, prediction made (using: 1979-05-30 - 1984-04-30)\n",
            "Test row date: 1984-06-30 -> Model trained, prediction made (using: 1979-06-30 - 1984-05-30)\n",
            "Test row date: 1984-07-30 -> Model trained, prediction made (using: 1979-07-30 - 1984-06-30)\n",
            "Test row date: 1984-08-30 -> Model trained, prediction made (using: 1979-08-30 - 1984-07-30)\n",
            "Test row date: 1984-09-30 -> Model trained, prediction made (using: 1979-09-30 - 1984-08-30)\n",
            "Test row date: 1984-10-30 -> Model trained, prediction made (using: 1979-10-30 - 1984-09-30)\n",
            "Test row date: 1984-11-30 -> Model trained, prediction made (using: 1979-11-30 - 1984-10-30)\n",
            "Test row date: 1984-12-30 -> Model trained, prediction made (using: 1979-12-30 - 1984-11-30)\n",
            "Test row date: 1985-01-30 -> Model trained, prediction made (using: 1980-01-30 - 1984-12-30)\n",
            "Test row date: 1985-02-28 -> Model trained, prediction made (using: 1980-02-29 - 1985-01-30)\n",
            "Test row date: 1985-03-30 -> Model trained, prediction made (using: 1980-03-30 - 1985-02-28)\n",
            "Test row date: 1985-04-30 -> Model trained, prediction made (using: 1980-04-30 - 1985-03-30)\n",
            "Test row date: 1985-05-30 -> Model trained, prediction made (using: 1980-05-30 - 1985-04-30)\n",
            "Test row date: 1985-06-30 -> Model trained, prediction made (using: 1980-06-30 - 1985-05-30)\n",
            "Test row date: 1985-07-30 -> Model trained, prediction made (using: 1980-07-30 - 1985-06-30)\n",
            "Test row date: 1985-08-30 -> Model trained, prediction made (using: 1980-08-30 - 1985-07-30)\n",
            "Test row date: 1985-09-30 -> Model trained, prediction made (using: 1980-09-30 - 1985-08-30)\n",
            "Test row date: 1985-10-30 -> Model trained, prediction made (using: 1980-10-30 - 1985-09-30)\n",
            "Test row date: 1985-11-30 -> Model trained, prediction made (using: 1980-11-30 - 1985-10-30)\n",
            "Test row date: 1985-12-30 -> Model trained, prediction made (using: 1980-12-30 - 1985-11-30)\n",
            "Test row date: 1986-01-30 -> Model trained, prediction made (using: 1981-01-30 - 1985-12-30)\n",
            "Test row date: 1986-02-28 -> Model trained, prediction made (using: 1981-02-28 - 1986-01-30)\n",
            "Test row date: 1986-03-30 -> Model trained, prediction made (using: 1981-03-30 - 1986-02-28)\n",
            "Test row date: 1986-04-30 -> Model trained, prediction made (using: 1981-04-30 - 1986-03-30)\n",
            "Test row date: 1986-05-30 -> Model trained, prediction made (using: 1981-05-30 - 1986-04-30)\n",
            "Test row date: 1986-06-30 -> Model trained, prediction made (using: 1981-06-30 - 1986-05-30)\n",
            "Test row date: 1986-07-30 -> Model trained, prediction made (using: 1981-07-30 - 1986-06-30)\n",
            "Test row date: 1986-08-30 -> Model trained, prediction made (using: 1981-08-30 - 1986-07-30)\n",
            "Test row date: 1986-09-30 -> Model trained, prediction made (using: 1981-09-30 - 1986-08-30)\n",
            "Test row date: 1986-10-30 -> Model trained, prediction made (using: 1981-10-30 - 1986-09-30)\n",
            "Test row date: 1986-11-30 -> Model trained, prediction made (using: 1981-11-30 - 1986-10-30)\n",
            "Test row date: 1986-12-30 -> Model trained, prediction made (using: 1981-12-30 - 1986-11-30)\n",
            "Test row date: 1987-01-30 -> Model trained, prediction made (using: 1982-01-30 - 1986-12-30)\n",
            "Test row date: 1987-02-28 -> Model trained, prediction made (using: 1982-02-28 - 1987-01-30)\n",
            "Test row date: 1987-03-30 -> Model trained, prediction made (using: 1982-03-30 - 1987-02-28)\n",
            "Test row date: 1987-04-30 -> Model trained, prediction made (using: 1982-04-30 - 1987-03-30)\n",
            "Test row date: 1987-05-30 -> Model trained, prediction made (using: 1982-05-30 - 1987-04-30)\n",
            "Test row date: 1987-06-30 -> Model trained, prediction made (using: 1982-06-30 - 1987-05-30)\n",
            "Test row date: 1987-07-30 -> Model trained, prediction made (using: 1982-07-30 - 1987-06-30)\n",
            "Test row date: 1987-08-30 -> Model trained, prediction made (using: 1982-08-30 - 1987-07-30)\n",
            "Test row date: 1987-09-30 -> Model trained, prediction made (using: 1982-09-30 - 1987-08-30)\n",
            "Test row date: 1987-10-30 -> Model trained, prediction made (using: 1982-10-30 - 1987-09-30)\n",
            "Test row date: 1987-11-30 -> Model trained, prediction made (using: 1982-11-30 - 1987-10-30)\n",
            "Test row date: 1987-12-30 -> Model trained, prediction made (using: 1982-12-30 - 1987-11-30)\n",
            "Test row date: 1988-01-30 -> Model trained, prediction made (using: 1983-01-30 - 1987-12-30)\n",
            "Test row date: 1988-02-29 -> Model trained, prediction made (using: 1983-02-28 - 1988-01-30)\n",
            "Test row date: 1988-03-30 -> Model trained, prediction made (using: 1983-03-30 - 1988-02-29)\n",
            "Test row date: 1988-04-30 -> Model trained, prediction made (using: 1983-04-30 - 1988-03-30)\n",
            "Test row date: 1988-05-30 -> Model trained, prediction made (using: 1983-05-30 - 1988-04-30)\n",
            "Test row date: 1988-06-30 -> Model trained, prediction made (using: 1983-06-30 - 1988-05-30)\n",
            "Test row date: 1988-07-30 -> Model trained, prediction made (using: 1983-07-30 - 1988-06-30)\n",
            "Test row date: 1988-08-30 -> Model trained, prediction made (using: 1983-08-30 - 1988-07-30)\n",
            "Test row date: 1988-09-30 -> Model trained, prediction made (using: 1983-09-30 - 1988-08-30)\n",
            "Test row date: 1988-10-30 -> Model trained, prediction made (using: 1983-10-30 - 1988-09-30)\n",
            "Test row date: 1988-11-30 -> Model trained, prediction made (using: 1983-11-30 - 1988-10-30)\n",
            "Test row date: 1988-12-30 -> Model trained, prediction made (using: 1983-12-30 - 1988-11-30)\n",
            "Test row date: 1989-01-30 -> Model trained, prediction made (using: 1984-01-30 - 1988-12-30)\n",
            "Test row date: 1989-02-28 -> Model trained, prediction made (using: 1984-02-29 - 1989-01-30)\n",
            "Test row date: 1989-03-30 -> Model trained, prediction made (using: 1984-03-30 - 1989-02-28)\n",
            "Test row date: 1989-04-30 -> Model trained, prediction made (using: 1984-04-30 - 1989-03-30)\n",
            "Test row date: 1989-05-30 -> Model trained, prediction made (using: 1984-05-30 - 1989-04-30)\n",
            "Test row date: 1989-06-30 -> Model trained, prediction made (using: 1984-06-30 - 1989-05-30)\n",
            "Test row date: 1989-07-30 -> Model trained, prediction made (using: 1984-07-30 - 1989-06-30)\n",
            "Test row date: 1989-08-30 -> Model trained, prediction made (using: 1984-08-30 - 1989-07-30)\n",
            "Test row date: 1989-09-30 -> Model trained, prediction made (using: 1984-09-30 - 1989-08-30)\n",
            "Test row date: 1989-10-30 -> Model trained, prediction made (using: 1984-10-30 - 1989-09-30)\n",
            "Test row date: 1989-11-30 -> Model trained, prediction made (using: 1984-11-30 - 1989-10-30)\n",
            "Test row date: 1989-12-30 -> Model trained, prediction made (using: 1984-12-30 - 1989-11-30)\n",
            "Test row date: 1990-01-30 -> Model trained, prediction made (using: 1985-01-30 - 1989-12-30)\n",
            "Test row date: 1990-02-28 -> Model trained, prediction made (using: 1985-02-28 - 1990-01-30)\n",
            "Test row date: 1990-03-30 -> Model trained, prediction made (using: 1985-03-30 - 1990-02-28)\n",
            "Test row date: 1990-04-30 -> Model trained, prediction made (using: 1985-04-30 - 1990-03-30)\n",
            "Test row date: 1990-05-30 -> Model trained, prediction made (using: 1985-05-30 - 1990-04-30)\n",
            "Test row date: 1990-06-30 -> Model trained, prediction made (using: 1985-06-30 - 1990-05-30)\n",
            "Test row date: 1990-07-30 -> Model trained, prediction made (using: 1985-07-30 - 1990-06-30)\n",
            "Test row date: 1990-08-30 -> Model trained, prediction made (using: 1985-08-30 - 1990-07-30)\n",
            "Test row date: 1990-09-30 -> Model trained, prediction made (using: 1985-09-30 - 1990-08-30)\n",
            "Test row date: 1990-10-30 -> Model trained, prediction made (using: 1985-10-30 - 1990-09-30)\n",
            "Test row date: 1990-11-30 -> Model trained, prediction made (using: 1985-11-30 - 1990-10-30)\n",
            "Test row date: 1990-12-30 -> Model trained, prediction made (using: 1985-12-30 - 1990-11-30)\n",
            "Test row date: 1991-01-30 -> Model trained, prediction made (using: 1986-01-30 - 1990-12-30)\n",
            "Test row date: 1991-02-28 -> Model trained, prediction made (using: 1986-02-28 - 1991-01-30)\n",
            "Test row date: 1991-03-30 -> Model trained, prediction made (using: 1986-03-30 - 1991-02-28)\n",
            "Test row date: 1991-04-30 -> Model trained, prediction made (using: 1986-04-30 - 1991-03-30)\n",
            "Test row date: 1991-05-30 -> Model trained, prediction made (using: 1986-05-30 - 1991-04-30)\n",
            "Test row date: 1991-06-30 -> Model trained, prediction made (using: 1986-06-30 - 1991-05-30)\n",
            "Test row date: 1991-07-30 -> Model trained, prediction made (using: 1986-07-30 - 1991-06-30)\n",
            "Test row date: 1991-08-30 -> Model trained, prediction made (using: 1986-08-30 - 1991-07-30)\n",
            "Test row date: 1991-09-30 -> Model trained, prediction made (using: 1986-09-30 - 1991-08-30)\n",
            "Test row date: 1991-10-30 -> Model trained, prediction made (using: 1986-10-30 - 1991-09-30)\n",
            "Test row date: 1991-11-30 -> Model trained, prediction made (using: 1986-11-30 - 1991-10-30)\n",
            "Test row date: 1991-12-30 -> Model trained, prediction made (using: 1986-12-30 - 1991-11-30)\n",
            "Test row date: 1992-01-30 -> Model trained, prediction made (using: 1987-01-30 - 1991-12-30)\n",
            "Test row date: 1992-02-29 -> Model trained, prediction made (using: 1987-02-28 - 1992-01-30)\n",
            "Test row date: 1992-03-30 -> Model trained, prediction made (using: 1987-03-30 - 1992-02-29)\n",
            "Test row date: 1992-04-30 -> Model trained, prediction made (using: 1987-04-30 - 1992-03-30)\n",
            "Test row date: 1992-05-30 -> Model trained, prediction made (using: 1987-05-30 - 1992-04-30)\n",
            "Test row date: 1992-06-30 -> Model trained, prediction made (using: 1987-06-30 - 1992-05-30)\n",
            "Test row date: 1992-07-30 -> Model trained, prediction made (using: 1987-07-30 - 1992-06-30)\n",
            "Test row date: 1992-08-30 -> Model trained, prediction made (using: 1987-08-30 - 1992-07-30)\n",
            "Test row date: 1992-09-30 -> Model trained, prediction made (using: 1987-09-30 - 1992-08-30)\n",
            "Test row date: 1992-10-30 -> Model trained, prediction made (using: 1987-10-30 - 1992-09-30)\n",
            "Test row date: 1992-11-30 -> Model trained, prediction made (using: 1987-11-30 - 1992-10-30)\n",
            "Test row date: 1992-12-30 -> Model trained, prediction made (using: 1987-12-30 - 1992-11-30)\n",
            "Test row date: 1993-01-30 -> Model trained, prediction made (using: 1988-01-30 - 1992-12-30)\n",
            "Test row date: 1993-02-28 -> Model trained, prediction made (using: 1988-02-29 - 1993-01-30)\n",
            "Test row date: 1993-03-30 -> Model trained, prediction made (using: 1988-03-30 - 1993-02-28)\n",
            "Test row date: 1993-04-30 -> Model trained, prediction made (using: 1988-04-30 - 1993-03-30)\n",
            "Test row date: 1993-05-30 -> Model trained, prediction made (using: 1988-05-30 - 1993-04-30)\n",
            "Test row date: 1993-06-30 -> Model trained, prediction made (using: 1988-06-30 - 1993-05-30)\n",
            "Test row date: 1993-07-30 -> Model trained, prediction made (using: 1988-07-30 - 1993-06-30)\n",
            "Test row date: 1993-08-30 -> Model trained, prediction made (using: 1988-08-30 - 1993-07-30)\n",
            "Test row date: 1993-09-30 -> Model trained, prediction made (using: 1988-09-30 - 1993-08-30)\n",
            "Test row date: 1993-10-30 -> Model trained, prediction made (using: 1988-10-30 - 1993-09-30)\n",
            "Test row date: 1993-11-30 -> Model trained, prediction made (using: 1988-11-30 - 1993-10-30)\n",
            "Test row date: 1993-12-30 -> Model trained, prediction made (using: 1988-12-30 - 1993-11-30)\n",
            "Test row date: 1994-01-30 -> Model trained, prediction made (using: 1989-01-30 - 1993-12-30)\n",
            "Test row date: 1994-02-28 -> Model trained, prediction made (using: 1989-02-28 - 1994-01-30)\n",
            "Test row date: 1994-03-30 -> Model trained, prediction made (using: 1989-03-30 - 1994-02-28)\n",
            "Test row date: 1994-04-30 -> Model trained, prediction made (using: 1989-04-30 - 1994-03-30)\n",
            "Test row date: 1994-05-30 -> Model trained, prediction made (using: 1989-05-30 - 1994-04-30)\n",
            "Test row date: 1994-06-30 -> Model trained, prediction made (using: 1989-06-30 - 1994-05-30)\n",
            "Test row date: 1994-07-30 -> Model trained, prediction made (using: 1989-07-30 - 1994-06-30)\n",
            "Test row date: 1994-08-30 -> Model trained, prediction made (using: 1989-08-30 - 1994-07-30)\n",
            "Test row date: 1994-09-30 -> Model trained, prediction made (using: 1989-09-30 - 1994-08-30)\n",
            "Test row date: 1994-10-30 -> Model trained, prediction made (using: 1989-10-30 - 1994-09-30)\n",
            "Test row date: 1994-11-30 -> Model trained, prediction made (using: 1989-11-30 - 1994-10-30)\n",
            "Test row date: 1994-12-30 -> Model trained, prediction made (using: 1989-12-30 - 1994-11-30)\n",
            "Test row date: 1995-01-30 -> Model trained, prediction made (using: 1990-01-30 - 1994-12-30)\n",
            "Test row date: 1995-02-28 -> Model trained, prediction made (using: 1990-02-28 - 1995-01-30)\n",
            "Test row date: 1995-03-30 -> Model trained, prediction made (using: 1990-03-30 - 1995-02-28)\n",
            "Test row date: 1995-04-30 -> Model trained, prediction made (using: 1990-04-30 - 1995-03-30)\n",
            "Test row date: 1995-05-30 -> Model trained, prediction made (using: 1990-05-30 - 1995-04-30)\n",
            "Test row date: 1995-06-30 -> Model trained, prediction made (using: 1990-06-30 - 1995-05-30)\n",
            "Test row date: 1995-07-30 -> Model trained, prediction made (using: 1990-07-30 - 1995-06-30)\n",
            "Test row date: 1995-08-30 -> Model trained, prediction made (using: 1990-08-30 - 1995-07-30)\n",
            "Test row date: 1995-09-30 -> Model trained, prediction made (using: 1990-09-30 - 1995-08-30)\n",
            "Test row date: 1995-10-30 -> Model trained, prediction made (using: 1990-10-30 - 1995-09-30)\n",
            "Test row date: 1995-11-30 -> Model trained, prediction made (using: 1990-11-30 - 1995-10-30)\n",
            "Test row date: 1995-12-30 -> Model trained, prediction made (using: 1990-12-30 - 1995-11-30)\n",
            "Test row date: 1996-01-30 -> Model trained, prediction made (using: 1991-01-30 - 1995-12-30)\n",
            "Test row date: 1996-02-29 -> Model trained, prediction made (using: 1991-02-28 - 1996-01-30)\n",
            "Test row date: 1996-03-30 -> Model trained, prediction made (using: 1991-03-30 - 1996-02-29)\n",
            "Test row date: 1996-04-30 -> Model trained, prediction made (using: 1991-04-30 - 1996-03-30)\n",
            "Test row date: 1996-05-30 -> Model trained, prediction made (using: 1991-05-30 - 1996-04-30)\n",
            "Test row date: 1996-06-30 -> Model trained, prediction made (using: 1991-06-30 - 1996-05-30)\n",
            "Test row date: 1996-07-30 -> Model trained, prediction made (using: 1991-07-30 - 1996-06-30)\n",
            "Test row date: 1996-08-30 -> Model trained, prediction made (using: 1991-08-30 - 1996-07-30)\n",
            "Test row date: 1996-09-30 -> Model trained, prediction made (using: 1991-09-30 - 1996-08-30)\n",
            "Test row date: 1996-10-30 -> Model trained, prediction made (using: 1991-10-30 - 1996-09-30)\n",
            "Test row date: 1996-11-30 -> Model trained, prediction made (using: 1991-11-30 - 1996-10-30)\n",
            "Test row date: 1996-12-30 -> Model trained, prediction made (using: 1991-12-30 - 1996-11-30)\n",
            "Test row date: 1997-01-30 -> Model trained, prediction made (using: 1992-01-30 - 1996-12-30)\n",
            "Test row date: 1997-02-28 -> Model trained, prediction made (using: 1992-02-29 - 1997-01-30)\n",
            "Test row date: 1997-03-30 -> Model trained, prediction made (using: 1992-03-30 - 1997-02-28)\n",
            "Test row date: 1997-04-30 -> Model trained, prediction made (using: 1992-04-30 - 1997-03-30)\n",
            "Test row date: 1997-05-30 -> Model trained, prediction made (using: 1992-05-30 - 1997-04-30)\n",
            "Test row date: 1997-06-30 -> Model trained, prediction made (using: 1992-06-30 - 1997-05-30)\n",
            "Test row date: 1997-07-30 -> Model trained, prediction made (using: 1992-07-30 - 1997-06-30)\n",
            "Test row date: 1997-08-30 -> Model trained, prediction made (using: 1992-08-30 - 1997-07-30)\n",
            "Test row date: 1997-09-30 -> Model trained, prediction made (using: 1992-09-30 - 1997-08-30)\n",
            "Test row date: 1997-10-30 -> Model trained, prediction made (using: 1992-10-30 - 1997-09-30)\n",
            "Test row date: 1997-11-30 -> Model trained, prediction made (using: 1992-11-30 - 1997-10-30)\n",
            "Test row date: 1997-12-30 -> Model trained, prediction made (using: 1992-12-30 - 1997-11-30)\n",
            "Test row date: 1998-01-30 -> Model trained, prediction made (using: 1993-01-30 - 1997-12-30)\n",
            "Test row date: 1998-02-28 -> Model trained, prediction made (using: 1993-02-28 - 1998-01-30)\n",
            "Test row date: 1998-03-30 -> Model trained, prediction made (using: 1993-03-30 - 1998-02-28)\n",
            "Test row date: 1998-04-30 -> Model trained, prediction made (using: 1993-04-30 - 1998-03-30)\n",
            "Test row date: 1998-05-30 -> Model trained, prediction made (using: 1993-05-30 - 1998-04-30)\n",
            "Test row date: 1998-06-30 -> Model trained, prediction made (using: 1993-06-30 - 1998-05-30)\n",
            "Test row date: 1998-07-30 -> Model trained, prediction made (using: 1993-07-30 - 1998-06-30)\n",
            "Test row date: 1998-08-30 -> Model trained, prediction made (using: 1993-08-30 - 1998-07-30)\n",
            "Test row date: 1998-09-30 -> Model trained, prediction made (using: 1993-09-30 - 1998-08-30)\n",
            "Test row date: 1998-10-30 -> Model trained, prediction made (using: 1993-10-30 - 1998-09-30)\n",
            "Test row date: 1998-11-30 -> Model trained, prediction made (using: 1993-11-30 - 1998-10-30)\n",
            "Test row date: 1998-12-30 -> Model trained, prediction made (using: 1993-12-30 - 1998-11-30)\n",
            "Test row date: 1999-01-30 -> Model trained, prediction made (using: 1994-01-30 - 1998-12-30)\n",
            "Test row date: 1999-02-28 -> Model trained, prediction made (using: 1994-02-28 - 1999-01-30)\n",
            "Test row date: 1999-03-30 -> Model trained, prediction made (using: 1994-03-30 - 1999-02-28)\n",
            "Test row date: 1999-04-30 -> Model trained, prediction made (using: 1994-04-30 - 1999-03-30)\n",
            "Test row date: 1999-05-30 -> Model trained, prediction made (using: 1994-05-30 - 1999-04-30)\n",
            "Test row date: 1999-06-30 -> Model trained, prediction made (using: 1994-06-30 - 1999-05-30)\n",
            "Test row date: 1999-07-30 -> Model trained, prediction made (using: 1994-07-30 - 1999-06-30)\n",
            "Test row date: 1999-08-30 -> Model trained, prediction made (using: 1994-08-30 - 1999-07-30)\n",
            "Test row date: 1999-09-30 -> Model trained, prediction made (using: 1994-09-30 - 1999-08-30)\n",
            "Test row date: 1999-10-30 -> Model trained, prediction made (using: 1994-10-30 - 1999-09-30)\n",
            "Test row date: 1999-11-30 -> Model trained, prediction made (using: 1994-11-30 - 1999-10-30)\n",
            "Test row date: 1999-12-30 -> Model trained, prediction made (using: 1994-12-30 - 1999-11-30)\n",
            "Test row date: 2000-01-30 -> Model trained, prediction made (using: 1995-01-30 - 1999-12-30)\n",
            "Test row date: 2000-02-29 -> Model trained, prediction made (using: 1995-02-28 - 2000-01-30)\n",
            "Test row date: 2000-03-30 -> Model trained, prediction made (using: 1995-03-30 - 2000-02-29)\n",
            "Test row date: 2000-04-30 -> Model trained, prediction made (using: 1995-04-30 - 2000-03-30)\n",
            "Test row date: 2000-05-30 -> Model trained, prediction made (using: 1995-05-30 - 2000-04-30)\n",
            "Test row date: 2000-06-30 -> Model trained, prediction made (using: 1995-06-30 - 2000-05-30)\n",
            "Test row date: 2000-07-30 -> Model trained, prediction made (using: 1995-07-30 - 2000-06-30)\n",
            "Test row date: 2000-08-30 -> Model trained, prediction made (using: 1995-08-30 - 2000-07-30)\n",
            "Test row date: 2000-09-30 -> Model trained, prediction made (using: 1995-09-30 - 2000-08-30)\n",
            "Test row date: 2000-10-30 -> Model trained, prediction made (using: 1995-10-30 - 2000-09-30)\n",
            "Test row date: 2000-11-30 -> Model trained, prediction made (using: 1995-11-30 - 2000-10-30)\n",
            "Test row date: 2000-12-30 -> Model trained, prediction made (using: 1995-12-30 - 2000-11-30)\n",
            "Test row date: 2001-01-30 -> Model trained, prediction made (using: 1996-01-30 - 2000-12-30)\n",
            "Test row date: 2001-02-28 -> Model trained, prediction made (using: 1996-02-29 - 2001-01-30)\n",
            "Test row date: 2001-03-30 -> Model trained, prediction made (using: 1996-03-30 - 2001-02-28)\n",
            "Test row date: 2001-04-30 -> Model trained, prediction made (using: 1996-04-30 - 2001-03-30)\n",
            "Test row date: 2001-05-30 -> Model trained, prediction made (using: 1996-05-30 - 2001-04-30)\n",
            "Test row date: 2001-06-30 -> Model trained, prediction made (using: 1996-06-30 - 2001-05-30)\n",
            "Test row date: 2001-07-30 -> Model trained, prediction made (using: 1996-07-30 - 2001-06-30)\n",
            "Test row date: 2001-08-30 -> Model trained, prediction made (using: 1996-08-30 - 2001-07-30)\n",
            "Test row date: 2001-09-30 -> Model trained, prediction made (using: 1996-09-30 - 2001-08-30)\n",
            "Test row date: 2001-10-30 -> Model trained, prediction made (using: 1996-10-30 - 2001-09-30)\n",
            "Test row date: 2001-11-30 -> Model trained, prediction made (using: 1996-11-30 - 2001-10-30)\n",
            "Test row date: 2001-12-30 -> Model trained, prediction made (using: 1996-12-30 - 2001-11-30)\n",
            "Test row date: 2002-01-30 -> Model trained, prediction made (using: 1997-01-30 - 2001-12-30)\n",
            "Test row date: 2002-02-28 -> Model trained, prediction made (using: 1997-02-28 - 2002-01-30)\n",
            "Test row date: 2002-03-30 -> Model trained, prediction made (using: 1997-03-30 - 2002-02-28)\n",
            "Test row date: 2002-04-30 -> Model trained, prediction made (using: 1997-04-30 - 2002-03-30)\n",
            "Test row date: 2002-05-30 -> Model trained, prediction made (using: 1997-05-30 - 2002-04-30)\n",
            "Test row date: 2002-06-30 -> Model trained, prediction made (using: 1997-06-30 - 2002-05-30)\n",
            "Test row date: 2002-07-30 -> Model trained, prediction made (using: 1997-07-30 - 2002-06-30)\n",
            "Test row date: 2002-08-30 -> Model trained, prediction made (using: 1997-08-30 - 2002-07-30)\n",
            "Test row date: 2002-09-30 -> Model trained, prediction made (using: 1997-09-30 - 2002-08-30)\n",
            "Test row date: 2002-10-30 -> Model trained, prediction made (using: 1997-10-30 - 2002-09-30)\n",
            "Test row date: 2002-11-30 -> Model trained, prediction made (using: 1997-11-30 - 2002-10-30)\n",
            "Test row date: 2002-12-30 -> Model trained, prediction made (using: 1997-12-30 - 2002-11-30)\n",
            "Test row date: 2003-01-30 -> Model trained, prediction made (using: 1998-01-30 - 2002-12-30)\n",
            "Test row date: 2003-02-28 -> Model trained, prediction made (using: 1998-02-28 - 2003-01-30)\n",
            "Test row date: 2003-03-30 -> Model trained, prediction made (using: 1998-03-30 - 2003-02-28)\n",
            "Test row date: 2003-04-30 -> Model trained, prediction made (using: 1998-04-30 - 2003-03-30)\n",
            "Test row date: 2003-05-30 -> Model trained, prediction made (using: 1998-05-30 - 2003-04-30)\n",
            "Test row date: 2003-06-30 -> Model trained, prediction made (using: 1998-06-30 - 2003-05-30)\n",
            "Test row date: 2003-07-30 -> Model trained, prediction made (using: 1998-07-30 - 2003-06-30)\n",
            "Test row date: 2003-08-30 -> Model trained, prediction made (using: 1998-08-30 - 2003-07-30)\n",
            "Test row date: 2003-09-30 -> Model trained, prediction made (using: 1998-09-30 - 2003-08-30)\n",
            "Test row date: 2003-10-30 -> Model trained, prediction made (using: 1998-10-30 - 2003-09-30)\n",
            "Test row date: 2003-11-30 -> Model trained, prediction made (using: 1998-11-30 - 2003-10-30)\n",
            "Test row date: 2003-12-30 -> Model trained, prediction made (using: 1998-12-30 - 2003-11-30)\n",
            "Test row date: 2004-01-30 -> Model trained, prediction made (using: 1999-01-30 - 2003-12-30)\n",
            "Test row date: 2004-02-29 -> Model trained, prediction made (using: 1999-02-28 - 2004-01-30)\n",
            "Test row date: 2004-03-30 -> Model trained, prediction made (using: 1999-03-30 - 2004-02-29)\n",
            "Test row date: 2004-04-30 -> Model trained, prediction made (using: 1999-04-30 - 2004-03-30)\n",
            "Test row date: 2004-05-30 -> Model trained, prediction made (using: 1999-05-30 - 2004-04-30)\n",
            "Test row date: 2004-06-30 -> Model trained, prediction made (using: 1999-06-30 - 2004-05-30)\n",
            "Test row date: 2004-07-30 -> Model trained, prediction made (using: 1999-07-30 - 2004-06-30)\n",
            "Test row date: 2004-08-30 -> Model trained, prediction made (using: 1999-08-30 - 2004-07-30)\n",
            "Test row date: 2004-09-30 -> Model trained, prediction made (using: 1999-09-30 - 2004-08-30)\n",
            "Test row date: 2004-10-30 -> Model trained, prediction made (using: 1999-10-30 - 2004-09-30)\n",
            "Test row date: 2004-11-30 -> Model trained, prediction made (using: 1999-11-30 - 2004-10-30)\n",
            "Test row date: 2004-12-30 -> Model trained, prediction made (using: 1999-12-30 - 2004-11-30)\n",
            "Test row date: 2005-01-30 -> Model trained, prediction made (using: 2000-01-30 - 2004-12-30)\n",
            "Test row date: 2005-02-28 -> Model trained, prediction made (using: 2000-02-29 - 2005-01-30)\n",
            "Test row date: 2005-03-30 -> Model trained, prediction made (using: 2000-03-30 - 2005-02-28)\n",
            "Test row date: 2005-04-30 -> Model trained, prediction made (using: 2000-04-30 - 2005-03-30)\n",
            "Test row date: 2005-05-30 -> Model trained, prediction made (using: 2000-05-30 - 2005-04-30)\n",
            "Test row date: 2005-06-30 -> Model trained, prediction made (using: 2000-06-30 - 2005-05-30)\n",
            "Test row date: 2005-07-30 -> Model trained, prediction made (using: 2000-07-30 - 2005-06-30)\n",
            "Test row date: 2005-08-30 -> Model trained, prediction made (using: 2000-08-30 - 2005-07-30)\n",
            "Test row date: 2005-09-30 -> Model trained, prediction made (using: 2000-09-30 - 2005-08-30)\n",
            "Test row date: 2005-10-30 -> Model trained, prediction made (using: 2000-10-30 - 2005-09-30)\n",
            "Test row date: 2005-11-30 -> Model trained, prediction made (using: 2000-11-30 - 2005-10-30)\n",
            "Test row date: 2005-12-30 -> Model trained, prediction made (using: 2000-12-30 - 2005-11-30)\n",
            "Test row date: 2006-01-30 -> Model trained, prediction made (using: 2001-01-30 - 2005-12-30)\n",
            "Test row date: 2006-02-28 -> Model trained, prediction made (using: 2001-02-28 - 2006-01-30)\n",
            "Test row date: 2006-03-30 -> Model trained, prediction made (using: 2001-03-30 - 2006-02-28)\n",
            "Test row date: 2006-04-30 -> Model trained, prediction made (using: 2001-04-30 - 2006-03-30)\n",
            "Test row date: 2006-05-30 -> Model trained, prediction made (using: 2001-05-30 - 2006-04-30)\n",
            "Test row date: 2006-06-30 -> Model trained, prediction made (using: 2001-06-30 - 2006-05-30)\n",
            "Test row date: 2006-07-30 -> Model trained, prediction made (using: 2001-07-30 - 2006-06-30)\n",
            "Test row date: 2006-08-30 -> Model trained, prediction made (using: 2001-08-30 - 2006-07-30)\n",
            "Test row date: 2006-09-30 -> Model trained, prediction made (using: 2001-09-30 - 2006-08-30)\n",
            "Test row date: 2006-10-30 -> Model trained, prediction made (using: 2001-10-30 - 2006-09-30)\n",
            "Test row date: 2006-11-30 -> Model trained, prediction made (using: 2001-11-30 - 2006-10-30)\n",
            "Test row date: 2006-12-30 -> Model trained, prediction made (using: 2001-12-30 - 2006-11-30)\n",
            "Test row date: 2007-01-30 -> Model trained, prediction made (using: 2002-01-30 - 2006-12-30)\n",
            "Test row date: 2007-02-28 -> Model trained, prediction made (using: 2002-02-28 - 2007-01-30)\n",
            "Test row date: 2007-03-30 -> Model trained, prediction made (using: 2002-03-30 - 2007-02-28)\n",
            "Test row date: 2007-04-30 -> Model trained, prediction made (using: 2002-04-30 - 2007-03-30)\n",
            "Test row date: 2007-05-30 -> Model trained, prediction made (using: 2002-05-30 - 2007-04-30)\n",
            "Test row date: 2007-06-30 -> Model trained, prediction made (using: 2002-06-30 - 2007-05-30)\n",
            "Test row date: 2007-07-30 -> Model trained, prediction made (using: 2002-07-30 - 2007-06-30)\n",
            "Test row date: 2007-08-30 -> Model trained, prediction made (using: 2002-08-30 - 2007-07-30)\n",
            "Test row date: 2007-09-30 -> Model trained, prediction made (using: 2002-09-30 - 2007-08-30)\n",
            "Test row date: 2007-10-30 -> Model trained, prediction made (using: 2002-10-30 - 2007-09-30)\n",
            "Test row date: 2007-11-30 -> Model trained, prediction made (using: 2002-11-30 - 2007-10-30)\n",
            "Test row date: 2007-12-30 -> Model trained, prediction made (using: 2002-12-30 - 2007-11-30)\n",
            "Test row date: 2008-01-30 -> Model trained, prediction made (using: 2003-01-30 - 2007-12-30)\n",
            "Test row date: 2008-02-29 -> Model trained, prediction made (using: 2003-02-28 - 2008-01-30)\n",
            "Test row date: 2008-03-30 -> Model trained, prediction made (using: 2003-03-30 - 2008-02-29)\n",
            "Test row date: 2008-04-30 -> Model trained, prediction made (using: 2003-04-30 - 2008-03-30)\n",
            "Test row date: 2008-05-30 -> Model trained, prediction made (using: 2003-05-30 - 2008-04-30)\n",
            "Test row date: 2008-06-30 -> Model trained, prediction made (using: 2003-06-30 - 2008-05-30)\n",
            "Test row date: 2008-07-30 -> Model trained, prediction made (using: 2003-07-30 - 2008-06-30)\n",
            "Test row date: 2008-08-30 -> Model trained, prediction made (using: 2003-08-30 - 2008-07-30)\n",
            "Test row date: 2008-09-30 -> Model trained, prediction made (using: 2003-09-30 - 2008-08-30)\n",
            "Test row date: 2008-10-30 -> Model trained, prediction made (using: 2003-10-30 - 2008-09-30)\n",
            "Test row date: 2008-11-30 -> Model trained, prediction made (using: 2003-11-30 - 2008-10-30)\n",
            "Test row date: 2008-12-30 -> Model trained, prediction made (using: 2003-12-30 - 2008-11-30)\n",
            "Test row date: 2009-01-30 -> Model trained, prediction made (using: 2004-01-30 - 2008-12-30)\n",
            "Test row date: 2009-02-28 -> Model trained, prediction made (using: 2004-02-29 - 2009-01-30)\n",
            "Test row date: 2009-03-30 -> Model trained, prediction made (using: 2004-03-30 - 2009-02-28)\n",
            "Test row date: 2009-04-30 -> Model trained, prediction made (using: 2004-04-30 - 2009-03-30)\n",
            "Test row date: 2009-05-30 -> Model trained, prediction made (using: 2004-05-30 - 2009-04-30)\n",
            "Test row date: 2009-06-30 -> Model trained, prediction made (using: 2004-06-30 - 2009-05-30)\n",
            "Test row date: 2009-07-30 -> Model trained, prediction made (using: 2004-07-30 - 2009-06-30)\n",
            "Test row date: 2009-08-30 -> Model trained, prediction made (using: 2004-08-30 - 2009-07-30)\n",
            "Test row date: 2009-09-30 -> Model trained, prediction made (using: 2004-09-30 - 2009-08-30)\n",
            "Test row date: 2009-10-30 -> Model trained, prediction made (using: 2004-10-30 - 2009-09-30)\n",
            "Test row date: 2009-11-30 -> Model trained, prediction made (using: 2004-11-30 - 2009-10-30)\n",
            "Test row date: 2009-12-30 -> Model trained, prediction made (using: 2004-12-30 - 2009-11-30)\n",
            "Test row date: 2010-01-30 -> Model trained, prediction made (using: 2005-01-30 - 2009-12-30)\n",
            "Test row date: 2010-02-28 -> Model trained, prediction made (using: 2005-02-28 - 2010-01-30)\n",
            "Test row date: 2010-03-30 -> Model trained, prediction made (using: 2005-03-30 - 2010-02-28)\n",
            "Test row date: 2010-04-30 -> Model trained, prediction made (using: 2005-04-30 - 2010-03-30)\n",
            "Test row date: 2010-05-30 -> Model trained, prediction made (using: 2005-05-30 - 2010-04-30)\n",
            "Test row date: 2010-06-30 -> Model trained, prediction made (using: 2005-06-30 - 2010-05-30)\n",
            "Test row date: 2010-07-30 -> Model trained, prediction made (using: 2005-07-30 - 2010-06-30)\n",
            "Test row date: 2010-08-30 -> Model trained, prediction made (using: 2005-08-30 - 2010-07-30)\n",
            "Test row date: 2010-09-30 -> Model trained, prediction made (using: 2005-09-30 - 2010-08-30)\n",
            "Test row date: 2010-10-30 -> Model trained, prediction made (using: 2005-10-30 - 2010-09-30)\n",
            "Test row date: 2010-11-30 -> Model trained, prediction made (using: 2005-11-30 - 2010-10-30)\n",
            "Test row date: 2010-12-30 -> Model trained, prediction made (using: 2005-12-30 - 2010-11-30)\n",
            "Test row date: 2011-01-30 -> Model trained, prediction made (using: 2006-01-30 - 2010-12-30)\n",
            "Test row date: 2011-02-28 -> Model trained, prediction made (using: 2006-02-28 - 2011-01-30)\n",
            "Test row date: 2011-03-30 -> Model trained, prediction made (using: 2006-03-30 - 2011-02-28)\n",
            "Test row date: 2011-04-30 -> Model trained, prediction made (using: 2006-04-30 - 2011-03-30)\n",
            "Test row date: 2011-05-30 -> Model trained, prediction made (using: 2006-05-30 - 2011-04-30)\n",
            "Test row date: 2011-06-30 -> Model trained, prediction made (using: 2006-06-30 - 2011-05-30)\n",
            "Test row date: 2011-07-30 -> Model trained, prediction made (using: 2006-07-30 - 2011-06-30)\n",
            "Test row date: 2011-08-30 -> Model trained, prediction made (using: 2006-08-30 - 2011-07-30)\n",
            "Test row date: 2011-09-30 -> Model trained, prediction made (using: 2006-09-30 - 2011-08-30)\n",
            "Test row date: 2011-10-30 -> Model trained, prediction made (using: 2006-10-30 - 2011-09-30)\n",
            "Test row date: 2011-11-30 -> Model trained, prediction made (using: 2006-11-30 - 2011-10-30)\n",
            "Test row date: 2011-12-30 -> Model trained, prediction made (using: 2006-12-30 - 2011-11-30)\n",
            "Test row date: 2012-01-30 -> Model trained, prediction made (using: 2007-01-30 - 2011-12-30)\n",
            "Test row date: 2012-02-29 -> Model trained, prediction made (using: 2007-02-28 - 2012-01-30)\n",
            "Test row date: 2012-03-30 -> Model trained, prediction made (using: 2007-03-30 - 2012-02-29)\n",
            "Test row date: 2012-04-30 -> Model trained, prediction made (using: 2007-04-30 - 2012-03-30)\n",
            "Test row date: 2012-05-30 -> Model trained, prediction made (using: 2007-05-30 - 2012-04-30)\n",
            "Test row date: 2012-06-30 -> Model trained, prediction made (using: 2007-06-30 - 2012-05-30)\n",
            "Test row date: 2012-07-30 -> Model trained, prediction made (using: 2007-07-30 - 2012-06-30)\n",
            "Test row date: 2012-08-30 -> Model trained, prediction made (using: 2007-08-30 - 2012-07-30)\n",
            "Test row date: 2012-09-30 -> Model trained, prediction made (using: 2007-09-30 - 2012-08-30)\n",
            "Test row date: 2012-10-30 -> Model trained, prediction made (using: 2007-10-30 - 2012-09-30)\n",
            "Test row date: 2012-11-30 -> Model trained, prediction made (using: 2007-11-30 - 2012-10-30)\n",
            "Test row date: 2012-12-30 -> Model trained, prediction made (using: 2007-12-30 - 2012-11-30)\n",
            "Test row date: 2013-01-30 -> Model trained, prediction made (using: 2008-01-30 - 2012-12-30)\n",
            "Test row date: 2013-02-28 -> Model trained, prediction made (using: 2008-02-29 - 2013-01-30)\n",
            "Test row date: 2013-03-30 -> Model trained, prediction made (using: 2008-03-30 - 2013-02-28)\n",
            "Test row date: 2013-04-30 -> Model trained, prediction made (using: 2008-04-30 - 2013-03-30)\n",
            "Test row date: 2013-05-30 -> Model trained, prediction made (using: 2008-05-30 - 2013-04-30)\n",
            "Test row date: 2013-06-30 -> Model trained, prediction made (using: 2008-06-30 - 2013-05-30)\n",
            "Test row date: 2013-07-30 -> Model trained, prediction made (using: 2008-07-30 - 2013-06-30)\n",
            "Test row date: 2013-08-30 -> Model trained, prediction made (using: 2008-08-30 - 2013-07-30)\n",
            "Test row date: 2013-09-30 -> Model trained, prediction made (using: 2008-09-30 - 2013-08-30)\n",
            "Test row date: 2013-10-30 -> Model trained, prediction made (using: 2008-10-30 - 2013-09-30)\n",
            "Test row date: 2013-11-30 -> Model trained, prediction made (using: 2008-11-30 - 2013-10-30)\n",
            "Test row date: 2013-12-30 -> Model trained, prediction made (using: 2008-12-30 - 2013-11-30)\n",
            "Test row date: 2014-01-30 -> Model trained, prediction made (using: 2009-01-30 - 2013-12-30)\n",
            "Test row date: 2014-02-28 -> Model trained, prediction made (using: 2009-02-28 - 2014-01-30)\n",
            "Test row date: 2014-03-30 -> Model trained, prediction made (using: 2009-03-30 - 2014-02-28)\n",
            "Test row date: 2014-04-30 -> Model trained, prediction made (using: 2009-04-30 - 2014-03-30)\n",
            "Test row date: 2014-05-30 -> Model trained, prediction made (using: 2009-05-30 - 2014-04-30)\n",
            "Test row date: 2014-06-30 -> Model trained, prediction made (using: 2009-06-30 - 2014-05-30)\n",
            "Test row date: 2014-07-30 -> Model trained, prediction made (using: 2009-07-30 - 2014-06-30)\n",
            "Test row date: 2014-08-30 -> Model trained, prediction made (using: 2009-08-30 - 2014-07-30)\n",
            "Test row date: 2014-09-30 -> Model trained, prediction made (using: 2009-09-30 - 2014-08-30)\n",
            "Test row date: 2014-10-30 -> Model trained, prediction made (using: 2009-10-30 - 2014-09-30)\n",
            "Test row date: 2014-11-30 -> Model trained, prediction made (using: 2009-11-30 - 2014-10-30)\n",
            "Test row date: 2014-12-30 -> Model trained, prediction made (using: 2009-12-30 - 2014-11-30)\n",
            "Test row date: 2015-01-30 -> Model trained, prediction made (using: 2010-01-30 - 2014-12-30)\n",
            "Test row date: 2015-02-28 -> Model trained, prediction made (using: 2010-02-28 - 2015-01-30)\n",
            "Test row date: 2015-03-30 -> Model trained, prediction made (using: 2010-03-30 - 2015-02-28)\n",
            "Test row date: 2015-04-30 -> Model trained, prediction made (using: 2010-04-30 - 2015-03-30)\n",
            "Test row date: 2015-05-30 -> Model trained, prediction made (using: 2010-05-30 - 2015-04-30)\n",
            "Test row date: 2015-06-30 -> Model trained, prediction made (using: 2010-06-30 - 2015-05-30)\n",
            "Test row date: 2015-07-30 -> Model trained, prediction made (using: 2010-07-30 - 2015-06-30)\n",
            "Test row date: 2015-08-30 -> Model trained, prediction made (using: 2010-08-30 - 2015-07-30)\n",
            "Test row date: 2015-09-30 -> Model trained, prediction made (using: 2010-09-30 - 2015-08-30)\n",
            "Test row date: 2015-10-30 -> Model trained, prediction made (using: 2010-10-30 - 2015-09-30)\n",
            "Test row date: 2015-11-30 -> Model trained, prediction made (using: 2010-11-30 - 2015-10-30)\n",
            "Test row date: 2015-12-30 -> Model trained, prediction made (using: 2010-12-30 - 2015-11-30)\n",
            "Test row date: 2016-01-30 -> Model trained, prediction made (using: 2011-01-30 - 2015-12-30)\n",
            "Test row date: 2016-02-29 -> Model trained, prediction made (using: 2011-02-28 - 2016-01-30)\n",
            "Test row date: 2016-03-30 -> Model trained, prediction made (using: 2011-03-30 - 2016-02-29)\n",
            "Test row date: 2016-04-30 -> Model trained, prediction made (using: 2011-04-30 - 2016-03-30)\n",
            "Test row date: 2016-05-30 -> Model trained, prediction made (using: 2011-05-30 - 2016-04-30)\n",
            "Test row date: 2016-06-30 -> Model trained, prediction made (using: 2011-06-30 - 2016-05-30)\n",
            "Test row date: 2016-07-30 -> Model trained, prediction made (using: 2011-07-30 - 2016-06-30)\n",
            "Test row date: 2016-08-30 -> Model trained, prediction made (using: 2011-08-30 - 2016-07-30)\n",
            "Test row date: 2016-09-30 -> Model trained, prediction made (using: 2011-09-30 - 2016-08-30)\n",
            "Test row date: 2016-10-30 -> Model trained, prediction made (using: 2011-10-30 - 2016-09-30)\n",
            "Test row date: 2016-11-30 -> Model trained, prediction made (using: 2011-11-30 - 2016-10-30)\n",
            "Test row date: 2016-12-30 -> Model trained, prediction made (using: 2011-12-30 - 2016-11-30)\n",
            "Test row date: 2017-01-30 -> Model trained, prediction made (using: 2012-01-30 - 2016-12-30)\n",
            "Test row date: 2017-02-28 -> Model trained, prediction made (using: 2012-02-29 - 2017-01-30)\n",
            "Test row date: 2017-03-30 -> Model trained, prediction made (using: 2012-03-30 - 2017-02-28)\n",
            "Test row date: 2017-04-30 -> Model trained, prediction made (using: 2012-04-30 - 2017-03-30)\n",
            "Test row date: 2017-05-30 -> Model trained, prediction made (using: 2012-05-30 - 2017-04-30)\n",
            "Test row date: 2017-06-30 -> Model trained, prediction made (using: 2012-06-30 - 2017-05-30)\n",
            "Test row date: 2017-07-30 -> Model trained, prediction made (using: 2012-07-30 - 2017-06-30)\n",
            "Test row date: 2017-08-30 -> Model trained, prediction made (using: 2012-08-30 - 2017-07-30)\n",
            "Test row date: 2017-09-30 -> Model trained, prediction made (using: 2012-09-30 - 2017-08-30)\n",
            "Test row date: 2017-10-30 -> Model trained, prediction made (using: 2012-10-30 - 2017-09-30)\n",
            "Test row date: 2017-11-30 -> Model trained, prediction made (using: 2012-11-30 - 2017-10-30)\n",
            "Test row date: 2017-12-30 -> Model trained, prediction made (using: 2012-12-30 - 2017-11-30)\n",
            "Test row date: 2018-01-30 -> Model trained, prediction made (using: 2013-01-30 - 2017-12-30)\n",
            "Test row date: 2018-02-28 -> Model trained, prediction made (using: 2013-02-28 - 2018-01-30)\n",
            "Test row date: 2018-03-30 -> Model trained, prediction made (using: 2013-03-30 - 2018-02-28)\n",
            "Test row date: 2018-04-30 -> Model trained, prediction made (using: 2013-04-30 - 2018-03-30)\n",
            "Test row date: 2018-05-30 -> Model trained, prediction made (using: 2013-05-30 - 2018-04-30)\n",
            "Test row date: 2018-06-30 -> Model trained, prediction made (using: 2013-06-30 - 2018-05-30)\n",
            "Test row date: 2018-07-30 -> Model trained, prediction made (using: 2013-07-30 - 2018-06-30)\n",
            "Test row date: 2018-08-30 -> Model trained, prediction made (using: 2013-08-30 - 2018-07-30)\n",
            "Test row date: 2018-09-30 -> Model trained, prediction made (using: 2013-09-30 - 2018-08-30)\n",
            "Test row date: 2018-10-30 -> Model trained, prediction made (using: 2013-10-30 - 2018-09-30)\n",
            "Test row date: 2018-11-30 -> Model trained, prediction made (using: 2013-11-30 - 2018-10-30)\n",
            "Test row date: 2018-12-30 -> Model trained, prediction made (using: 2013-12-30 - 2018-11-30)\n",
            "Test row date: 2019-01-30 -> Model trained, prediction made (using: 2014-01-30 - 2018-12-30)\n",
            "Test row date: 2019-02-28 -> Model trained, prediction made (using: 2014-02-28 - 2019-01-30)\n",
            "Test row date: 2019-03-30 -> Model trained, prediction made (using: 2014-03-30 - 2019-02-28)\n",
            "Test row date: 2019-04-30 -> Model trained, prediction made (using: 2014-04-30 - 2019-03-30)\n",
            "Test row date: 2019-05-30 -> Model trained, prediction made (using: 2014-05-30 - 2019-04-30)\n",
            "Test row date: 2019-06-30 -> Model trained, prediction made (using: 2014-06-30 - 2019-05-30)\n",
            "Test row date: 2019-07-30 -> Model trained, prediction made (using: 2014-07-30 - 2019-06-30)\n",
            "Test row date: 2019-08-30 -> Model trained, prediction made (using: 2014-08-30 - 2019-07-30)\n",
            "Test row date: 2019-09-30 -> Model trained, prediction made (using: 2014-09-30 - 2019-08-30)\n",
            "Test row date: 2019-10-30 -> Model trained, prediction made (using: 2014-10-30 - 2019-09-30)\n",
            "Test row date: 2019-11-30 -> Model trained, prediction made (using: 2014-11-30 - 2019-10-30)\n",
            "Test row date: 2019-12-30 -> Model trained, prediction made (using: 2014-12-30 - 2019-11-30)\n",
            "Test row date: 2020-01-30 -> Model trained, prediction made (using: 2015-01-30 - 2019-12-30)\n",
            "Test row date: 2020-02-29 -> Model trained, prediction made (using: 2015-02-28 - 2020-01-30)\n",
            "Test row date: 2020-03-30 -> Model trained, prediction made (using: 2015-03-30 - 2020-02-29)\n",
            "Test row date: 2020-04-30 -> Model trained, prediction made (using: 2015-04-30 - 2020-03-30)\n",
            "Test row date: 2020-05-30 -> Model trained, prediction made (using: 2015-05-30 - 2020-04-30)\n",
            "Test row date: 2020-06-30 -> Model trained, prediction made (using: 2015-06-30 - 2020-05-30)\n",
            "Test row date: 2020-07-30 -> Model trained, prediction made (using: 2015-07-30 - 2020-06-30)\n",
            "Test row date: 2020-08-30 -> Model trained, prediction made (using: 2015-08-30 - 2020-07-30)\n",
            "Test row date: 2020-09-30 -> Model trained, prediction made (using: 2015-09-30 - 2020-08-30)\n",
            "Test row date: 2020-10-30 -> Model trained, prediction made (using: 2015-10-30 - 2020-09-30)\n",
            "Test row date: 2020-11-30 -> Model trained, prediction made (using: 2015-11-30 - 2020-10-30)\n",
            "Test row date: 2020-12-30 -> Model trained, prediction made (using: 2015-12-30 - 2020-11-30)\n",
            "Test row date: 2021-01-30 -> Model trained, prediction made (using: 2016-01-30 - 2020-12-30)\n",
            "Test row date: 2021-02-28 -> Model trained, prediction made (using: 2016-02-29 - 2021-01-30)\n",
            "Test row date: 2021-03-30 -> Model trained, prediction made (using: 2016-03-30 - 2021-02-28)\n",
            "Test row date: 2021-04-30 -> Model trained, prediction made (using: 2016-04-30 - 2021-03-30)\n",
            "Test row date: 2021-05-30 -> Model trained, prediction made (using: 2016-05-30 - 2021-04-30)\n",
            "Test row date: 2021-06-30 -> Model trained, prediction made (using: 2016-06-30 - 2021-05-30)\n",
            "Test row date: 2021-07-30 -> Model trained, prediction made (using: 2016-07-30 - 2021-06-30)\n",
            "Test row date: 2021-08-30 -> Model trained, prediction made (using: 2016-08-30 - 2021-07-30)\n",
            "Test row date: 2021-09-30 -> Model trained, prediction made (using: 2016-09-30 - 2021-08-30)\n",
            "Test row date: 2021-10-30 -> Model trained, prediction made (using: 2016-10-30 - 2021-09-30)\n",
            "Test row date: 2021-11-30 -> Model trained, prediction made (using: 2016-11-30 - 2021-10-30)\n",
            "Test row date: 2021-12-30 -> Model trained, prediction made (using: 2016-12-30 - 2021-11-30)\n",
            "Test row date: 2022-01-30 -> Model trained, prediction made (using: 2017-01-30 - 2021-12-30)\n",
            "Test row date: 2022-02-28 -> Model trained, prediction made (using: 2017-02-28 - 2022-01-30)\n",
            "Test row date: 2022-03-30 -> Model trained, prediction made (using: 2017-03-30 - 2022-02-28)\n",
            "Test row date: 2022-04-30 -> Model trained, prediction made (using: 2017-04-30 - 2022-03-30)\n",
            "Test row date: 2022-05-30 -> Model trained, prediction made (using: 2017-05-30 - 2022-04-30)\n",
            "Test row date: 2022-06-30 -> Model trained, prediction made (using: 2017-06-30 - 2022-05-30)\n",
            "Test row date: 2022-07-30 -> Model trained, prediction made (using: 2017-07-30 - 2022-06-30)\n",
            "Test row date: 2022-08-30 -> Model trained, prediction made (using: 2017-08-30 - 2022-07-30)\n",
            "Test row date: 2022-09-30 -> Model trained, prediction made (using: 2017-09-30 - 2022-08-30)\n",
            "Test row date: 2022-10-30 -> Model trained, prediction made (using: 2017-10-30 - 2022-09-30)\n",
            "Test row date: 2022-11-30 -> Model trained, prediction made (using: 2017-11-30 - 2022-10-30)\n",
            "Test row date: 2022-12-30 -> Model trained, prediction made (using: 2017-12-30 - 2022-11-30)\n",
            "Test row date: 2023-01-30 -> Model trained, prediction made (using: 2018-01-30 - 2022-12-30)\n",
            "Test row date: 2023-02-28 -> Model trained, prediction made (using: 2018-02-28 - 2023-01-30)\n",
            "Test row date: 2023-03-30 -> Model trained, prediction made (using: 2018-03-30 - 2023-02-28)\n",
            "Test row date: 2023-04-30 -> Model trained, prediction made (using: 2018-04-30 - 2023-03-30)\n",
            "Test row date: 2023-05-30 -> Model trained, prediction made (using: 2018-05-30 - 2023-04-30)\n",
            "Test row date: 2023-06-30 -> Model trained, prediction made (using: 2018-06-30 - 2023-05-30)\n",
            "Test row date: 2023-07-30 -> Model trained, prediction made (using: 2018-07-30 - 2023-06-30)\n",
            "Test row date: 2023-08-30 -> Model trained, prediction made (using: 2018-08-30 - 2023-07-30)\n",
            "Test row date: 2023-09-30 -> Model trained, prediction made (using: 2018-09-30 - 2023-08-30)\n",
            "Test row date: 2023-10-30 -> Model trained, prediction made (using: 2018-10-30 - 2023-09-30)\n",
            "Test row date: 2023-11-30 -> Model trained, prediction made (using: 2018-11-30 - 2023-10-30)\n",
            "Test row date: 2023-12-30 -> Model trained, prediction made (using: 2018-12-30 - 2023-11-30)\n",
            "Test row date: 2024-01-30 -> Model trained, prediction made (using: 2019-01-30 - 2023-12-30)\n",
            "Test row date: 2024-02-29 -> Model trained, prediction made (using: 2019-02-28 - 2024-01-30)\n",
            "Test row date: 2024-03-30 -> Model trained, prediction made (using: 2019-03-30 - 2024-02-29)\n",
            "Test row date: 2024-04-30 -> Model trained, prediction made (using: 2019-04-30 - 2024-03-30)\n",
            "Test row date: 2024-05-30 -> Model trained, prediction made (using: 2019-05-30 - 2024-04-30)\n",
            "Test row date: 2024-06-30 -> Model trained, prediction made (using: 2019-06-30 - 2024-05-30)\n",
            "Test row date: 2024-07-30 -> Model trained, prediction made (using: 2019-07-30 - 2024-06-30)\n",
            "Test row date: 2024-08-30 -> Model trained, prediction made (using: 2019-08-30 - 2024-07-30)\n",
            "Test row date: 2024-09-30 -> Model trained, prediction made (using: 2019-09-30 - 2024-08-30)\n",
            "Test row date: 2024-10-30 -> Model trained, prediction made (using: 2019-10-30 - 2024-09-30)\n",
            "Test row date: 2024-11-30 -> Model trained, prediction made (using: 2019-11-30 - 2024-10-30)\n",
            "Final results_df_rf columns: ['Regime', 'Predicted_month', 'Train_Start_Date', 'Train_End_Date', 'Train_Count', 'Feature_Importances', 'Predicted_Probabilities', 'Predicted_Winner', 'Allocated_Return', 'Equal_Weight_Return', 'Actual_Winner', 'Num_Trees', 'Average_Tree_Depth', 'Max_Tree_Depth', 'Prediction_Horizon_Months', 'Feature_Level_CPI%', 'Feature_Level_T10YFF', 'Feature_Level_CFNAI', 'Feature_Level_GARCH_1M', 'Feature_Level_SMB_MA12', 'Feature_Level_HML_MA12', 'Feature_Level_CMA_MA12', 'Feature_Level_RMW_MA12']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Regime Predicted_month Train_Start_Date Train_End_Date  Train_Count  \\\n",
              "622  NoRegime      2024-02-29       2019-02-28     2024-01-30           60   \n",
              "623  NoRegime      2024-03-30       2019-03-30     2024-02-29           60   \n",
              "624  NoRegime      2024-04-30       2019-04-30     2024-03-30           60   \n",
              "625  NoRegime      2024-05-30       2019-05-30     2024-04-30           60   \n",
              "626  NoRegime      2024-06-30       2019-06-30     2024-05-30           60   \n",
              "627  NoRegime      2024-07-30       2019-07-30     2024-06-30           60   \n",
              "628  NoRegime      2024-08-30       2019-08-30     2024-07-30           60   \n",
              "629  NoRegime      2024-09-30       2019-09-30     2024-08-30           60   \n",
              "630  NoRegime      2024-10-30       2019-10-30     2024-09-30           60   \n",
              "631  NoRegime      2024-11-30       2019-11-30     2024-10-30           60   \n",
              "\n",
              "                                   Feature_Importances  \\\n",
              "622  [0.0013977369972425638, 0.038018446324997617, ...   \n",
              "623  [0.0016301259096684801, 0.037638442878595385, ...   \n",
              "624  [0.0012938005390835628, 0.0377358490566038, 0....   \n",
              "625  [0.0, 0.030191507077435457, 0.4007972240327639...   \n",
              "626  [0.0, 0.029698996655518374, 0.4463940544995044...   \n",
              "627  [0.02203031776196578, 0.02203031776196578, 0.5...   \n",
              "628  [0.0, 0.14534915459476647, 0.2904117101155817,...   \n",
              "629  [0.10367892976588623, 0.2994792196661857, 0.15...   \n",
              "630  [0.12433706540954631, 0.343946445449918, 0.057...   \n",
              "631  [0.12983091547977074, 0.2729471722376977, 0.13...   \n",
              "\n",
              "                               Predicted_Probabilities Predicted_Winner  \\\n",
              "622                               [0.0, 0.0, 0.0, 1.0]              RMW   \n",
              "623  [0.518, 0.24600000000000016, 0.235999999999999...              SMB   \n",
              "624                           [0.0, 0.625, 0.375, 0.0]              HML   \n",
              "625                               [0.0, 0.0, 0.0, 1.0]              RMW   \n",
              "626                               [0.0, 0.0, 0.0, 1.0]              RMW   \n",
              "627  [0.39999999999999925, 0.19999999999999962, 0.0...              RMW   \n",
              "628                               [0.5, 0.0, 0.0, 0.5]              RMW   \n",
              "629  [0.3333333333333329, 0.0, 0.16666666666666646,...              RMW   \n",
              "630                               [0.0, 0.0, 0.0, 1.0]              RMW   \n",
              "631  [0.11111111111111088, 0.11111111111111088, 0.4...              CMA   \n",
              "\n",
              "     Allocated_Return  Equal_Weight_Return  ... Max_Tree_Depth  \\\n",
              "622         -0.019800            -0.021050  ...              5   \n",
              "623          0.007077             0.014250  ...              5   \n",
              "624         -0.004375            -0.004725  ...              5   \n",
              "625          0.029700            -0.002500  ...              4   \n",
              "626          0.005100            -0.022375  ...              4   \n",
              "627          0.045480             0.036675  ...              5   \n",
              "628         -0.014000            -0.007675  ...              5   \n",
              "629         -0.003633            -0.009575  ...              6   \n",
              "630         -0.013800            -0.000850  ...              6   \n",
              "631         -0.013122            -0.000150  ...              6   \n",
              "\n",
              "     Prediction_Horizon_Months  Feature_Level_CPI%  Feature_Level_T10YFF  \\\n",
              "622                          1             0.21033                 -1.34   \n",
              "623                          1             0.34301                 -1.08   \n",
              "624                          1             0.39639                 -1.13   \n",
              "625                          1             0.34885                 -0.64   \n",
              "626                          1             0.29125                 -0.82   \n",
              "627                          1             0.03961                 -0.97   \n",
              "628                          1            -0.00287                 -1.24   \n",
              "629                          1             0.13892                 -1.42   \n",
              "630                          1             0.18019                 -1.02   \n",
              "631                          1             0.22920                 -0.55   \n",
              "\n",
              "     Feature_Level_CFNAI  Feature_Level_GARCH_1M  Feature_Level_SMB_MA12  \\\n",
              "622                -0.14                0.726558               -0.002417   \n",
              "623                -0.83                0.780011               -0.010833   \n",
              "624                 0.39                0.496170               -0.012017   \n",
              "625                -0.20                0.944764               -0.007225   \n",
              "626                -0.39                0.569002               -0.007208   \n",
              "627                 0.15                0.327046               -0.006250   \n",
              "628                -0.17                1.087011               -0.011025   \n",
              "629                -0.30                0.641443               -0.006492   \n",
              "630                -0.05                0.396629               -0.006467   \n",
              "631                -0.21                0.684819               -0.005825   \n",
              "\n",
              "     Feature_Level_HML_MA12  Feature_Level_CMA_MA12  Feature_Level_RMW_MA12  \n",
              "622               -0.008700               -0.014225                0.003442  \n",
              "623               -0.007425               -0.011375                0.006008  \n",
              "624               -0.009667               -0.012075                0.003500  \n",
              "625                0.001242               -0.009092                0.002783  \n",
              "626                0.000850               -0.011717                0.002000  \n",
              "627                0.005908               -0.008275                0.005992  \n",
              "628                0.003317               -0.008408                0.004525  \n",
              "629                0.004675               -0.008567                0.005183  \n",
              "630                0.004633               -0.005875                0.003042  \n",
              "631                0.001267               -0.005392                0.001533  \n",
              "\n",
              "[10 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7c376cee-3ea6-443a-b60d-09ddd6f6ee6d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Regime</th>\n",
              "      <th>Predicted_month</th>\n",
              "      <th>Train_Start_Date</th>\n",
              "      <th>Train_End_Date</th>\n",
              "      <th>Train_Count</th>\n",
              "      <th>Feature_Importances</th>\n",
              "      <th>Predicted_Probabilities</th>\n",
              "      <th>Predicted_Winner</th>\n",
              "      <th>Allocated_Return</th>\n",
              "      <th>Equal_Weight_Return</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Tree_Depth</th>\n",
              "      <th>Prediction_Horizon_Months</th>\n",
              "      <th>Feature_Level_CPI%</th>\n",
              "      <th>Feature_Level_T10YFF</th>\n",
              "      <th>Feature_Level_CFNAI</th>\n",
              "      <th>Feature_Level_GARCH_1M</th>\n",
              "      <th>Feature_Level_SMB_MA12</th>\n",
              "      <th>Feature_Level_HML_MA12</th>\n",
              "      <th>Feature_Level_CMA_MA12</th>\n",
              "      <th>Feature_Level_RMW_MA12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>622</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-02-29</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>2024-01-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0013977369972425638, 0.038018446324997617, ...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.019800</td>\n",
              "      <td>-0.021050</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21033</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-0.14</td>\n",
              "      <td>0.726558</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>-0.008700</td>\n",
              "      <td>-0.014225</td>\n",
              "      <td>0.003442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>623</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-03-30</td>\n",
              "      <td>2019-03-30</td>\n",
              "      <td>2024-02-29</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0016301259096684801, 0.037638442878595385, ...</td>\n",
              "      <td>[0.518, 0.24600000000000016, 0.235999999999999...</td>\n",
              "      <td>SMB</td>\n",
              "      <td>0.007077</td>\n",
              "      <td>0.014250</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34301</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-0.83</td>\n",
              "      <td>0.780011</td>\n",
              "      <td>-0.010833</td>\n",
              "      <td>-0.007425</td>\n",
              "      <td>-0.011375</td>\n",
              "      <td>0.006008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>624</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-04-30</td>\n",
              "      <td>2019-04-30</td>\n",
              "      <td>2024-03-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0012938005390835628, 0.0377358490566038, 0....</td>\n",
              "      <td>[0.0, 0.625, 0.375, 0.0]</td>\n",
              "      <td>HML</td>\n",
              "      <td>-0.004375</td>\n",
              "      <td>-0.004725</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.39639</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>0.39</td>\n",
              "      <td>0.496170</td>\n",
              "      <td>-0.012017</td>\n",
              "      <td>-0.009667</td>\n",
              "      <td>-0.012075</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>625</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-05-30</td>\n",
              "      <td>2019-05-30</td>\n",
              "      <td>2024-04-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.030191507077435457, 0.4007972240327639...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.029700</td>\n",
              "      <td>-0.002500</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34885</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.20</td>\n",
              "      <td>0.944764</td>\n",
              "      <td>-0.007225</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>-0.009092</td>\n",
              "      <td>0.002783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>626</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>2019-06-30</td>\n",
              "      <td>2024-05-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.029698996655518374, 0.4463940544995044...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>-0.022375</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.29125</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-0.39</td>\n",
              "      <td>0.569002</td>\n",
              "      <td>-0.007208</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>-0.011717</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>627</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>2019-07-30</td>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.02203031776196578, 0.02203031776196578, 0.5...</td>\n",
              "      <td>[0.39999999999999925, 0.19999999999999962, 0.0...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.045480</td>\n",
              "      <td>0.036675</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.03961</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.327046</td>\n",
              "      <td>-0.006250</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>-0.008275</td>\n",
              "      <td>0.005992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>2019-08-30</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.14534915459476647, 0.2904117101155817,...</td>\n",
              "      <td>[0.5, 0.0, 0.0, 0.5]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.014000</td>\n",
              "      <td>-0.007675</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.00287</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>1.087011</td>\n",
              "      <td>-0.011025</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>-0.008408</td>\n",
              "      <td>0.004525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>629</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>2019-09-30</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.10367892976588623, 0.2994792196661857, 0.15...</td>\n",
              "      <td>[0.3333333333333329, 0.0, 0.16666666666666646,...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.003633</td>\n",
              "      <td>-0.009575</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.13892</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>-0.30</td>\n",
              "      <td>0.641443</td>\n",
              "      <td>-0.006492</td>\n",
              "      <td>0.004675</td>\n",
              "      <td>-0.008567</td>\n",
              "      <td>0.005183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>630</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>2019-10-30</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12433706540954631, 0.343946445449918, 0.057...</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.013800</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.18019</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0.396629</td>\n",
              "      <td>-0.006467</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>-0.005875</td>\n",
              "      <td>0.003042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>631</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-11-30</td>\n",
              "      <td>2019-11-30</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12983091547977074, 0.2729471722376977, 0.13...</td>\n",
              "      <td>[0.11111111111111088, 0.11111111111111088, 0.4...</td>\n",
              "      <td>CMA</td>\n",
              "      <td>-0.013122</td>\n",
              "      <td>-0.000150</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22920</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.21</td>\n",
              "      <td>0.684819</td>\n",
              "      <td>-0.005825</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>-0.005392</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows √ó 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7c376cee-3ea6-443a-b60d-09ddd6f6ee6d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7c376cee-3ea6-443a-b60d-09ddd6f6ee6d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7c376cee-3ea6-443a-b60d-09ddd6f6ee6d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3c8fe9a0-82ad-4770-88ea-34c0605664cd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c8fe9a0-82ad-4770-88ea-34c0605664cd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3c8fe9a0-82ad-4770-88ea-34c0605664cd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cumulative returns 2000-01-30 - 2024-11-30 - ML strategy: 1.7499 / Equal weight: 1.1529\n",
            "\n",
            "Cumulative returns 1972-04-30 - 2024-11-30 - ML strategy: 8.3267 / Equal weight: 3.5990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#RF2"
      ],
      "metadata": {
        "id": "JuCQA8Oco0kP"
      },
      "id": "JuCQA8Oco0kP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 ‚Äî second RF run under RF2\n",
        "if RF2 or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # RF2: Drop _MA12 and GARCH_1M\n",
        "    # -------------------\n",
        "    RF2_FEATURES = [f for f in FEATURES if not f.endswith('_MA12') and f != 'GARCH_1M']\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train    = 60\n",
        "    min_obs_regime      = 50\n",
        "    min_obs_train       = 0\n",
        "    use_regime_split    = False\n",
        "    default_hyperparams = False\n",
        "\n",
        "    use_fixed_window    = True\n",
        "    rolling_window_size = 60\n",
        "\n",
        "    n_jobs = -1\n",
        "\n",
        "    # -------------------\n",
        "    # Hyperparameter Settings\n",
        "    # -------------------\n",
        "    if default_hyperparams:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': 'sqrt',\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 1,\n",
        "            'bootstrap': True,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "    else:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': None,\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 5,\n",
        "            'bootstrap': False,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results_rf2 = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Skip if insufficient data or overlapping dates\n",
        "        if len(train_window) < min_months_train:\n",
        "            continue\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train using RF2_FEATURES\n",
        "        X_train = train_window[RF2_FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            continue\n",
        "\n",
        "        # Train RF\n",
        "        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Prepare X_test\n",
        "        X_test = train_window[RF2_FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            continue\n",
        "\n",
        "        # Predict and map probabilities\n",
        "        probs = rf_model.predict_proba(X_test)[0]\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, p in zip(rf_model.classes_, probs):\n",
        "            if cls in FACTORS:\n",
        "                full_probs[FACTORS.index(cls)] = p\n",
        "\n",
        "        # Calculate returns\n",
        "        allocated_return    = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Feature levels from RF2_FEATURES\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in RF2_FEATURES}\n",
        "\n",
        "        result = {\n",
        "            'Regime': 'NoRegime',\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_window['Date'].iloc[0],\n",
        "            'Train_End_Date': last_train_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': rf_model.classes_[probs.argmax()],\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': np.mean([t.tree_.max_depth for t in rf_model.estimators_]),\n",
        "            'Max_Tree_Depth': np.max([t.tree_.max_depth for t in rf_model.estimators_]),\n",
        "            'Prediction_Horizon_Months': ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                                         (Predicted_month.month - last_train_date.month)),\n",
        "            **feature_levels\n",
        "        }\n",
        "        results_rf2.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build RF2 results DataFrame\n",
        "    # -------------------\n",
        "    results_df_rf2 = pd.DataFrame(results_rf2)\n",
        "    print(\"Final results_df_rf2 columns:\", results_df_rf2.columns.tolist())\n",
        "    display(results_df_rf2.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # Cumulative returns (2000 onward & total)\n",
        "    # -------------------\n",
        "    filtered2 = results_df_rf2[results_df_rf2['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered2.empty:\n",
        "        cum_alloc2 = (1 + filtered2['Allocated_Return']).prod() - 1\n",
        "        cum_eq2    = (1 + filtered2['Equal_Weight_Return']).prod() - 1\n",
        "        print(f\"Cumulative 2000‚Äìpresent ‚Äî RF2: {cum_alloc2:.4f}  /  Equal: {cum_eq2:.4f}\")\n",
        "\n",
        "    if not results_df_rf2.empty:\n",
        "        cum_alloc_all2 = (1 + results_df_rf2['Allocated_Return']).prod() - 1\n",
        "        cum_eq_all2    = (1 + results_df_rf2['Equal_Weight_Return']).prod() - 1\n",
        "        print(f\"Total cumulative ‚Äî RF2: {cum_alloc_all2:.4f}  /  Equal: {cum_eq_all2:.4f}\")\n"
      ],
      "metadata": {
        "id": "CMbhV42UouKY"
      },
      "id": "CMbhV42UouKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient boosting\n"
      ],
      "metadata": {
        "id": "MSWv9xFlDbMz"
      },
      "id": "MSWv9xFlDbMz"
    },
    {
      "cell_type": "code",
      "source": [
        "if GB:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Minimum observations per regime if splitting\n",
        "    min_obs_train = 0         # Minimum total observations after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # Toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either fixed-size (rolling) or expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check that we have enough training rows (i.e., months)\n",
        "        if len(train_window) < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training rows ({len(train_window)} rows). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last training dates\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks (if enabled)\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active. Insufficient data in: {regime_str}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Use only training data for the current regime\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure the last training date is strictly before the test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Convert y_train from strings to numeric codes and save mapping\n",
        "        y_train_cat = y_train.astype('category')\n",
        "        mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "        y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "        # -------------------\n",
        "        # Set hyperparameters based on default_hyperparameters flag\n",
        "        # -------------------\n",
        "        if default_hyperparameters:\n",
        "            xgb_params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 3,\n",
        "                'learning_rate': 0.1,\n",
        "                'subsample': 1.0,\n",
        "                'colsample_bytree': 1.0,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss'\n",
        "            }\n",
        "        else:\n",
        "            # Use manually defined hyperparameters (from Optuna or otherwise)\n",
        "            xgb_params = {\n",
        "                'n_estimators': 500,\n",
        "                'max_depth': 15,\n",
        "                'learning_rate': 0.07,\n",
        "                'subsample': 1,\n",
        "                'colsample_bytree': 0.55,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss',\n",
        "                'min_child_weight': 2,\n",
        "                'gamma': 0.019\n",
        "            }\n",
        "\n",
        "        # Fit XGBoost gradient boosting classifier on numeric labels (full training, no early stopping)\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X_train, y_train_numeric)\n",
        "\n",
        "        # Prepare test data (using the last row in the training window)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = xgb_model.predict_proba(X_test)[0]\n",
        "        # Get predicted numeric class and convert back to original factor name\n",
        "        predicted_numeric = xgb_model.classes_[predicted_probabilities.argmax()]\n",
        "        predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "        # Map predicted probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for code, prob in zip(xgb_model.classes_, predicted_probabilities):\n",
        "            factor_name = mapping[code]\n",
        "            try:\n",
        "                idx = FACTORS.index(factor_name)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass  # Skip if factor not found in FACTORS\n",
        "\n",
        "        # Compute allocated return and equal weight return using the test row's factor returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Tree depth statistics are not required for XGB; set to None\n",
        "        avg_depth = None\n",
        "        max_depth = None\n",
        "\n",
        "        # Calculate prediction horizon (months ahead)\n",
        "        months_ahead = (Predicted_month.year - last_train_date.year) * 12 + (Predicted_month.month - last_train_date.month)\n",
        "\n",
        "        # Store the actual feature levels used in X_test\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        # Build the result dictionary for this iteration\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': xgb_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': xgb_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for GB\n",
        "    # -------------------\n",
        "    results_df_gb = pd.DataFrame(results)\n",
        "    print(\"Final results_df_gb columns:\", results_df_gb.columns.tolist())\n",
        "    display(results_df_gb.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # 4) Calculate and Print Cumulative Returns (Filtered: from 1 Jan 2000 onwards)\n",
        "    # -------------------\n",
        "    filtered_results = results_df_gb[results_df_gb['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered_results.empty:\n",
        "        cum_return_allocated = (1 + filtered_results['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal = (1 + filtered_results['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_pred_month = filtered_results.iloc[0]['Predicted_month']\n",
        "        last_pred_month = filtered_results.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_pred_month.date(), last_pred_month.date(),\n",
        "            cum_return_allocated, cum_return_equal))\n",
        "    else:\n",
        "        print(\"No predictions from 1 Jan 2000 onwards.\")\n",
        "\n",
        "    # -------------------\n",
        "    # 5) Calculate and Print Cumulative Returns for Total Time\n",
        "    # -------------------\n",
        "    if not results_df_gb.empty:\n",
        "        cum_return_allocated_total = (1 + results_df_gb['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal_total = (1 + results_df_gb['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_total_month = results_df_gb.iloc[0]['Predicted_month']\n",
        "        last_total_month = results_df_gb.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_total_month.date(), last_total_month.date(),\n",
        "            cum_return_allocated_total, cum_return_equal_total))\n",
        "    else:\n",
        "        print(\"No predictions available for total time.\")\n"
      ],
      "metadata": {
        "id": "9sWAd_BnDVlB"
      },
      "id": "9sWAd_BnDVlB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradint boosting loop"
      ],
      "metadata": {
        "id": "cqpzfBYhagcZ"
      },
      "id": "cqpzfBYhagcZ"
    },
    {
      "cell_type": "code",
      "source": [
        "if gb_loop:\n",
        "    import optuna\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    # Define global parameters required by the objective function:\n",
        "    min_months_train = 60       # Minimum training period (in months)\n",
        "    min_obs_regime = 50         # Minimum observations per regime (if regime splitting is used)\n",
        "    min_obs_train = 0           # Minimum training observations after dropna\n",
        "    use_regime_split = False    # Toggle regime-based training\n",
        "    use_fixed_window = True     # Use fixed (rolling) window if True; else, expanding window\n",
        "    rolling_window_size = 60    # Number of rows for the fixed window\n",
        "\n",
        "    # Define feature and factor lists, regime column name, and regime mapping:\n",
        "    FEATURES = ['CPI%', 'T10YFF', 'LEI%', 'Amihud', 'GARCH_1M']\n",
        "    FACTORS = ['SMB', 'HML', 'CMA', 'RMW']\n",
        "    REGIMES_COLUMN = 'Predicted_reg'\n",
        "    # Use your actual regime mapping; this is a dummy mapping for demonstration:\n",
        "    regime_short_mapping = {0: 'RegimeA', 1: 'RegimeB'}\n",
        "\n",
        "    # Ensure the sorted dataframe is defined globally\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # Global list for storing trial results for CSV logging later\n",
        "    trial_results = []\n",
        "\n",
        "    def objective(trial):\n",
        "        # Declare all global variables needed inside the function\n",
        "        global df_sorted, use_fixed_window, rolling_window_size, min_months_train\n",
        "        global use_regime_split, min_obs_regime, min_obs_train, FEATURES, FACTORS, REGIMES_COLUMN, regime_short_mapping\n",
        "\n",
        "        start_time = time.time()  # Start time of the trial\n",
        "\n",
        "        # Define hyperparameters; note n_jobs=-1 uses all available cores\n",
        "        xgb_params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'n_jobs': -1,\n",
        "            'random_state': 42,\n",
        "            'eval_metric': 'mlogloss'\n",
        "        }\n",
        "\n",
        "        cumulative_return_total = 0.0      # Cumulative return over entire period\n",
        "        cumulative_return_after2000 = 0.0    # Cumulative return for dates >= 2000-01-01\n",
        "\n",
        "        # Loop over test rows in df_sorted. Predictions are made as soon as training period is long enough.\n",
        "        for i in range(1, len(df_sorted)):\n",
        "            test_row = df_sorted.iloc[i]\n",
        "            Predicted_month = test_row['Date']\n",
        "\n",
        "            # Build training window based on the selected window type\n",
        "            if use_fixed_window:\n",
        "                start_idx = max(0, i - rolling_window_size)\n",
        "                train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "                if len(train_window) < rolling_window_size:\n",
        "                    continue  # Skip if window is too short\n",
        "            else:\n",
        "                train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "            # Check that the training period is long enough\n",
        "            start_date = train_window['Date'].iloc[0]\n",
        "            training_months = (Predicted_month.year - start_date.year) * 12 + (Predicted_month.month - start_date.month)\n",
        "            if training_months < min_months_train:\n",
        "                continue  # Skip if training period is too short\n",
        "\n",
        "            # Optional regime-based splitting if enabled\n",
        "            if use_regime_split:\n",
        "                regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                if insufficient_regimes:\n",
        "                    continue\n",
        "                current_regime = test_row[REGIMES_COLUMN]\n",
        "                train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                if len(train_window) < min_obs_regime:\n",
        "                    continue\n",
        "\n",
        "            # Ensure the last training date is strictly before the test date\n",
        "            last_train_date = train_window['Date'].iloc[-1]\n",
        "            if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                continue\n",
        "\n",
        "            # Prepare training data\n",
        "            X_train = train_window[FEATURES].dropna()\n",
        "            y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "            if len(X_train) < min_obs_train:\n",
        "                continue\n",
        "\n",
        "            # Convert categorical target to numeric codes\n",
        "            y_train_cat = y_train.astype('category')\n",
        "            mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "            y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "            # Train the XGBoost model using the trial's hyperparameters\n",
        "            model = XGBClassifier(**xgb_params)\n",
        "            model.fit(X_train, y_train_numeric)\n",
        "\n",
        "            # Prepare test data: use the last row from the training window as test features\n",
        "            X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "            if X_test.empty:\n",
        "                continue\n",
        "\n",
        "            predicted_probabilities = model.predict_proba(X_test)[0]\n",
        "            predicted_numeric = model.classes_[predicted_probabilities.argmax()]\n",
        "            predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "            # Map predicted probabilities onto the full set of FACTORS\n",
        "            full_probs = np.zeros(len(FACTORS))\n",
        "            for code, prob in zip(model.classes_, predicted_probabilities):\n",
        "                factor_name = mapping[code]\n",
        "                try:\n",
        "                    idx = FACTORS.index(factor_name)\n",
        "                    full_probs[idx] = prob\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "            cumulative_return_total += allocated_return\n",
        "            if Predicted_month >= pd.Timestamp('2000-01-01'):\n",
        "                cumulative_return_after2000 += allocated_return\n",
        "\n",
        "        # Record trial runtime\n",
        "        elapsed = time.time() - start_time\n",
        "        minutes = int(elapsed // 60)\n",
        "        seconds = int(elapsed % 60)\n",
        "        runtime_str = f\"{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "        # Print trial summary\n",
        "        print(f\"\\nTrial {trial.number} finished in {runtime_str}\")\n",
        "        print(f\"Hyperparameters: {trial.params}\")\n",
        "        print(f\"Cumulative Return (after 2000): {cumulative_return_after2000:.4f}\")\n",
        "        print(f\"Cumulative Return (total): {cumulative_return_total:.4f}\\n\")\n",
        "\n",
        "        # Save trial details (formatting floats with a comma as the decimal separator)\n",
        "        trial_record = {\n",
        "            'Trial': trial.number,\n",
        "            'Runtime': runtime_str,\n",
        "            'CumulativeReturnAfter2000': str(f\"{cumulative_return_after2000:.4f}\").replace('.', ','),\n",
        "            'CumulativeReturnTotal': str(f\"{cumulative_return_total:.4f}\").replace('.', ','),\n",
        "            **trial.params\n",
        "        }\n",
        "        trial_results.append(trial_record)\n",
        "\n",
        "        # Return cumulative return after 2000 as the objective value\n",
        "        return cumulative_return_after2000\n",
        "\n",
        "    def print_trial_info(study, trial):\n",
        "        best_trial = study.best_trial\n",
        "        print(\"\\n=== Current Best Trial ===\")\n",
        "        print(f\"Trial {best_trial.number}: Value: {best_trial.value:.4f}\")\n",
        "        print(f\"Hyperparameters: {best_trial.params}\\n\")\n",
        "\n",
        "    # Create and run the Optuna study\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=30, callbacks=[print_trial_info])\n",
        "\n",
        "    # After the study, save all trial results to CSV (using ';' as separator)\n",
        "    results_df = pd.DataFrame(trial_results)\n",
        "    results_df.to_csv(\"optuna_trials_results.csv\", sep=\";\", index=False)\n",
        "    print(\"All trial results saved to optuna_trials_results.csv\")\n"
      ],
      "metadata": {
        "id": "K6JoZwTdQ8lL"
      },
      "id": "K6JoZwTdQ8lL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid / t√§√§ s√§ilytt√§√§ random forestin dataframen mut averagee painot ja laskee allocated returns nist√§"
      ],
      "metadata": {
        "id": "0HZuBVFOW8qu"
      },
      "id": "0HZuBVFOW8qu"
    },
    {
      "cell_type": "code",
      "source": [
        "if Hybrid:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from functools import reduce\n",
        "\n",
        "    # Choose which model DataFrames to combine by name\n",
        "    MODEL_DF_NAMES = ['results_df_rf', 'results_df_rf2']  # e.g. ['results_df_rf', 'results_df_rf2', 'results_df_gb']\n",
        "\n",
        "    # Collect the actual DataFrames, error if missing\n",
        "    model_dfs = {}\n",
        "    for name in MODEL_DF_NAMES:\n",
        "        if name in locals():\n",
        "            model_dfs[name] = locals()[name]\n",
        "        else:\n",
        "            raise ValueError(f\"DataFrame '{name}' not found in locals()\")\n",
        "\n",
        "    # Build list of probability subsets, renaming for merge\n",
        "    prob_dfs = []\n",
        "    for name, df_model in model_dfs.items():\n",
        "        subset = df_model[['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "        subset.rename(columns={'Predicted_Probabilities': f'Prob_{name}'}, inplace=True)\n",
        "        prob_dfs.append(subset)\n",
        "\n",
        "    # Merge all on Predicted_month\n",
        "    hybrid_temp = reduce(lambda left, right: pd.merge(left, right, on='Predicted_month'), prob_dfs)\n",
        "\n",
        "    # Average the probabilities across the selected models\n",
        "    prob_cols = [col for col in hybrid_temp.columns if col.startswith('Prob_')]\n",
        "    hybrid_temp['Predicted_Probabilities'] = hybrid_temp[prob_cols].apply(\n",
        "        lambda row: sum(np.array(row[col]) for col in prob_cols) / len(prob_cols),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Helper to compute allocated return\n",
        "    def compute_allocated_return(prob_vector, factor_returns):\n",
        "        return np.dot(prob_vector, factor_returns)\n",
        "\n",
        "    # Compute the hybrid allocated return\n",
        "    hybrid_temp['Hybrid_Allocated_Return'] = hybrid_temp.apply(\n",
        "        lambda row: compute_allocated_return(\n",
        "            row['Predicted_Probabilities'],\n",
        "            df.loc[df['Date'] == row['Predicted_month'], FACTORS].values.flatten()\n",
        "        ) if not df.loc[df['Date'] == row['Predicted_month'], FACTORS].empty else np.nan,\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # Merge hybrid results back into the primary DataFrame (first in list)\n",
        "    base_df = model_dfs[MODEL_DF_NAMES[0]]\n",
        "    hybrid_df = base_df.merge(\n",
        "        hybrid_temp[['Predicted_month', 'Predicted_Probabilities', 'Hybrid_Allocated_Return']],\n",
        "        on='Predicted_month',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # Override with hybrid values and drop temps\n",
        "    hybrid_df['Predicted_Probabilities'] = hybrid_df['Predicted_Probabilities_y']\n",
        "    hybrid_df['Allocated_Return']          = hybrid_df['Hybrid_Allocated_Return']\n",
        "    drop_cols = ['Predicted_Probabilities_x', 'Predicted_Probabilities_y', 'Hybrid_Allocated_Return'] + prob_cols\n",
        "    hybrid_df.drop(columns=drop_cols, inplace=True)\n",
        "\n",
        "    print(\"Hybrid hybrid_df created combining:\", MODEL_DF_NAMES)\n",
        "    display(hybrid_df.head())"
      ],
      "metadata": {
        "id": "JKYtkk2MWPC0"
      },
      "id": "JKYtkk2MWPC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model evaluation"
      ],
      "metadata": {
        "id": "A1BqWL_QXA8e"
      },
      "id": "A1BqWL_QXA8e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell¬†3 ‚Äî collect both RF and RF2 (and others) into your dict\n",
        "results_dfs = {}\n",
        "\n",
        "if RF:\n",
        "    results_dfs[\"Random Forest\"] = results_df_rf.copy()\n",
        "    print(\"Results from Random Forest added.\")\n",
        "\n",
        "if RF2:\n",
        "    results_dfs[\"Random Forest 2\"] = results_df_rf2.copy()\n",
        "    print(\"Results from Random Forest¬†2 added.\")\n",
        "\n",
        "if GB:\n",
        "    results_dfs[\"Gradient Boosting\"] = results_df_gb.copy()\n",
        "    print(\"Results from Gradient Boosting added.\")\n",
        "\n",
        "if Hybrid:\n",
        "    results_dfs[\"Hybrid\"] = hybrid_df.copy()\n",
        "    print(\"Results from Hybrid Model added.\")\n",
        "\n",
        "if not results_dfs:\n",
        "    raise ValueError(\"No valid model was selected; set at least one of [RF, RF2, GB, Hybrid] to True.\")\n",
        "\n",
        "print(\"\\nAvailable model results:\")\n",
        "for name, df in results_dfs.items():\n",
        "    print(f\" ‚Ä¢ {name}: {df.shape[0]} rows √ó {df.shape[1]} cols\")\n",
        "\n",
        "from IPython.display import display\n",
        "for name, df in results_dfs.items():\n",
        "    print(f\"\\n=== {name} (first 5 rows) ===\")\n",
        "    display(df.head())"
      ],
      "metadata": {
        "id": "FOzCGJrxXKUm"
      },
      "id": "FOzCGJrxXKUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Increase column width so no text is truncated\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Define the date range\n",
        "start_date = pd.to_datetime('1968-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Dictionary to store filtered results for each model using the new naming format.\n",
        "filtered_results_dfs = {}\n",
        "\n",
        "# Loop through each model's results dataframe in results_dfs and add numbering.\n",
        "for i, (model_name, df) in enumerate(results_dfs.items(), 1):\n",
        "    new_model_name = f\"ML{i}: {model_name}\"\n",
        "\n",
        "    # Convert 'Predicted_month' to datetime if not already\n",
        "    df['Predicted_month'] = pd.to_datetime(df['Predicted_month'])\n",
        "\n",
        "    # Filter the DataFrame within the specified date range and sort by date.\n",
        "    filtered_df = df[(df['Predicted_month'] >= start_date) & (df['Predicted_month'] <= end_date)].copy().sort_values('Predicted_month')\n",
        "\n",
        "    # Store the filtered dataframe in our new dictionary using the new model name.\n",
        "    filtered_results_dfs[new_model_name] = filtered_df\n",
        "\n",
        "    # Display the filtered results with a header showing the new model name.\n",
        "    print(f\"\\n=== Filtered Results for Model '{new_model_name}' ===\")\n",
        "    display(filtered_df)\n",
        "\n",
        "# Reset column width option to default after display.\n",
        "pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "id": "JmpzyRpGWP0n"
      },
      "id": "JmpzyRpGWP0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d0f495-1290-403b-86a2-7bd1c2c12814",
      "metadata": {
        "tags": [],
        "id": "88d0f495-1290-403b-86a2-7bd1c2c12814"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Use FACTORS directly as labels for the confusion matrix axes.\n",
        "labels = FACTORS\n",
        "\n",
        "# Determine how many models we have.\n",
        "num_models = len(results_dfs)\n",
        "\n",
        "# Create a figure with 1 row and num_models columns.\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_models, figsize=(8 * num_models, 8))\n",
        "if num_models == 1:\n",
        "    axes = [axes]  # Make it iterable\n",
        "\n",
        "# A dictionary to hold overall metrics for each model.\n",
        "overall_metrics_dict = {}\n",
        "\n",
        "# Iterate over the dictionary and plot the overall confusion matrix for each model.\n",
        "for i, (model_key, results_df) in enumerate(results_dfs.items()):\n",
        "    # Extract true and predicted winners.\n",
        "    all_true = results_df['Actual_Winner']\n",
        "    all_pred = results_df['Predicted_Winner']\n",
        "\n",
        "    # Compute the overall confusion matrix.\n",
        "    cm_total = confusion_matrix(all_true, all_pred, labels=labels)\n",
        "\n",
        "    # Plot the confusion matrix for this model.\n",
        "    sns.heatmap(cm_total, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[i])\n",
        "    axes[i].set_xlabel(\"Predicted Labels\")\n",
        "    axes[i].set_ylabel(\"True Labels\")\n",
        "    axes[i].set_title(f\"{model_key} (n = {len(all_true)})\")\n",
        "\n",
        "    # Compute overall performance metrics.\n",
        "    overall_metrics_dict[model_key] = {\n",
        "         \"Accuracy\": accuracy_score(all_true, all_pred),\n",
        "         \"Precision\": precision_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"Recall\": recall_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"F1 Score\": f1_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"Samples\": len(all_true)\n",
        "    }\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.show()\n",
        "\n",
        "# Build an HTML table summarizing overall performance metrics for all models.\n",
        "html_overall = \"<h3>Overall Performance Metrics Summary (Side by Side)</h3>\"\n",
        "html_overall += \"<table border='1' cellpadding='5'><tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th><th>Samples</th></tr>\"\n",
        "\n",
        "for model_key, metrics in overall_metrics_dict.items():\n",
        "    html_overall += f\"<tr><td>{model_key}</td><td>{metrics['Accuracy']:.4f}</td><td>{metrics['Precision']:.4f}</td>\"\n",
        "    html_overall += f\"<td>{metrics['Recall']:.4f}</td><td>{metrics['F1 Score']:.4f}</td><td>{metrics['Samples']}</td></tr>\"\n",
        "\n",
        "html_overall += \"</table>\"\n",
        "\n",
        "# Display the metrics table.\n",
        "display(HTML(html_overall))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance"
      ],
      "metadata": {
        "id": "-U5ovbRVPoPM"
      },
      "id": "-U5ovbRVPoPM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5",
      "metadata": {
        "tags": [],
        "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume FEATURES is defined (e.g., FEATURES = ['CPI%', 'T10YFF', 'LEI%', 'Amihud', 'GARCH_1M'])\n",
        "# and results_dfs is a dictionary with keys like \"RF\", \"GB\", (and maybe \"Hybrid\")\n",
        "# where each value is a DataFrame that has a column \"Feature_Importances\" (an array).\n",
        "\n",
        "# 1. Compute overall average feature importances for each model.\n",
        "model_importances = {}\n",
        "for model_key, df in results_dfs.items():\n",
        "    # Stack the arrays from the \"Feature_Importances\" column and average over predictions.\n",
        "    model_importances[model_key] = np.vstack(df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# 2. Create a DataFrame from the computed importances.\n",
        "# Rows: features, Columns: model keys.\n",
        "importance_df = pd.DataFrame(model_importances, index=FEATURES)\n",
        "\n",
        "# Optional: sort features by overall mean importance (averaged across models) so that\n",
        "# the most important features appear on top.\n",
        "importance_df['Mean'] = importance_df.mean(axis=1)\n",
        "importance_df = importance_df.sort_values(by='Mean', ascending=False)\n",
        "sorted_features = importance_df.index.tolist()\n",
        "importance_df = importance_df.drop(columns=['Mean'])\n",
        "\n",
        "# 3. Plot a grouped horizontal bar chart.\n",
        "models = importance_df.columns.tolist()\n",
        "n_models = len(models)\n",
        "n_features = len(sorted_features)\n",
        "y = np.arange(n_features)  # base positions for each feature group\n",
        "bar_height = 0.8 / n_models  # total group thickness is 0.8\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(4, n_features * 0.6)))\n",
        "for i, model in enumerate(models):\n",
        "    # Calculate an offset for each model in the group.\n",
        "    offset = (i - n_models/2) * bar_height + bar_height/2\n",
        "    ax.barh(y + offset, importance_df.loc[sorted_features, model], height=bar_height, label=model)\n",
        "\n",
        "ax.set_yticks(y)\n",
        "ax.set_yticklabels(sorted_features)\n",
        "ax.invert_yaxis()  # so the top feature is at the top\n",
        "ax.set_xlabel(\"Average Feature Importance\")\n",
        "ax.set_title(\"Comparison of Overall Average Feature Importances Across Models\")\n",
        "ax.legend(title=\"Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Allocation chart"
      ],
      "metadata": {
        "id": "bhRxmOoFPtEI"
      },
      "id": "bhRxmOoFPtEI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8",
      "metadata": {
        "tags": [],
        "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Determine how many models to plot\n",
        "num_models = len(results_dfs)\n",
        "\n",
        "# Create subplots: one row per model, sharing the x-axis\n",
        "fig, axes = plt.subplots(nrows=num_models, ncols=1, figsize=(12, 6 * num_models), sharex=True)\n",
        "if num_models == 1:\n",
        "    axes = [axes]  # Ensure iterable if only one model\n",
        "\n",
        "# Loop through each model's result dataframe\n",
        "for ax, (model_key, df_model) in zip(axes, results_dfs.items()):\n",
        "    df_temp = df_model.copy()\n",
        "\n",
        "    # 1) Convert Predicted_month to datetime (robust handling)\n",
        "    df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "\n",
        "    # 2) Drop rows with invalid dates\n",
        "    df_temp = df_temp.dropna(subset=[\"Predicted_month\"])\n",
        "    df_temp = df_temp.sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "    # 3) Stack probabilities\n",
        "    full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "    probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "    probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "    # 4) Sort by date again (just in case)\n",
        "    probability_df = probability_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    # 5) Plot stackplot\n",
        "    ax.stackplot(\n",
        "        probability_df[\"Date\"],\n",
        "        [probability_df[col] for col in FACTORS],\n",
        "        labels=FACTORS,\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    # ‚úÖ Legend: top right, inside chart, solid white background\n",
        "    ax.legend(\n",
        "        loc=\"upper right\",\n",
        "        fontsize=\"small\",\n",
        "        title=\"Factors\",\n",
        "        frameon=True,\n",
        "        framealpha=1.0,      # fully opaque\n",
        "        facecolor='white',   # solid background\n",
        "        edgecolor='black'    # optional: to match style\n",
        "    )\n",
        "\n",
        "    # Set title, axes, and formatting\n",
        "    ax.set_title(f\"{model_key} Outperforming Probabilities\", fontsize=14)\n",
        "    ax.set_ylabel(\"Probability\", fontsize=12)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Trim x-axis to data range\n",
        "    ax.set_xlim(probability_df[\"Date\"].min(), probability_df[\"Date\"].max())\n",
        "\n",
        "# Final x-axis label and formatting\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Factor weight analysis"
      ],
      "metadata": {
        "id": "PY247n9MPyWG"
      },
      "id": "PY247n9MPyWG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toggle:\n",
        "combine_all_models = False  # Set to True to combine all models into the same charts; False for individual charts per model\n",
        "\n",
        "# Set the date range for viewing.\n",
        "start_date = pd.to_datetime(\"1968-07-30\")\n",
        "end_date   = pd.to_datetime(\"2024-11-30\")\n",
        "\n",
        "# Define static equal weight value.\n",
        "equal_weight = 1 / len(FACTORS)  # e.g., for 5 factors equal_weight = 0.20\n",
        "\n",
        "if combine_all_models:\n",
        "    # Combined charts: One set of subplots (one per factor) for all models.\n",
        "    n_factors = len(FACTORS)\n",
        "    fig, axs = plt.subplots(n_factors, 1, figsize=(12, 4 * n_factors), sharex=False)\n",
        "    if n_factors == 1:\n",
        "        axs = [axs]  # ensure axs is iterable\n",
        "\n",
        "    for i, factor in enumerate(FACTORS):\n",
        "        ax = axs[i]\n",
        "        min_dates = []\n",
        "        max_dates = []\n",
        "\n",
        "        # Loop through each model's results\n",
        "        for model_key, df_model in results_dfs.items():\n",
        "            df_temp = df_model.copy()\n",
        "            df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "            df_temp = df_temp.dropna(subset=[\"Predicted_month\"]).sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "            # Stack predicted probabilities into a DataFrame.\n",
        "            full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "            probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "            probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "            # Filter to desired date range.\n",
        "            mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "            filtered_df = probability_df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "            if filtered_df.empty:\n",
        "                continue\n",
        "\n",
        "            ax.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "                    label=f\"{factor}_{model_key}\", linewdth=0.6)\n",
        "\n",
        "            min_dates.append(filtered_df[\"Date\"].min())\n",
        "            max_dates.append(filtered_df[\"Date\"].max())\n",
        "\n",
        "        # Set x-axis limits to exactly the data span (if any data exist)\n",
        "        if min_dates and max_dates:\n",
        "            ax.set_xlim(min(min_dates), max(max_dates))\n",
        "\n",
        "        # Draw the static equal weight horizontal line.\n",
        "        ax.axhline(equal_weight, color='black', linestyle='--',\n",
        "                   label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "        ax.set_title(f\"{factor} Predicted Probabilities Across Models\", fontsize=14)\n",
        "        ax.set_ylabel(\"Probability\", fontsize=12)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        ax.legend(loc='best', fontsize='small')\n",
        "\n",
        "    plt.xlabel(\"Date\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # Separate charts: Loop over each model and for each factor create individual charts.\n",
        "    for model_key, df_model in results_dfs.items():\n",
        "        for factor in FACTORS:\n",
        "            df_temp = df_model.copy()\n",
        "            df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "            df_temp = df_temp.dropna(subset=[\"Predicted_month\"]).sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "            full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "            probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "            probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "            # Filter to desired date range.\n",
        "            mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "            filtered_df = probability_df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "            if filtered_df.empty:\n",
        "                continue\n",
        "\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "                     label=f\"{factor} Predicted Probability\", color='blue', linewidth=0.6)\n",
        "\n",
        "            plt.axhline(equal_weight, color='black', linestyle='--',\n",
        "                        label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "            plt.fill_between(filtered_df[\"Date\"],\n",
        "                             filtered_df[factor],\n",
        "                             equal_weight,\n",
        "                             where=(filtered_df[factor] > equal_weight),\n",
        "                             interpolate=True, color='green', alpha=0.3, label='Overweight')\n",
        "            plt.fill_between(filtered_df[\"Date\"],\n",
        "                             filtered_df[factor],\n",
        "                             equal_weight,\n",
        "                             where=(filtered_df[factor] < equal_weight),\n",
        "                             interpolate=True, color='red', alpha=0.3, label='Underweight')\n",
        "\n",
        "            plt.title(f\"{model_key} - Over/Under Weight for {factor}\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Probability\")\n",
        "            plt.ylim(0, 1)\n",
        "            # Set x-axis limits to exactly where data exists.\n",
        "            plt.xlim(filtered_df[\"Date\"].min(), filtered_df[\"Date\"].max())\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "5My7U6ny7GAJ"
      },
      "id": "5My7U6ny7GAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Total outperforming probabilities"
      ],
      "metadata": {
        "id": "6wkhRaZrP6aF"
      },
      "id": "6wkhRaZrP6aF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Compute average predicted probabilities per model\n",
        "# ----------------------------------------------------\n",
        "avg_probs_dict = {}\n",
        "avg_highest_factor_weight_dict = {}\n",
        "\n",
        "for model_key, df_model in results_dfs.items():\n",
        "    full_probs = np.vstack(df_model[\"Predicted_Probabilities\"].values)\n",
        "\n",
        "    # 1a. Compute the average probabilities across all rows\n",
        "    avg = full_probs.mean(axis=0)\n",
        "    avg_probs_dict[model_key] = pd.Series(avg, index=FACTORS)\n",
        "\n",
        "    # 1b. Compute the average of the highest weight factor\n",
        "    #     (for each time step, pick the max factor weight, then average those)\n",
        "    avg_highest_factor_weight_dict[model_key] = full_probs.max(axis=1).mean()\n",
        "\n",
        "# Create a DataFrame where rows = models, columns = factors\n",
        "avg_probs_df = pd.DataFrame(avg_probs_dict).T\n",
        "avg_probs_df.index.name = \"Model\"\n",
        "avg_probs_df = avg_probs_df.round(4)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Generate consistent factor colors from stackplot\n",
        "# ----------------------------------------------------\n",
        "# Use a dummy stackplot to extract the assigned factor colors\n",
        "_, ax_dummy = plt.subplots()\n",
        "dummy_data = np.random.rand(10, len(FACTORS))\n",
        "dummy_dates = pd.date_range(\"2000-01-01\", periods=10)\n",
        "stack = ax_dummy.stackplot(dummy_dates, dummy_data.T, labels=FACTORS, alpha=0.8)\n",
        "plt.close()  # We don‚Äôt want to display this\n",
        "\n",
        "# Build color map: factor name ‚Üí RGBA color\n",
        "factor_colors = {factor: poly.get_facecolor()[0] for factor, poly in zip(FACTORS, stack)}\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. Display HTML Table of average probabilities\n",
        "# ----------------------------------------------------\n",
        "html_table = avg_probs_df.reset_index().to_html(index=False, classes=\"table table-striped table-bordered\", border=0)\n",
        "display(HTML(\"<h3>Average Outperforming Probabilities by Model</h3>\" + html_table))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. Print average of the highest factor weight by model\n",
        "# ----------------------------------------------------\n",
        "display(HTML(\"<h4>Average of the Highest Factor Weight by Model</h4>\"))\n",
        "for model_key, avg_highest in avg_highest_factor_weight_dict.items():\n",
        "    display(HTML(f\"<p><strong>{model_key}:</strong> {avg_highest:.4f}</p>\"))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Stacked Bar Chart with Consistent Colors and Labels\n",
        "# ----------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bottom = np.zeros(len(avg_probs_df))\n",
        "x = np.arange(len(avg_probs_df))\n",
        "\n",
        "for factor in FACTORS:\n",
        "    values = avg_probs_df[factor].values\n",
        "    bars = ax.bar(x, values, bottom=bottom,\n",
        "                  label=factor,\n",
        "                  color=factor_colors[factor],\n",
        "                  edgecolor=\"white\",\n",
        "                  linewidth=0.5)\n",
        "\n",
        "    # Centered labels\n",
        "    for bar, val in zip(bars, values):\n",
        "        if val > 0.03:\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_y() + bar.get_height() / 2,\n",
        "                f\"{val * 100:.1f}%\",\n",
        "                ha=\"center\", va=\"center\", fontsize=9, color=\"white\"\n",
        "            )\n",
        "\n",
        "    bottom += values\n",
        "\n",
        "# Final touches\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(avg_probs_df.index)\n",
        "ax.set_ylabel(\"Strategy average factor weights\")\n",
        "ax.set_title(\"Strategy average factor weights\")\n",
        "ax.legend(title=\"Factor\", loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7GvhTSes34Ci"
      },
      "id": "7GvhTSes34Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Return data"
      ],
      "metadata": {
        "id": "M0sRKlsm6Yja"
      },
      "id": "M0sRKlsm6Yja"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 0: Define column orders based on your requirements\n",
        "# ------------------------------------------------------------------------------\n",
        "# Common columns that are identical across all models.\n",
        "common_cols = ['Predicted_month', 'Mkt', 'RF', 'Mkt-RF', 'Us_standard'] + FACTORS + ['Equal_Weight_Return', 'Actual_Winner']\n",
        "\n",
        "# Model-specific columns that will be renamed.\n",
        "model_specific_cols = ['Allocated_Return', 'Predicted_Winner']\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 1. Build a base common DataFrame from one model's merged results.\n",
        "# ------------------------------------------------------------------------------\n",
        "# Take the first model as the base to extract common columns.\n",
        "base_key, base_df = list(results_dfs.items())[0]\n",
        "base_df = base_df.copy()\n",
        "base_df['Predicted_month'] = pd.to_datetime(base_df['Predicted_month'], errors='coerce')\n",
        "\n",
        "# Merge with df_sorted (the master DataFrame sorted by date) on date.\n",
        "base_df_local = base_df.merge(df_sorted, left_on='Predicted_month', right_on='Date', how='left')\n",
        "common_df = base_df_local[[c for c in common_cols if c in base_df_local.columns]].copy()\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2. Process each model individually to extract the model-specific columns.\n",
        "# ------------------------------------------------------------------------------\n",
        "# We'll assign a new display name using numbering such that each model gets:\n",
        "# \"ML{number}: {Model Name}\"\n",
        "model_dfs = []         # Will hold one DataFrame per model.\n",
        "new_model_names = []   # To store new model names.\n",
        "for i, (model_key, df_model) in enumerate(results_dfs.items(), 1):\n",
        "    new_model_name = f\"ML{i}: {model_key}\"  # New display name.\n",
        "    new_model_names.append(new_model_name)\n",
        "\n",
        "    df_temp = df_model.copy()\n",
        "    df_temp['Predicted_month'] = pd.to_datetime(df_temp['Predicted_month'], errors='coerce')\n",
        "\n",
        "    # Merge with df_sorted on 'Predicted_month' = 'Date'\n",
        "    df_temp_local = df_temp.merge(df_sorted, left_on='Predicted_month', right_on='Date', how='left')\n",
        "\n",
        "    # Keep only the 'Predicted_month' plus the model-specific columns.\n",
        "    subset_cols = ['Predicted_month'] + [col for col in model_specific_cols if col in df_temp_local.columns]\n",
        "    df_subset = df_temp_local[subset_cols].copy()\n",
        "\n",
        "    # Rename model-specific columns with the new model name.\n",
        "    rename_dict = {}\n",
        "    for col in model_specific_cols:\n",
        "        if col in df_subset.columns:\n",
        "            rename_dict[col] = f\"{new_model_name} {col}\"\n",
        "    df_subset.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    model_dfs.append(df_subset)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3. Merge each model-specific DataFrame with the common DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "combined_df = common_df.copy()\n",
        "for df_sub in model_dfs:\n",
        "    combined_df = combined_df.merge(df_sub, on='Predicted_month', how='left')\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4. Reorder the columns to match the desired order.\n",
        "# ------------------------------------------------------------------------------\n",
        "benchmark_cols = ['Mkt', 'RF', 'Mkt-RF', 'Us_standard']\n",
        "common_order = ['Predicted_month'] + benchmark_cols + FACTORS\n",
        "# Model-specific allocated return columns.\n",
        "allocated_cols = [f\"{name} Allocated_Return\" for name in new_model_names]\n",
        "# Model-specific predicted winner columns.\n",
        "predicted_cols = [f\"{name} Predicted_Winner\" for name in new_model_names]\n",
        "\n",
        "final_order = common_order + ['Equal_Weight_Return'] + allocated_cols + ['Actual_Winner'] + predicted_cols\n",
        "final_order = [col for col in final_order if col in combined_df.columns]\n",
        "\n",
        "combined_df = combined_df[final_order].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 5. Display the final combined results table.\n",
        "# ------------------------------------------------------------------------------\n",
        "display(combined_df)\n",
        "print(\"\\nFirst date in 'Predicted_month':\", pd.to_datetime(combined_df['Predicted_month']).min())\n",
        "print(\"Last date in 'Predicted_month':\", pd.to_datetime(combined_df['Predicted_month']).max())\n"
      ],
      "metadata": {
        "id": "rA982P5EGhj4"
      },
      "id": "rA982P5EGhj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "# Define your date range\n",
        "start_date = pd.to_datetime('1950-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "\n",
        "# Create a filtered copy of combined_df (so the original data isn't lost)\n",
        "filtered_df = combined_df.loc[\n",
        "    (combined_df['Predicted_month'] >= start_date) &\n",
        "    (combined_df['Predicted_month'] <= end_date)\n",
        "].copy()\n",
        "\n",
        "# Ensure \"Year\" column exists in filtered_df\n",
        "if 'Year' not in filtered_df.columns:\n",
        "    filtered_df['Year'] = filtered_df['Predicted_month'].dt.year\n",
        "\n",
        "# Identify ML return columns and create a name map to remove \"Allocated_Return\"\n",
        "ml_return_cols = [c for c in filtered_df.columns if 'Allocated_Return' in c]\n",
        "ml_name_map = {ml: ml.replace(\"Allocated_Return\", \"\").strip() for ml in ml_return_cols}\n",
        "\n",
        "# Helper: Compute annual metrics (RF-adjusted)\n",
        "def compute_annual_metrics(returns: pd.Series, rf: pd.Series):\n",
        "    returns = returns.dropna()\n",
        "    if returns.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    rf = rf.reindex(returns.index)\n",
        "    ann_ret = (1 + returns).prod() - 1\n",
        "    ann_rf  = (1 + rf).prod() - 1\n",
        "    ann_ex_ret = ann_ret - ann_rf\n",
        "    ann_vol = (returns - rf).std() * np.sqrt(12)\n",
        "    ann_sharpe = ann_ex_ret / ann_vol if ann_vol else np.nan\n",
        "    return ann_ret, ann_vol, ann_sharpe\n",
        "\n",
        "# Build df_metrics (raw values) and df_excess (excess over Equal Weight)\n",
        "metrics_rows, excess_rows = [], []\n",
        "for year, grp in filtered_df.groupby('Year'):\n",
        "    ew_ret, ew_vol, ew_sharpe = compute_annual_metrics(grp['Equal_Weight_Return'], grp['RF'])\n",
        "    row_m = {'Year': year,\n",
        "             'Equal_Weight Return': ew_ret,\n",
        "             'Equal_Weight Vol':    ew_vol,\n",
        "             'Equal_Weight Sharpe': ew_sharpe}\n",
        "    row_e = {'Year': year}\n",
        "\n",
        "    for ml in ml_return_cols:\n",
        "        ml_short = ml_name_map[ml]\n",
        "        ml_ret, ml_vol, ml_sharpe = compute_annual_metrics(grp[ml], grp['RF'])\n",
        "        row_m[f\"{ml_short} Return\"]  = ml_ret\n",
        "        row_m[f\"{ml_short} Vol\"]     = ml_vol\n",
        "        row_m[f\"{ml_short} Sharpe\"]  = ml_sharpe\n",
        "\n",
        "        row_e[f\"{ml_short} Excess Return\"]  = ml_ret - ew_ret\n",
        "        row_e[f\"{ml_short} Excess Vol\"]     = ml_vol - ew_vol\n",
        "        row_e[f\"{ml_short} Excess Sharpe\"]  = ml_sharpe - ew_sharpe\n",
        "\n",
        "    metrics_rows.append(row_m)\n",
        "    excess_rows.append(row_e)\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_rows).set_index('Year').sort_index()\n",
        "df_excess  = pd.DataFrame(excess_rows).set_index('Year').sort_index()\n",
        "\n",
        "# Function: Insert newline breaks for column names longer than max_width characters.\n",
        "def wrap_colname(colname, max_width=15):\n",
        "    lines = textwrap.wrap(colname, width=max_width)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# Apply wrapping to all column names\n",
        "df_metrics.columns = [wrap_colname(col) for col in df_metrics.columns]\n",
        "df_excess.columns  = [wrap_colname(col) for col in df_excess.columns]\n",
        "\n",
        "# Function: Style dataframe to enable multiline headers and format numbers with a max of 3 decimals.\n",
        "def style_with_wrapping_and_format(df):\n",
        "    styled = df.style.set_table_styles([\n",
        "        {\n",
        "            'selector': 'th',\n",
        "            'props': [\n",
        "                ('white-space', 'pre-wrap'),  # allow multiline\n",
        "                ('word-wrap', 'break-word')   # break long words\n",
        "            ]\n",
        "        }\n",
        "    ]).format(lambda x: f\"{x:.3f}\" if isinstance(x, float) else x)\n",
        "    return styled\n",
        "\n",
        "# Display the raw metrics and excess metrics with formatted output.\n",
        "display(style_with_wrapping_and_format(df_metrics))\n",
        "display(style_with_wrapping_and_format(df_excess))\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Summary: Count how often each ML strategy \"beats\" Equal Weight.\n",
        "#  - Excess Return is \"better\" if > 0.\n",
        "#  - Excess Volatility is \"better\" if < 0.\n",
        "#  - Excess Sharpe is \"better\" if > 0.\n",
        "#\n",
        "# Average excess metrics are now calculated from all observations.\n",
        "# -----------------------------------------------------------------\n",
        "summary_rows = []\n",
        "total_years = len(df_excess)\n",
        "\n",
        "def w(ml_short, suffix):\n",
        "    return wrap_colname(f\"{ml_short} {suffix}\")\n",
        "\n",
        "for ml in ml_return_cols:\n",
        "    ml_short = ml_name_map[ml]\n",
        "    ret_series    = df_excess[w(ml_short, \"Excess Return\")]\n",
        "    vol_series    = df_excess[w(ml_short, \"Excess Vol\")]\n",
        "    sharpe_series = df_excess[w(ml_short, \"Excess Sharpe\")]\n",
        "\n",
        "    ret_pos_count    = (ret_series > 0).sum()\n",
        "    vol_neg_count    = (vol_series < 0).sum()\n",
        "    sharpe_pos_count = (sharpe_series > 0).sum()\n",
        "\n",
        "    # Average excess metrics now computed over all observations:\n",
        "    avg_ret    = ret_series.mean()\n",
        "    avg_vol    = vol_series.mean()\n",
        "    avg_sharpe = sharpe_series.mean()\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Strategy\": ml_short,\n",
        "        \"Excess Return (Positive) Count\": f\"{ret_pos_count}/{total_years}\",\n",
        "        \"Avg Excess Return\":   avg_ret,\n",
        "        \"Excess Vol (Negative) Count\":    f\"{vol_neg_count}/{total_years}\",\n",
        "        \"Avg Excess Vol\":      avg_vol,\n",
        "        \"Excess Sharpe (Positive) Count\": f\"{sharpe_pos_count}/{total_years}\",\n",
        "        \"Avg Excess Sharpe\":   avg_sharpe\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df.columns = [wrap_colname(col) for col in summary_df.columns]\n",
        "\n",
        "display(style_with_wrapping_and_format(summary_df))\n"
      ],
      "metadata": {
        "id": "j8ieygX5yPei"
      },
      "id": "j8ieygX5yPei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns table"
      ],
      "metadata": {
        "id": "E3Xi_BH1iwiv"
      },
      "id": "E3Xi_BH1iwiv"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PRELIMINARY: Use the merged multi-model table (combined_df)\n",
        "# ---------------------------------------------------------------------\n",
        "start_date = pd.to_datetime('2000-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "df_filtered = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                            (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Rename benchmark column if present (using the first element of BENCHMARK)\n",
        "rename_dict = {}\n",
        "if BENCHMARK[0] in df_filtered.columns:\n",
        "    rename_dict[BENCHMARK[0]] = 'Benchmark Return'\n",
        "df_filtered.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Remove RF from factors (we exclude it)\n",
        "factors_to_use = [fac for fac in FACTORS if fac.upper() != 'RF']\n",
        "\n",
        "# Define possible benchmark columns (for cumulative returns).\n",
        "possible_bench = [\"Benchmark Return\", \"Mkt\", \"Mkt-RF\", \"Us_standard\"]\n",
        "benchmark_cols = [col for col in possible_bench if col in df_filtered.columns]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Calculate Equal-Weighted Returns based on factors_to_use.\n",
        "if all(f in df_filtered.columns for f in factors_to_use):\n",
        "    df_filtered['Equal Factor Weight Strategy Return'] = df_filtered[factors_to_use].mean(axis=1)\n",
        "    equal_ret_col_list = ['Equal Factor Weight Strategy Return']\n",
        "else:\n",
        "    equal_ret_col_list = []\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Predicted Winner Weighted Strategy Return:\n",
        "# Use the base model's predicted winner column using new naming.\n",
        "base_model_key = list(results_dfs.keys())[0]\n",
        "base_model_new = f\"ML1: {base_model_key}\"  # First model is ML1.\n",
        "base_model_pred_col = f\"{base_model_new} Predicted_Winner\"\n",
        "if base_model_pred_col in df_filtered.columns:\n",
        "    df_filtered['Predicted_Winner'] = df_filtered[base_model_pred_col]\n",
        "\n",
        "def calc_winner_strategy(row):\n",
        "    pred = row['Predicted_Winner']\n",
        "    if pred in factors_to_use:\n",
        "        other_factors = [f for f in factors_to_use if f != pred]\n",
        "        if other_factors:\n",
        "            return 0.5 * row[pred] + 0.5 * row[other_factors].mean()\n",
        "        else:\n",
        "            return row[pred]\n",
        "    else:\n",
        "        return row[factors_to_use].mean()\n",
        "\n",
        "df_filtered['Predicted Winner Weighted Strategy Return'] = df_filtered.apply(calc_winner_strategy, axis=1)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Compute cumulative returns for each return series.\n",
        "# We'll work on a copy for cumulative computations.\n",
        "cum = df_filtered.copy()\n",
        "\n",
        "# 1. For each model's allocated return (using new names).\n",
        "allocated_cum_cols = []\n",
        "new_model_names = [f\"ML{i}: {model_key}\" for i, model_key in enumerate(results_dfs.keys(), 1)]\n",
        "for name in new_model_names:\n",
        "    col_alloc = f\"{name} Allocated_Return\"\n",
        "    if col_alloc in cum.columns:\n",
        "        new_cum_col = col_alloc.replace(\"Allocated_Return\", \"Cumulative Allocated Return\")\n",
        "        cum[new_cum_col] = (1 + cum[col_alloc]).cumprod() - 1\n",
        "        allocated_cum_cols.append(new_cum_col)\n",
        "\n",
        "# 2. For equal factor weight returns.\n",
        "if 'Equal Factor Weight Strategy Return' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return'] = (1 + cum['Equal Factor Weight Strategy Return']).cumprod() - 1\n",
        "    equal_cum_cols = ['Equal Factor Weight Cumulative Return']\n",
        "else:\n",
        "    equal_cum_cols = []\n",
        "\n",
        "# 3. For each benchmark column.\n",
        "bench_cum_cols = []\n",
        "# If \"Benchmark Return\" is available, use the actual benchmark name from BENCHMARK[0].\n",
        "if \"Benchmark Return\" in cum.columns:\n",
        "    new_bench_col = f\"{BENCHMARK[0]} Cumulative Return\"\n",
        "    cum[new_bench_col] = (1 + cum[\"Benchmark Return\"]).cumprod() - 1\n",
        "    bench_cum_cols.append(new_bench_col)\n",
        "# Process any other benchmark columns\n",
        "for col in benchmark_cols:\n",
        "    if col != \"Benchmark Return\":\n",
        "        new_col = col + \" Cumulative Return\"\n",
        "        cum[new_col] = (1 + cum[col]).cumprod() - 1\n",
        "        bench_cum_cols.append(new_col)\n",
        "\n",
        "# 4. For Predicted Winner Weighted Strategy Return.\n",
        "if 'Predicted Winner Weighted Strategy Return' in cum.columns:\n",
        "    cum['Predicted Winner Weighted Cumulative Return'] = (1 + cum['Predicted Winner Weighted Strategy Return']).cumprod() - 1\n",
        "\n",
        "# 5. For each factor in factors_to_use: compute cumulative returns.\n",
        "factor_cum_cols = []\n",
        "for fac in factors_to_use:\n",
        "    if fac in cum.columns:\n",
        "        new_name = fac + \" Cumulative\"\n",
        "        cum[new_name] = (1 + cum[fac]).cumprod() - 1\n",
        "        factor_cum_cols.append(new_name)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Build the final cumulative returns table.\n",
        "# Final order:\n",
        "#   a. Common columns: Predicted_month, then benchmark cumulative returns, then factor cumulative returns.\n",
        "#   b. Then Equal Factor Weight Cumulative Return.\n",
        "#   c. Then each model's Cumulative Allocated Return.\n",
        "#   d. Then Predicted Winner Weighted Cumulative Return.\n",
        "# ---------------------------------------------------------------------\n",
        "final_common_order = ['Predicted_month'] + bench_cum_cols + factor_cum_cols\n",
        "final_order = final_common_order + equal_cum_cols + allocated_cum_cols\n",
        "if 'Predicted Winner Weighted Cumulative Return' in cum.columns:\n",
        "    final_order.append('Predicted Winner Weighted Cumulative Return')\n",
        "\n",
        "final_order = [col for col in final_order if col in cum.columns]\n",
        "cumulative_table = cum[final_order].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Display the Final Cumulative Returns Table.\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"Cumulative Returns Table:\")\n",
        "display(cumulative_table)\n",
        "print(\"\\nFirst date in 'Predicted_month':\", pd.to_datetime(cumulative_table['Predicted_month']).min())\n",
        "print(\"Last date in 'Predicted_month':\", pd.to_datetime(cumulative_table['Predicted_month']).max())\n"
      ],
      "metadata": {
        "id": "wm06N1gjGjVr"
      },
      "id": "wm06N1gjGjVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns chart"
      ],
      "metadata": {
        "id": "W-uq2G9iPQwD"
      },
      "id": "W-uq2G9iPQwD"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toggle settings\n",
        "show_50_50_strategy = False  # Toggle for the 50%/50% predicted winner weighted strategy line.\n",
        "show_benchmark = False       # Toggle for showing benchmark cumulative return(s).\n",
        "use_log_scale = False         # Toggle for logarithmic (True) or linear (False) y-axis scale.\n",
        "\n",
        "# Use the cumulative_table built in Cell 2.\n",
        "df_plot = cumulative_table.copy()\n",
        "\n",
        "# Determine the plotting date range.\n",
        "start_date = df_plot['Predicted_month'].min()\n",
        "end_date   = df_plot['Predicted_month'].max()\n",
        "print(f\"Updated plotting range: {start_date} to {end_date}\")\n",
        "print(\"Columns available for plotting:\", df_plot.columns.tolist())\n",
        "\n",
        "if start_date == end_date:\n",
        "    print(\"‚ö† Warning: The dataset might not have updated properly. Try rerunning the previous cell!\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.clf()  # Clear the figure\n",
        "\n",
        "# --- Plot each model's cumulative allocated return ---\n",
        "for col in df_plot.columns:\n",
        "    if \"Cumulative Allocated Return\" in col:\n",
        "        plt.plot(df_plot['Predicted_month'], df_plot[col], label=col)\n",
        "\n",
        "# --- Plot equal factor weight cumulative return ---\n",
        "if 'Equal Factor Weight Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Equal Factor Weight Cumulative Return'],\n",
        "             label='Equal Factor Weight Cumulative Return')\n",
        "\n",
        "# --- Plot benchmark cumulative returns, if toggle is set ---\n",
        "if show_benchmark:\n",
        "    if 'Benchmark Cumulative Return' in df_plot.columns:\n",
        "        plt.plot(df_plot['Predicted_month'], df_plot['Benchmark Cumulative Return'],\n",
        "                 label='Benchmark Cumulative Return')\n",
        "    for col in df_plot.columns:\n",
        "        # Only plot columns with \"Mkt\" or \"Us_standard\" that do NOT include \"Mkt-RF\"\n",
        "        if ((\"Mkt\" in col or \"Us_standard\" in col) and\n",
        "            \"Cumulative Return\" in col and \"Mkt-RF\" not in col and col != \"Benchmark Cumulative Return\"):\n",
        "            plt.plot(df_plot['Predicted_month'], df_plot[col], label=col)\n",
        "\n",
        "# --- Plot Predicted Winner Weighted Cumulative Return if available and toggle is on ---\n",
        "if show_50_50_strategy and 'Predicted Winner Weighted Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Predicted Winner Weighted Cumulative Return'],\n",
        "             label='50%/50% Predicted Winner Strategy')\n",
        "\n",
        "# --- Format and display the chart ---\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.title('Cumulative Returns Comparison')\n",
        "plt.xlim(start_date, end_date)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Toggle the y-axis scale\n",
        "if use_log_scale:\n",
        "    plt.yscale('log')\n",
        "else:\n",
        "    plt.yscale('linear')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wqfmAhDk3KBx"
      },
      "id": "wqfmAhDk3KBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PERFORMANCE METRICS"
      ],
      "metadata": {
        "id": "-YjFAG1ePN-9"
      },
      "id": "-YjFAG1ePN-9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
        "\n",
        "def annualized_metrics(monthly_returns):\n",
        "    \"\"\"\n",
        "    Compute the annualized return, volatility, and Sharpe ratio from monthly returns.\n",
        "    Assumes monthly returns.\n",
        "    \"\"\"\n",
        "    # Fill missing values with 0 to avoid issues.\n",
        "    monthly_returns = monthly_returns.fillna(0)\n",
        "    mean_m = monthly_returns.mean()\n",
        "    std_m = monthly_returns.std()\n",
        "    ann_ret = mean_m * 12\n",
        "    ann_vol = std_m * np.sqrt(12)\n",
        "    sharpe = ann_ret / ann_vol if ann_vol != 0 else np.nan\n",
        "    return ann_ret, ann_vol, sharpe\n",
        "\n",
        "def max_drawdown(monthly_returns):\n",
        "    \"\"\"\n",
        "    Compute the maximum drawdown from a series of monthly returns.\n",
        "    \"\"\"\n",
        "    wealth = (1 + monthly_returns.fillna(0)).cumprod()\n",
        "    dd_series = wealth / wealth.cummax() - 1\n",
        "    return dd_series.min()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Determine the raw return columns from combined_df.\n",
        "# We assume:\n",
        "# ‚Ä¢ \"Predicted_month\" is the date column.\n",
        "# ‚Ä¢ Raw (monthly) return strategy columns either contain \"Return\"\n",
        "#   (e.g. \"ML1: Random Forest Allocated_Return\")\n",
        "#   OR exactly match BENCHMARK[0] (e.g. \"Mkt\") or \"Benchmark Return\"\n",
        "# We exclude any column that contains \"Cumulative\" as well as non-return columns.\n",
        "# ---------------------------------------------------------------------\n",
        "all_columns = combined_df.columns.tolist()\n",
        "# Include columns that (a) have \"Return\" in them (but not \"Cumulative\"), OR\n",
        "# are exactly equal to BENCHMARK[0] or \"Benchmark Return\"\n",
        "strategy_cols = [\n",
        "    col for col in all_columns\n",
        "    if (\n",
        "         ((\"Return\" in col) or (col == BENCHMARK[0]) or (col == \"Benchmark Return\"))\n",
        "         and (\"Cumulative\" not in col)\n",
        "         and (col not in [\"Actual_Winner\", \"Predicted_month\"])\n",
        "       )\n",
        "]\n",
        "\n",
        "# Display the list of strategy columns for debugging.\n",
        "print(\"Strategy columns used for performance metrics:\")\n",
        "print(strategy_cols)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loop through each selected strategy column and compute performance metrics.\n",
        "# ---------------------------------------------------------------------\n",
        "metrics_list = []\n",
        "for col in strategy_cols:\n",
        "    monthly_ret_series = combined_df[col]\n",
        "    ann_ret, ann_vol, sharpe = annualized_metrics(monthly_ret_series)\n",
        "    mdd = max_drawdown(monthly_ret_series)\n",
        "\n",
        "    metrics_list.append({\n",
        "         \"Strategy\": col,\n",
        "         \"Annualized Return\": f\"{ann_ret*100:.2f}%\",\n",
        "         \"Annualized Volatility\": f\"{ann_vol*100:.2f}%\",\n",
        "         \"Sharpe Ratio\": f\"{sharpe:.2f}\",\n",
        "         \"Max Drawdown\": f\"{mdd*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the performance metrics.\n",
        "metrics_df = pd.DataFrame(metrics_list).sort_values(\"Strategy\").reset_index(drop=True)\n",
        "\n",
        "# Optionally, sort by another metric (e.g., Sharpe Ratio) if desired.\n",
        "# metrics_df = metrics_df.sort_values(\"Sharpe Ratio\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the performance metrics as an HTML table.\n",
        "display(HTML(metrics_df.to_html(index=False)))\n"
      ],
      "metadata": {
        "id": "1o0tq56K64bL"
      },
      "id": "1o0tq56K64bL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drawdown chart"
      ],
      "metadata": {
        "id": "YXD3cJKFO_sE"
      },
      "id": "YXD3cJKFO_sE"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. TOGGLE OPTIONS\n",
        "# -------------------------------------------------------------------------\n",
        "show_benchmark_drawdown = True           # Toggle benchmark drawdown\n",
        "show_equal_weight_drawdown = True        # Toggle Equal Weight (single) drawdown\n",
        "show_winner_weighted_drawdown = False     # Toggle Winner Weighted drawdown\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. COPY cumulative_table (assumed computed previously)\n",
        "# -------------------------------------------------------------------------\n",
        "drawdown_df = cumulative_table.copy()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. CALCULATE DRAWDOWNS USING WEALTH INDEX (Wealth = 1 + Cumulative Return)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# a) For each ML model's cumulative allocated return column:\n",
        "ml_alloc_cols = [col for col in drawdown_df.columns if \"Cumulative Allocated Return\" in col]\n",
        "for col in ml_alloc_cols:\n",
        "    wealth = 1 + drawdown_df[col]\n",
        "    drawdown_name = col.replace(\"Cumulative Allocated Return\", \"Drawdown\")\n",
        "    drawdown_df[drawdown_name] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# b) For Benchmark:\n",
        "# Use the benchmark name from BENCHMARK[0]\n",
        "benchmark_name = BENCHMARK[0]  # for example, \"Mkt\"\n",
        "benchmark_cum_col = f\"{benchmark_name} Cumulative Return\"\n",
        "if benchmark_cum_col in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[benchmark_cum_col]\n",
        "    drawdown_df[f\"{benchmark_name} Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "elif \"Mkt Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Mkt Cumulative Return\"]\n",
        "    drawdown_df[\"Mkt Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "else:\n",
        "    print(\"WARNING: No benchmark cumulative return column found.\")\n",
        "\n",
        "# c) For Equal Weight (single version):\n",
        "if \"Equal Factor Weight Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Equal Factor Weight Cumulative Return\"]\n",
        "    drawdown_df[\"Equal Weight Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# d) For Predicted Winner Weighted:\n",
        "if \"Predicted Winner Weighted Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Predicted Winner Weighted Cumulative Return\"]\n",
        "    drawdown_df[\"Winner Weighted Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4. FILTER BY DATE RANGE\n",
        "# -------------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "drawdown_df[\"Predicted_month\"] = pd.to_datetime(drawdown_df[\"Predicted_month\"])\n",
        "plot_df = drawdown_df[(drawdown_df[\"Predicted_month\"] >= start_date) &\n",
        "                      (drawdown_df[\"Predicted_month\"] <= end_date)]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5. PLOT THE DRAWDOWNS\n",
        "# -------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.clf()  # Clear any existing figure\n",
        "\n",
        "# a) Plot each ML model's drawdown (those columns that start with \"ML\" and contain \"Drawdown\")\n",
        "for col in plot_df.columns:\n",
        "    if col.startswith(\"ML\") and \"Drawdown\" in col:\n",
        "        plt.plot(plot_df[\"Predicted_month\"], plot_df[col], label=col)\n",
        "\n",
        "# b) Plot Benchmark Drawdown if toggled on\n",
        "if show_benchmark_drawdown:\n",
        "    # Look for the benchmark drawdown column with the actual benchmark name (e.g., \"Mkt Drawdown\")\n",
        "    bench_drawdown_col = f\"{benchmark_name} Drawdown\"\n",
        "    if bench_drawdown_col in plot_df.columns:\n",
        "        plt.plot(plot_df[\"Predicted_month\"], plot_df[bench_drawdown_col], label=bench_drawdown_col)\n",
        "\n",
        "# c) Plot Equal Weight Drawdown if available\n",
        "if show_equal_weight_drawdown and \"Equal Weight Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Equal Weight Drawdown\"], label=\"Equal Weight Drawdown\")\n",
        "\n",
        "# d) Plot Winner Weighted Drawdown if toggled on\n",
        "if show_winner_weighted_drawdown and \"Winner Weighted Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Winner Weighted Drawdown\"], label=\"Winner Weighted Drawdown\")\n",
        "\n",
        "# Format the y-axis as percentages.\n",
        "plt.gca().yaxis.set_major_formatter(\n",
        "    mticker.FuncFormatter(lambda val, _: f\"{val*100:.0f}%\")\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6. ENSURE THE X-AXIS SPANS EXACTLY THE DEFINED DATE RANGE\n",
        "# -------------------------------------------------------------------------\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Drawdown\")\n",
        "plt.title(\"Drawdowns of Strategies\")\n",
        "plt.xlim(start_date, end_date)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FK-XFzpv4SXo"
      },
      "id": "FK-XFzpv4SXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression"
      ],
      "metadata": {
        "id": "S244JpMyoSVe"
      },
      "id": "S244JpMyoSVe"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "#CHATIN MUKAAN T√Ñ√Ñ ON TRUE OLI LONG SHORT TAI LONG ONLY ILMEISESTI\n",
        "subtract_rf = True\n",
        "\n",
        "# Define the regression date range\n",
        "reg_start_date = pd.to_datetime('1973-08-01')\n",
        "reg_end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. LOAD FAMA-FRENCH DATA\n",
        "# -----------------------------------------------------------------------------\n",
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "df_factors = xls_file.parse(\"FF5\", dtype=str)  # read as string to easily clean decimals\n",
        "df_factors[\"Date\"] = pd.to_datetime(df_factors[\"Date\"])\n",
        "\n",
        "# Define which columns to use for the regression\n",
        "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "\n",
        "# We also need 'RF' for subtracting from portfolio returns\n",
        "all_ff5_cols = factors + ['RF']  # Mkt-RF, SMB, HML, RMW, CMA, and the risk-free rate (RF)\n",
        "\n",
        "# Convert all factor columns from string to float (assuming the Excel data is in %)\n",
        "for col in all_ff5_cols:\n",
        "    df_factors[col] = (\n",
        "        df_factors[col]\n",
        "        .str.replace(\",\", \".\", regex=False)\n",
        "        .astype(float)\n",
        "        .div(100)  # Convert percent to decimal if your Excel data is e.g. \"0.2\" as 0.2%\n",
        "    )\n",
        "\n",
        "# Sort by date\n",
        "df_factors = df_factors.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. PREPARE MODEL NAMES / STORAGE\n",
        "# -----------------------------------------------------------------------------\n",
        "# This assumes you already have some dictionary 'results_dfs' that holds your models.\n",
        "# We'll construct multi-model labels from it. Adjust as needed.\n",
        "new_model_names = [f\"ML{i}: {model_key}\" for i, model_key in enumerate(results_dfs.keys(), 1)]\n",
        "\n",
        "# We'll store final regression summary metrics in a list of dicts\n",
        "regression_summary_list = []\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. REGRESSION LOOP FOR EACH MODEL\n",
        "# -----------------------------------------------------------------------------\n",
        "for new_model_name in new_model_names:\n",
        "    # Build the column name for the model's allocated return\n",
        "    model_alloc_col = f\"{new_model_name} Allocated_Return\"\n",
        "    if model_alloc_col not in combined_df.columns:\n",
        "        print(f\"Column '{model_alloc_col}' not found for {new_model_name}. Skipping regression.\")\n",
        "        continue\n",
        "\n",
        "    # 3a. Filter your combined_df to the date range, rename the return column\n",
        "    df_model_reg = combined_df.loc[\n",
        "        (pd.to_datetime(combined_df['Predicted_month']) >= reg_start_date) &\n",
        "        (pd.to_datetime(combined_df['Predicted_month']) <= reg_end_date),\n",
        "        ['Predicted_month', model_alloc_col]\n",
        "    ].copy().sort_values('Predicted_month')\n",
        "    df_model_reg = df_model_reg.rename(columns={model_alloc_col: \"Allocated_Return\"})\n",
        "\n",
        "    # 3b. Merge in the Fama-French factors + RF\n",
        "    df_model_reg['Predicted_month'] = pd.to_datetime(df_model_reg['Predicted_month'])\n",
        "    merged_reg = pd.merge(\n",
        "        df_model_reg,\n",
        "        df_factors[['Date'] + all_ff5_cols],\n",
        "        left_on=\"Predicted_month\",\n",
        "        right_on=\"Date\",\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    merged_reg.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "    # 3c. Subtract RF from portfolio returns if needed\n",
        "    if subtract_rf and 'RF' in merged_reg.columns:\n",
        "        merged_reg['Adj_Allocated_Return'] = merged_reg['Allocated_Return'] - merged_reg['RF']\n",
        "    else:\n",
        "        merged_reg['Adj_Allocated_Return'] = merged_reg['Allocated_Return']\n",
        "\n",
        "    # 3d. Build regression (Y = Adj_Allocated_Return, X = Mkt-RF, SMB, HML, RMW, CMA)\n",
        "    X = merged_reg[factors]  # exclude 'RF' from the regressors\n",
        "    X = sm.add_constant(X)\n",
        "    y = merged_reg['Adj_Allocated_Return']\n",
        "\n",
        "    model_reg = sm.OLS(y, X, missing='drop').fit()\n",
        "\n",
        "    # 3e. Print summary\n",
        "    print(\"\\n=== Regression Results for\", new_model_name, \"vs. FF5 Factors ===\")\n",
        "    print(model_reg.summary())\n",
        "    print(f\"\\nAlpha (Monthly, {new_model_name}): {model_reg.params.get('const', np.nan):.4f}\")\n",
        "    if 'Mkt-RF' in model_reg.params:\n",
        "        print(f\"Market Beta ({new_model_name}): {model_reg.params['Mkt-RF']:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # 3f. Collect summary stats\n",
        "    regression_summary_list.append({\n",
        "        \"Model\": new_model_name,\n",
        "        \"Alpha\": model_reg.params.get('const', np.nan),\n",
        "        \"Market Beta\": model_reg.params.get('Mkt-RF', np.nan),\n",
        "        \"R-squared\": model_reg.rsquared,\n",
        "        \"Adj. R-squared\": model_reg.rsquared_adj,\n",
        "        \"p-value\": model_reg.f_pvalue\n",
        "    })\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. REGRESSION FOR EQUAL-WEIGHT STRATEGY\n",
        "# -----------------------------------------------------------------------------\n",
        "equal_df = combined_df.loc[\n",
        "    (pd.to_datetime(combined_df['Predicted_month']) >= reg_start_date) &\n",
        "    (pd.to_datetime(combined_df['Predicted_month']) <= reg_end_date),\n",
        "    ['Predicted_month', 'Equal_Weight_Return']\n",
        "].copy().sort_values('Predicted_month')\n",
        "equal_df = equal_df.rename(columns={'Equal_Weight_Return': 'Equal_Weight_Strategy_Return'})\n",
        "\n",
        "equal_df['Predicted_month'] = pd.to_datetime(equal_df['Predicted_month'])\n",
        "equal_merged = pd.merge(\n",
        "    equal_df,\n",
        "    df_factors[['Date'] + all_ff5_cols],  # also include RF for subtracting\n",
        "    left_on=\"Predicted_month\",\n",
        "    right_on=\"Date\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "equal_merged.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "if subtract_rf and 'RF' in equal_merged.columns:\n",
        "    print(\"Equal-Weight Strategy: Adjusting returns by subtracting RF.\")\n",
        "    equal_merged['Adj_Equal_Weight_Return'] = (\n",
        "        equal_merged['Equal_Weight_Strategy_Return'] - equal_merged['RF']\n",
        "    )\n",
        "else:\n",
        "    equal_merged['Adj_Equal_Weight_Return'] = equal_merged['Equal_Weight_Strategy_Return']\n",
        "\n",
        "X_eq = equal_merged[factors]  # Mkt-RF, SMB, HML, RMW, CMA\n",
        "X_eq = sm.add_constant(X_eq)\n",
        "y_eq = equal_merged['Adj_Equal_Weight_Return']\n",
        "\n",
        "model_eq = sm.OLS(y_eq, X_eq, missing='drop').fit()\n",
        "\n",
        "print(\"\\n=== Regression Results for Equal-Weight Strategy vs. FF5 Factors ===\")\n",
        "print(model_eq.summary())\n",
        "print(f\"\\nAlpha (Monthly, Equal-Weight Strategy): {model_eq.params.get('const', np.nan):.4f}\")\n",
        "print(f\"Market Beta (Equal-Weight Strategy):   {model_eq.params.get('Mkt-RF', np.nan):.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. BUILD A SUMMARY TABLE\n",
        "# -----------------------------------------------------------------------------\n",
        "regression_summary_df = pd.DataFrame(regression_summary_list)\n",
        "\n",
        "equal_summary = pd.DataFrame([{\n",
        "    \"Model\": \"Equal-Weight Strategy\",\n",
        "    \"Alpha\": model_eq.params.get(\"const\", np.nan),\n",
        "    \"Market Beta\": model_eq.params.get(\"Mkt-RF\", np.nan),\n",
        "    \"R-squared\": model_eq.rsquared,\n",
        "    \"Adj. R-squared\": model_eq.rsquared_adj,\n",
        "    \"p-value\": model_eq.f_pvalue\n",
        "}])\n",
        "\n",
        "regression_summary_df = pd.concat([regression_summary_df, equal_summary], ignore_index=True)\n",
        "regression_summary_df = regression_summary_df.round(3)\n",
        "\n",
        "print(\"\\nSummary of Regression Results:\")\n",
        "display(regression_summary_df)\n"
      ],
      "metadata": {
        "id": "MOb-rnOzH_4W"
      },
      "id": "MOb-rnOzH_4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
        "\n",
        "def annual_sharpe(series):\n",
        "    \"\"\"\n",
        "    Calculate the annual Sharpe ratio from a monthly return Series.\n",
        "    - Annual return = (product(1 + monthly returns)) - 1\n",
        "    - Annual volatility = std(monthly returns) * sqrt(12)\n",
        "    - Sharpe = annual return / annual volatility (assuming risk-free rate = 0)\n",
        "    \"\"\"\n",
        "    annual_return = (1 + series).prod() - 1\n",
        "    annual_vol = series.std() * np.sqrt(12)\n",
        "    if annual_vol == 0:\n",
        "        return np.nan\n",
        "    return annual_return / annual_vol\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Define the date range and filter combined_df accordingly.\n",
        "# ---------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "# Filter the merged DataFrame for the desired date range.\n",
        "df_filtered = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                            (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Create a \"Year\" column from Predicted_month.\n",
        "df_filtered[\"Year\"] = df_filtered[\"Predicted_month\"].dt.year\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Identify Strategy Columns for Regression Metrics:\n",
        "#    - ML strategy columns: Expected to have names like \"ML1: Random Forest Allocated_Return\", etc.\n",
        "#    - Benchmark: \"Benchmark Return\"\n",
        "#    - Equal Weight: \"Equal_Weight_Return\"\n",
        "# ---------------------------------------------------------------------\n",
        "ml_strategy_cols = [col for col in df_filtered.columns\n",
        "                    if col.startswith(\"ML\") and \"Allocated_Return\" in col and \"Cumulative\" not in col]\n",
        "\n",
        "bench_col = \"Benchmark Return\" if \"Benchmark Return\" in df_filtered.columns else None\n",
        "eq_col    = \"Equal_Weight_Return\" if \"Equal_Weight_Return\" in df_filtered.columns else None\n",
        "\n",
        "print(\"Strategy columns used for annual Sharpe calculation:\")\n",
        "print(\"ML Strategy Columns:\", ml_strategy_cols)\n",
        "if bench_col:\n",
        "    print(\"Benchmark Column:\", bench_col)\n",
        "if eq_col:\n",
        "    print(\"Equal Weight Column:\", eq_col)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Compute Annual Sharpe Ratios for Each Strategy\n",
        "# ---------------------------------------------------------------------\n",
        "annual_sharpe_dict = {}\n",
        "\n",
        "# Compute for each ML model column.\n",
        "for col in ml_strategy_cols:\n",
        "    annual_sharpe_dict[col] = df_filtered.groupby(\"Year\")[col].apply(annual_sharpe)\n",
        "\n",
        "# Compute for benchmark (if available).\n",
        "if bench_col is not None:\n",
        "    annual_sharpe_dict[bench_col] = df_filtered.groupby(\"Year\")[bench_col].apply(annual_sharpe)\n",
        "\n",
        "# Compute for equal weight (if available).\n",
        "if eq_col is not None:\n",
        "    annual_sharpe_dict[eq_col] = df_filtered.groupby(\"Year\")[eq_col].apply(annual_sharpe)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Combine the Results into One DataFrame and Round to 3 Decimals\n",
        "# ---------------------------------------------------------------------\n",
        "annual_sharpe_table = pd.DataFrame(annual_sharpe_dict)\n",
        "annual_sharpe_table = annual_sharpe_table.round(3)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Display the Annual Sharpe Ratios as a Neatly Formatted HTML Table\n",
        "# ---------------------------------------------------------------------\n",
        "display(HTML(\"<h3>Annual Sharpe Ratios by Year and Strategy</h3>\" + annual_sharpe_table.to_html(index=True)))\n"
      ],
      "metadata": {
        "id": "E0cAJzowyqZn"
      },
      "id": "E0cAJzowyqZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ============================================================================\n",
        "# 1) Prepare Annual Return Data\n",
        "# ============================================================================\n",
        "\n",
        "# Define the date range for annual analysis.\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "# Filter the merged DataFrame (combined_df) for the desired date range.\n",
        "# (combined_df comes from your earlier multi‚Äëmodel merging steps.)\n",
        "df_annual = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                          (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Create a \"Year\" column.\n",
        "df_annual['Year'] = df_annual['Predicted_month'].dt.year\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Identify columns:\n",
        "#   ‚Ä¢ ML strategy columns: they contain \"Allocated_Return\" (e.g., \"ML1: Random Forest Allocated_Return\")\n",
        "#   ‚Ä¢ Benchmark: assume the column is \"Benchmark Return\" (or use what was defined earlier).\n",
        "#   ‚Ä¢ Equal Weight: assume \"Equal_Weight_Return\"\n",
        "#   ‚Ä¢ Factor columns: using the global FACTORS (e.g., ['SMB', 'HML', 'CMA', 'RMW'])\n",
        "# ---------------------------------------------------------------------\n",
        "ml_cols = [col for col in df_annual.columns if (\"Allocated_Return\" in col) and (\"Cumulative\" not in col)]\n",
        "benchmark_col = \"Benchmark Return\" if \"Benchmark Return\" in df_annual.columns else None\n",
        "equal_weight_col = \"Equal_Weight_Return\" if \"Equal_Weight_Return\" in df_annual.columns else None\n",
        "factor_cols = [fac for fac in FACTORS if fac in df_annual.columns]\n",
        "\n",
        "print(\"ML Strategy Columns:\", ml_cols)\n",
        "print(\"Benchmark Column:\", benchmark_col)\n",
        "print(\"Equal Weight Column:\", equal_weight_col)\n",
        "print(\"Factor Columns:\", factor_cols)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Function to compute annual return as the compounded return over the year.\n",
        "# ---------------------------------------------------------------------\n",
        "def annual_return(group, col):\n",
        "    \"\"\"Compound return over the group: product(1 + r) - 1.\"\"\"\n",
        "    return (1 + group[col]).prod() - 1\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Compute annual returns for each strategy.\n",
        "# We'll build a dictionary where keys are strategy names and values are Series indexed by Year.\n",
        "annual_returns = {}\n",
        "\n",
        "# For each ML model column.\n",
        "for col in ml_cols:\n",
        "    annual_returns[col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, col))\n",
        "\n",
        "# For benchmark.\n",
        "if benchmark_col is not None:\n",
        "    annual_returns[benchmark_col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, benchmark_col))\n",
        "\n",
        "# For equal weight strategy.\n",
        "if equal_weight_col is not None:\n",
        "    annual_returns[equal_weight_col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, equal_weight_col))\n",
        "\n",
        "# For each factor.\n",
        "for fac in factor_cols:\n",
        "    annual_returns[fac] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, fac))\n",
        "\n",
        "# Combine the computed annual returns into one DataFrame.\n",
        "annual_returns_df = pd.DataFrame(annual_returns)\n",
        "annual_returns_df = annual_returns_df.round(3)\n",
        "\n",
        "# Display the Annual Returns Table.\n",
        "display(HTML(\"<h3>Annual Returns Table</h3>\" + annual_returns_df.to_html()))\n",
        "\n",
        "# ============================================================================\n",
        "# 2) Compute Excess Returns and Plot by Factor\n",
        "# ============================================================================\n",
        "\n",
        "# For each factor, compute excess return for each ML model relative to that factor.\n",
        "# We define excess return as: (ML Annual Return) - (Factor Annual Return)\n",
        "excess_returns = {}\n",
        "for fac in factor_cols:\n",
        "    # Create a DataFrame of excess returns for all ML models for factor 'fac'\n",
        "    excess_df = pd.DataFrame()\n",
        "    for ml in ml_cols:\n",
        "        excess_df[ml] = annual_returns_df[ml] - annual_returns_df[fac]\n",
        "    excess_returns[fac] = excess_df\n",
        "\n",
        "# Now plot excess returns by factor.\n",
        "n_factors = len(factor_cols)\n",
        "if n_factors > 0:\n",
        "    # Create one subplot per factor.\n",
        "    fig, axes = plt.subplots(1, n_factors, figsize=(6 * n_factors, 5), sharex=True)\n",
        "    if n_factors == 1:\n",
        "        axes = [axes]\n",
        "    for i, fac in enumerate(factor_cols):\n",
        "        ax = axes[i]\n",
        "        # Plot a line for each ML model excess return for this factor.\n",
        "        for ml in ml_cols:\n",
        "            ax.plot(excess_returns[fac].index, excess_returns[fac][ml],\n",
        "                    marker='o', label=f'{ml} - {fac}')\n",
        "        ax.set_title(f'Excess Return: ML - {fac}')\n",
        "        ax.set_xlabel(\"Year\")\n",
        "        ax.set_ylabel(\"Excess Return\")\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax.legend(fontsize='small')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 3) (Optional) Compute and Plot Annual Returns for Benchmark and Equal Weight\n",
        "# ============================================================================\n",
        "\n",
        "# For convenience, let's display the annual returns for benchmark and equal weight strategies.\n",
        "if benchmark_col is not None:\n",
        "    print(\"\\nAnnual Returns - Benchmark:\")\n",
        "    display(annual_returns_df[[benchmark_col]])\n",
        "if equal_weight_col is not None:\n",
        "    print(\"\\nAnnual Returns - Equal Weight Strategy:\")\n",
        "    display(annual_returns_df[[equal_weight_col]])\n"
      ],
      "metadata": {
        "id": "Lo1wj3nU3ceD"
      },
      "id": "Lo1wj3nU3ceD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Return comparison by period"
      ],
      "metadata": {
        "id": "tC8mxG1Ynm-j"
      },
      "id": "tC8mxG1Ynm-j"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1) Create a \"Year\" Column in the Merged DataFrame\n",
        "# -------------------------------------------------\n",
        "combined_df['Year'] = pd.to_datetime(combined_df['Predicted_month']).dt.year\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2) Define a Function to Calculate Annual Return\n",
        "# -------------------------------------------------\n",
        "def annual_return(series):\n",
        "    \"\"\"\n",
        "    Compute the annual compounded return from a monthly return series.\n",
        "    \"\"\"\n",
        "    return (1 + series).prod() - 1\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3) Compute Annual Returns for Each Strategy:\n",
        "#    a) For each ML model's allocated return.\n",
        "#    b) For Benchmark Return.\n",
        "#    c) For Equal Weight Return.\n",
        "# -------------------------------------------------\n",
        "# a) For ML models: we assume these columns start with \"ML\" and contain \"Allocated_Return\"\n",
        "ml_cols = [col for col in combined_df.columns\n",
        "           if col.startswith(\"ML\") and \"Allocated_Return\" in col and \"Cumulative\" not in col]\n",
        "annual_returns_ml = {col: combined_df.groupby(\"Year\")[col].apply(annual_return) for col in ml_cols}\n",
        "\n",
        "# b) For Benchmark:\n",
        "annual_return_bench = None\n",
        "if \"Benchmark Return\" in combined_df.columns:\n",
        "    annual_return_bench = combined_df.groupby(\"Year\")[\"Benchmark Return\"].apply(annual_return)\n",
        "\n",
        "# c) For Equal Weight:\n",
        "annual_return_eq = None\n",
        "if \"Equal_Weight_Return\" in combined_df.columns:\n",
        "    annual_return_eq = combined_df.groupby(\"Year\")[\"Equal_Weight_Return\"].apply(annual_return)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4) Compute Annual Returns for Each Factor in FACTORS:\n",
        "# -------------------------------------------------\n",
        "annual_returns_factors = {}\n",
        "for factor in FACTORS:\n",
        "    if factor in combined_df.columns:\n",
        "        annual_returns_factors[factor] = combined_df.groupby(\"Year\")[factor].apply(annual_return)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5) Compute Excess Returns for each ML model relative to each factor.\n",
        "#    Excess = (ML Model Annual Return) - (Factor Annual Return)\n",
        "# -------------------------------------------------\n",
        "excess_returns = {}\n",
        "for ml_col, ml_series in annual_returns_ml.items():\n",
        "    df_excess = pd.DataFrame(index=ml_series.index)\n",
        "    for factor, factor_series in annual_returns_factors.items():\n",
        "        df_excess[f\"Excess ({ml_col} - {factor})\"] = ml_series - factor_series\n",
        "    excess_returns[ml_col] = df_excess\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6) Build a Summary Table of Annual Returns for all Strategies\n",
        "# -------------------------------------------------\n",
        "years = sorted(combined_df['Year'].unique())\n",
        "annual_summary = pd.DataFrame(index=years)\n",
        "\n",
        "# Add ML model returns.\n",
        "for ml_col, series in annual_returns_ml.items():\n",
        "    annual_summary[ml_col] = series\n",
        "\n",
        "# Add benchmark return if available.\n",
        "if annual_return_bench is not None:\n",
        "    annual_summary[\"Benchmark Return\"] = annual_return_bench\n",
        "\n",
        "# Add equal weight return if available.\n",
        "if annual_return_eq is not None:\n",
        "    annual_summary[\"Equal_Weight_Return\"] = annual_return_eq\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7) Display the Summary Table and (optionally) Excess Returns for the First ML Model\n",
        "# -------------------------------------------------\n",
        "print(\"Annual Returns Summary:\")\n",
        "display(annual_summary.round(3))\n",
        "\n",
        "# Optionally, display excess returns for the first ML model.\n",
        "first_ml_col = ml_cols[0] if ml_cols else None\n",
        "if first_ml_col is not None:\n",
        "    print(f\"\\nExcess Returns for {first_ml_col}:\")\n",
        "    display(excess_returns[first_ml_col].round(3))\n"
      ],
      "metadata": {
        "id": "_KfEgK83EKLT"
      },
      "id": "_KfEgK83EKLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corr Heat map & regiimi sharpet\n"
      ],
      "metadata": {
        "id": "WnQxAZGgWuKt"
      },
      "id": "WnQxAZGgWuKt"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = xls_file.parse(SHEET_NAME)\n",
        "df = df[[\"Date\"] + FACTORS]\n",
        "\n",
        "# Calculate correlations\n",
        "correlation_matrix = df[FACTORS].corr()\n",
        "\n",
        "# Show regular correlation table\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap of Factors\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "trQUVO-yLcAS"
      },
      "id": "trQUVO-yLcAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "VXaJ_itz6opp"
      },
      "id": "VXaJ_itz6opp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1) Clone or pull your repo ---\n",
        "repo_url  = \"https://github.com/Elkkujou/Gradu.git\"\n",
        "repo_name = \"Gradu\"\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    subprocess.run([\"git\", \"-C\", repo_name, \"pull\"], check=True)\n",
        "else:\n",
        "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "# --- 2) Load the Excel sheet into data_ff5 ---\n",
        "xlsx_path = os.path.join(repo_name, \"THE_2ND_latest.xlsx\")\n",
        "xls_file  = pd.ExcelFile(xlsx_path)\n",
        "\n",
        "SHEET_NAME  = \"ajodata_FF5\"\n",
        "data_ff5    = xls_file.parse(SHEET_NAME)\n",
        "\n",
        "# --- 3) Parse the date column (assumed to be the first column) ---\n",
        "date_col = data_ff5.columns[0]\n",
        "data_ff5[date_col] = pd.to_datetime(data_ff5[date_col])\n",
        "\n",
        "# --- 4) Compute z‚Äëscores for your four features ---\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "z_cols   = [f + '_z' for f in FEATURES]\n",
        "\n",
        "# Cross‚Äësample z‚Äëscore: (x ‚Äì mean) / std\n",
        "data_ff5[z_cols] = data_ff5[FEATURES].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# --- 5) Quick check ---\n",
        "print(data_ff5.loc[:, FEATURES + z_cols].head(10))"
      ],
      "metadata": {
        "id": "Nbkz9QHj6yph"
      },
      "id": "Nbkz9QHj6yph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Make sure your Date column is datetime and set as index\n",
        "data_ff5['Date'] = pd.to_datetime(data_ff5.iloc[:, 0])\n",
        "data_ff5.set_index('Date', inplace=True)\n",
        "\n",
        "# 2) Define features and their z‚Äëscore column names\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "z_cols   = [f + '_z' for f in FEATURES]\n",
        "\n",
        "# 3) (Re‚Äë)compute z‚Äëscores if you haven‚Äôt yet\n",
        "data_ff5[z_cols] = data_ff5[FEATURES].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# 4) Plot each in its own figure\n",
        "for feat, zc in zip(FEATURES, z_cols):\n",
        "    median_val = data_ff5[zc].median()\n",
        "\n",
        "    plt.figure()                          # new figure for each chart\n",
        "    plt.plot(data_ff5.index, data_ff5[zc], label=f'{feat} Z‚ÄëScore')\n",
        "    plt.axhline(0,        linewidth=1,   label='Zero')\n",
        "    plt.axhline(median_val, linestyle='--', linewidth=1, label=f'Median = {median_val:.2f}')\n",
        "\n",
        "    plt.title(f'{feat} Z‚ÄëScore Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Z‚ÄëScore')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BqHRQ1Fk7ujB"
      },
      "id": "BqHRQ1Fk7ujB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Define your features and factors\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "ZCOLS    = [f + '_z' for f in FEATURES]\n",
        "FACTORS  = ['SMB', 'HML', 'CMA', 'RMW']\n",
        "\n",
        "# 2) Prepare an empty DataFrame to hold Sharpe ratios\n",
        "cols = []\n",
        "for feat in FEATURES:\n",
        "    cols += [f\"{feat} > 0 SR\", f\"{feat} < 0 SR\"]\n",
        "sharpe_df = pd.DataFrame(index=FACTORS, columns=cols, dtype=float)\n",
        "\n",
        "# 3) Compute annualized Sharpe = ‚àö12 * mean(return) / std(return)\n",
        "for feat, zcol in zip(FEATURES, ZCOLS):\n",
        "    for fac in FACTORS:\n",
        "        mask_pos = data_ff5[zcol] > 0\n",
        "        mask_neg = ~mask_pos\n",
        "\n",
        "        r_pos = data_ff5.loc[mask_pos, fac]\n",
        "        r_neg = data_ff5.loc[mask_neg, fac]\n",
        "\n",
        "        # avoid division by zero\n",
        "        sr_pos = np.sqrt(12) * r_pos.mean() / r_pos.std() if r_pos.std() != 0 else np.nan\n",
        "        sr_neg = np.sqrt(12) * r_neg.mean() / r_neg.std() if r_neg.std() != 0 else np.nan\n",
        "\n",
        "        sharpe_df.loc[fac, f\"{feat} > 0 SR\"] = sr_pos\n",
        "        sharpe_df.loc[fac, f\"{feat} < 0 SR\"] = sr_neg\n",
        "\n",
        "# 4) Round and display\n",
        "sharpe_df = sharpe_df.round(3)\n",
        "print(sharpe_df)"
      ],
      "metadata": {
        "id": "2HuD8lXC8Z2X"
      },
      "id": "2HuD8lXC8Z2X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Regime-wise Correlation Heatmaps\n",
        "#\n",
        "# For the selected return columns, compute and plot the correlation matrix\n",
        "# for each market regime as a heatmap.\n",
        "\n",
        "# %%\n",
        "# Use the global FACTORS instead of redefining returns_columns\n",
        "unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "for regime in unique_regimes:\n",
        "    regime_data = df[df[REGIMES_COLUMN] == regime][FACTORS]\n",
        "    corr = regime_data.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "    plt.title(f\"Return Correlation Heatmap - {regime}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4GbzDKk2FZYH"
      },
      "id": "4GbzDKk2FZYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Sharpe Ratios by Market Regime\n",
        "#\n",
        "# Compute and visualize Sharpe ratios for selected factors across each regime,\n",
        "# as well as the unconditional (all-data) values, using a bar chart.\n",
        "# The numeric regime codes are converted back to their original names using the regime_mapping,\n",
        "# and then further shortened using regime_short_mapping.\n",
        "\n",
        "# %%\n",
        "# Define factors and regime columns (using global variables if already defined)\n",
        "factors_columns = FACTORS\n",
        "regimes_column = REGIMES_COLUMN   # Assumes REGIMES_COLUMN was defined earlier\n",
        "\n",
        "# Use the previously created regime_short_mapping to convert numeric codes back to short names.\n",
        "# (If a code is not in regime_short_mapping, it will default to \"Regime <code>\")\n",
        "regime_short_names = {reg: regime_short_mapping.get(reg, f\"Regime {reg}\")\n",
        "                      for reg in df[regimes_column].unique()}\n",
        "\n",
        "sharpe_ratios = {\n",
        "    regime_short_names[regime]: (\n",
        "        df[df[regimes_column] == regime][factors_columns].mean() /\n",
        "        df[df[regimes_column] == regime][factors_columns].std()\n",
        "    )\n",
        "    for regime in df[regimes_column].unique()\n",
        "}\n",
        "\n",
        "# Calculate the \"Unconditional\" Sharpe ratios (using all data)\n",
        "sharpe_ratios[\"Unconditional\"] = df[factors_columns].mean() / df[factors_columns].std()\n",
        "\n",
        "# Convert the dictionary to a DataFrame and set column names\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios).T\n",
        "sharpe_ratios_df.columns = factors_columns\n",
        "\n",
        "# Plot the Sharpe ratios using the same styling as before.\n",
        "plt.figure(figsize=(14, 8))\n",
        "sharpe_ratios_df.plot(\n",
        "    kind=\"bar\",\n",
        "    grid=True,\n",
        "    colormap=\"viridis\",\n",
        "    title=\"Sharpe Ratios by Regime and Unconditional\",\n",
        "    figsize=(14, 8)\n",
        ")\n",
        "plt.ylabel(\"Sharpe Ratio\", fontsize=12)\n",
        "plt.xlabel(\"Market Regimes\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(title=\"Factors\", fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCkBikW6A2o0"
      },
      "id": "GCkBikW6A2o0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance by period"
      ],
      "metadata": {
        "id": "SwsjcsWyoH2Z"
      },
      "id": "SwsjcsWyoH2Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======= USER-DEFINED DATE RANGE =======\n",
        "# Adjust these dates to view feature importances for a specific period\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date   = pd.to_datetime('2022-12-31')\n",
        "\n",
        "# ======= Filter the Data =======\n",
        "# Filter the results_df for the specified date range based on the 'Predicted_month' column\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "]\n",
        "\n",
        "# ======= Get Unique Regimes and Feature Count =======\n",
        "existing_regimes = filtered_results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "n_features = len(filtered_results_df['Feature_Importances'].iloc[0])  # Assumes each entry is a vector\n",
        "\n",
        "# ======= Robust Feature Naming =======\n",
        "try:\n",
        "    # Validate if the predefined FEATURES list matches the actual feature count\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"‚ö†Ô∏è Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features}).\")\n",
        "        print(\"Using auto-generated feature names instead.\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate default feature names if there's a mismatch or if FEATURES is undefined\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features.\")\n",
        "\n",
        "# ======= Compute Overall Average Feature Importances =======\n",
        "overall_avg_fi = np.vstack(filtered_results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# ======= Compute Regime-Specific Average Feature Importances =======\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = filtered_results_df[filtered_results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# ======= Sort Features by Overall Importance (Descending) =======\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # Ensure index bounds\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# ======= Plotting =======\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One overall plot plus one for each regime\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Feature Importances ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    # If zero or one regime, show only the overall chart\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Extra space for feature labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XgeSIiAu-M2b"
      },
      "id": "XgeSIiAu-M2b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}