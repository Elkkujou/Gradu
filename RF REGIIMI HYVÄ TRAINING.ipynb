{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IDEAT: KMEANS VIX, Cap max weight for factor\n",
        "\n",
        "<div style=\"font-size:14px;\">\n",
        "<strong>TO DO:</strong><br><br>\n",
        "Tarkistaa ovatko regiimit oikein, exp. antaa ainoana kaikille neg sharpet<br>\n",
        "Katsoa vielä financial turbulence koodi<br>\n",
        "Data varmistukset (ei dataa tulevaisuudesta)<br><br>\n",
        "\n",
        "<strong>Lisää features:</strong><br>\n",
        "RSI<br>\n",
        "Yield spread<br>\n",
        "Muita??<br><br>\n",
        "\n",
        "<strong>Muuta:</strong><br>\n",
        "regiimi testaus drawdowneilla?<br><br>\n",
        "regiimi specifi model ennustus?\n",
        "\n",
        "\n",
        "<strong>Mallin kehitys:</strong><br>\n",
        "1. Feature eliminointi<br>\n",
        "2. Training interval<br>\n",
        "3. Hyperparametrit\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "mqpPtrCOkXAO"
      },
      "id": "mqpPtrCOkXAO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GIT & imports"
      ],
      "metadata": {
        "id": "k1mEzhhWXup6"
      },
      "id": "k1mEzhhWXup6"
    },
    {
      "cell_type": "code",
      "source": [
        "# # GitHub Repository Setup\n",
        "#\n",
        "# This cell navigates to `/content`, removes any previous clone of the repository,\n",
        "# clones the latest version from GitHub, and lists the repository files.\n",
        "\n",
        "# %%\n",
        "%cd /content\n",
        "!rm -rf Gradu\n",
        "!git clone https://github.com/Elkkujou/Gradu.git\n",
        "%cd /content/Gradu\n",
        "!ls\n"
      ],
      "metadata": {
        "id": "O58kIxTz4ycb",
        "outputId": "6444f9df-280a-4acb-8743-f36bedcaa375",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "O58kIxTz4ycb",
      "execution_count": 807,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Gradu'...\n",
            "remote: Enumerating objects: 578, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 578 (delta 147), reused 124 (delta 124), pack-reused 420 (from 2)\u001b[K\n",
            "Receiving objects: 100% (578/578), 114.25 MiB | 32.38 MiB/s, done.\n",
            "Resolving deltas: 100% (271/271), done.\n",
            "/content/Gradu\n",
            " chatti_RF.ipynb\t\t      regime_prediction_msci.ipynb\n",
            " data+regimes.xlsx\t\t      regime_pred.txt\n",
            " Fama_french_XGBOOST.ipynb\t      RF_Gradu.ipynb\n",
            "'Financial turbulence.ipynb'\t     'RF REGIIMI HYVÄ TRAINING.ipynb'\n",
            " FT_source.xlsx\t\t\t     'RF_REGIIMI_HYVÄ_TRAINING (MSCI).ipynb'\n",
            " Gradient_boost_malli.ipynb\t     'RF_regime (3).ipynb'\n",
            " MSCI_XGBOOST.ipynb\t\t      THE_2ND_latest.xlsx\n",
            " Regiimi_prediction.ipynb\t      THE_2ND.xlsx\n",
            " regime_prediction_famafrench.ipynb   THE_ONE.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_regime_split = False\n",
        "\n",
        "RF = False # perus random forest\n",
        "GB = False # perus gradient boost\n",
        "RF_feature_seek = True # random forest all combinations / not updated\n",
        "Hybrid = False\n",
        "\n",
        "FF5 = True\n",
        "FF5_long = False\n",
        "MSCI = False\n"
      ],
      "metadata": {
        "id": "-AF3FdwvaLPp"
      },
      "id": "-AF3FdwvaLPp",
      "execution_count": 808,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if sum([RF, GB, RF_feature_seek, Hybrid]) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [RF, GB, RF_feature_seek, Hybrid] must be True.\")\n",
        "\n",
        "# Check subgroup 2: Exactly one of [FF5, FF5_long, MSCI] must be True\n",
        "if sum([FF5, FF5_long, MSCI]) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [FF5, FF5_long, MSCI] must be True.\")\n",
        "\n",
        "print(\"Toggles are correctly set.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ktRzXgrQHYX",
        "outputId": "23d37305-d050-41c1-df19-31434a9881f5"
      },
      "id": "7ktRzXgrQHYX",
      "execution_count": 809,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toggles are correctly set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 810,
      "id": "4085d55c-e568-465c-9bcc-6013281c105d",
      "metadata": {
        "tags": [],
        "id": "4085d55c-e568-465c-9bcc-6013281c105d"
      },
      "outputs": [],
      "source": [
        "# # Import Required Libraries\n",
        "#\n",
        "# Import all necessary libraries for data manipulation, visualization,\n",
        "# machine learning, and regression analysis.\n",
        "\n",
        "# %%\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from tabulate import tabulate\n",
        "\n",
        "from IPython.display import display, HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5:\n",
        "  SHEET_NAME = \"ajodata_FF5\"\n",
        "  FEATURES = ['CPI%', 'GARCH_1M', 'T10YFF', 'LEI%', 'Amihud']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = False"
      ],
      "metadata": {
        "id": "R_dU9PNF-Puv"
      },
      "id": "R_dU9PNF-Puv",
      "execution_count": 811,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5_long:\n",
        "  SHEET_NAME = \"ajodata_FF5_long\"\n",
        "  FEATURES = ['CPI%', 'GARCH_1M', 'T10YFF', 'LEI%', 'Amihud']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "wJo52ErY-v0Q"
      },
      "id": "wJo52ErY-v0Q",
      "execution_count": 812,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MSCI:\n",
        "  SHEET_NAME = \"ajodata_MSCI\"\n",
        "  FEATURES = ['CPI%', 'GARCH_1M', 'T10YFF', 'LEI%', 'Amihud']\n",
        "  FACTORS = [\n",
        "    'Size',\n",
        "    'value',\n",
        "    'Quality',\n",
        "    'min_vola']\n",
        "  BENCHMARK = ['Us_standard']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "AT6Cf_ge_ApN"
      },
      "id": "AT6Cf_ge_ApN",
      "execution_count": 813,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 814,
      "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
      "metadata": {
        "tags": [],
        "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
        "outputId": "e5ceb91b-d0a9-4375-a64a-8af195914aeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers in the 'ajodata_MSCI' sheet:\n",
            "Index(['Date', 'Us_standard', 'Size', 'Momentum', 'min_vola', 'value',\n",
            "       'Quality', 'GARCH_1M', 'CPI%', 'T10YFF', 'T10YFF_CHG', 'Amihud',\n",
            "       'LEI%'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table class=\"dataframe table table-striped\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>First observation date</td>\n",
              "      <td>1979-12-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Last observation date</td>\n",
              "      <td>2024-11-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Total number of observations</td>\n",
              "      <td>540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "df = xls_file.parse(SHEET_NAME)\n",
        "\n",
        "df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "# Print headers dynamically\n",
        "print(f\"Headers in the '{SHEET_NAME}' sheet:\")\n",
        "print(df.columns)\n",
        "\n",
        "REGIMES_COLUMN = 'Predicted_reg'\n",
        "\n",
        "# Convert the leftmost column (assumed to be the date column) to datetime\n",
        "date_column = df.columns[0]\n",
        "df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "# Retrieve first and last observation dates and count observations\n",
        "first_date = df[date_column].iloc[0]\n",
        "last_date = df[date_column].iloc[-1]\n",
        "n_observations = len(df)\n",
        "\n",
        "# Create a DataFrame with the information\n",
        "info_df = pd.DataFrame({\n",
        "    \"Description\": [\"First observation date\", \"Last observation date\", \"Total number of observations\"],\n",
        "    \"Value\": [first_date, last_date, n_observations]\n",
        "})\n",
        "\n",
        "# Display the results as a neat HTML table\n",
        "display(HTML(info_df.to_html(index=False, classes=\"table table-striped\", border=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Define Helper Functions ---\n",
        "def annualized_return(returns):\n",
        "    \"\"\"Compute the compounded annualized return (assuming monthly returns).\"\"\"\n",
        "    return np.prod(1 + returns)**(12 / len(returns)) - 1\n",
        "\n",
        "def compute_metrics(returns):\n",
        "    \"\"\"\n",
        "    Compute key metrics for a returns series:\n",
        "      - Annualized Return\n",
        "      - Annualized Volatility (assuming monthly returns)\n",
        "      - Total Cumulative Return\n",
        "    \"\"\"\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    total_cum_return = cumulative_returns.iloc[-1] - 1\n",
        "    ann_ret = annualized_return(returns)\n",
        "    ann_vol = np.std(returns) * np.sqrt(12)\n",
        "    return ann_ret, ann_vol, total_cum_return\n",
        "\n",
        "# --- Compute Metrics for Benchmark and Each Factor ---\n",
        "metrics = []\n",
        "\n",
        "# Compute metrics for the benchmark.\n",
        "benchmark_returns = df[BENCHMARK[0]]\n",
        "bench_ann_ret, bench_ann_vol, bench_cum_return = compute_metrics(benchmark_returns)\n",
        "metrics.append({\n",
        "    \"Strategy\": \"Benchmark\",\n",
        "    \"Annualized Return\": f\"{bench_ann_ret*100:.2f}%\",\n",
        "    \"Annualized Volatility\": f\"{bench_ann_vol*100:.2f}%\",\n",
        "    \"Total Cumulative Return\": f\"{bench_cum_return*100:.2f}%\"\n",
        "})\n",
        "\n",
        "# Compute metrics for each factor in FACTORS.\n",
        "for factor in FACTORS:\n",
        "    factor_returns = df[factor]\n",
        "    factor_ann_ret, factor_ann_vol, factor_cum_return = compute_metrics(factor_returns)\n",
        "    metrics.append({\n",
        "        \"Strategy\": factor,\n",
        "        \"Annualized Return\": f\"{factor_ann_ret*100:.2f}%\",\n",
        "        \"Annualized Volatility\": f\"{factor_ann_vol*100:.2f}%\",\n",
        "        \"Total Cumulative Return\": f\"{factor_cum_return*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the metrics.\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# --- Display the Results as an HTML Table ---\n",
        "display(HTML(metrics_df.to_html(index=False)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "phBSNpYXviHe",
        "outputId": "1d41c884-c781-4443-f981-ec1cb59b4197",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "phBSNpYXviHe",
      "execution_count": 815,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Strategy</th>\n",
              "      <th>Annualized Return</th>\n",
              "      <th>Annualized Volatility</th>\n",
              "      <th>Total Cumulative Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Benchmark</td>\n",
              "      <td>9.35%</td>\n",
              "      <td>15.20%</td>\n",
              "      <td>5492.47%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Size</td>\n",
              "      <td>9.60%</td>\n",
              "      <td>14.27%</td>\n",
              "      <td>6085.82%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>value</td>\n",
              "      <td>9.14%</td>\n",
              "      <td>15.39%</td>\n",
              "      <td>5012.40%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Quality</td>\n",
              "      <td>11.35%</td>\n",
              "      <td>15.12%</td>\n",
              "      <td>12519.41%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>min_vola</td>\n",
              "      <td>6.64%</td>\n",
              "      <td>11.72%</td>\n",
              "      <td>1706.35%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 816,
      "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
      "metadata": {
        "tags": [],
        "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
        "outputId": "fbeb448c-009a-47f4-caab-f04af7cb8ee3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows before cleaning: 540\n",
            "\n",
            "Missing values in feature columns before dropping NA:\n",
            "CPI%        0\n",
            "GARCH_1M    0\n",
            "T10YFF      0\n",
            "LEI%        0\n",
            "Amihud      0\n",
            "dtype: int64\n",
            "\n",
            "No missing values found in feature columns. Data is clean.\n",
            "\n",
            "Indices aligned: True\n",
            "\n",
            "Parameters and dataset verified.\n",
            "\n",
            "Winning Factor counts:\n",
            "Quality     193\n",
            "Size        126\n",
            "value       116\n",
            "min_vola    105\n",
            "Total       540\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# # Prepare Data for Model Training – Data Cleaning & Verification\n",
        "\n",
        "# we keep all rows and compute the winning factor as the factor (from FACTORS) with the highest value in each row.\n",
        "\n",
        "# Print the initial number of rows.\n",
        "initial_rows = len(df)\n",
        "print(f\"Total number of rows before cleaning: {initial_rows}\")\n",
        "\n",
        "# Check missing values in feature columns (FEATURES) before dropping NAs.\n",
        "missing_counts = df[FEATURES].isna().sum()\n",
        "print(\"\\nMissing values in feature columns before dropping NA:\")\n",
        "print(missing_counts)\n",
        "\n",
        "# Save the number of rows before dropping NA and then drop rows with missing values in FEATURES.\n",
        "initial_rows_features = len(df)\n",
        "X = df[FEATURES].dropna()\n",
        "rows_after_drop = len(X)\n",
        "dropped_rows = initial_rows_features - rows_after_drop\n",
        "\n",
        "if dropped_rows > 0:\n",
        "    print(f\"\\nDropped {dropped_rows} rows due to missing values in feature columns.\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found in feature columns. Data is clean.\")\n",
        "\n",
        "# Compute the Winning Factor by taking the column (from FACTORS) that has the maximum value in each row.\n",
        "# This assumes that the FACTORS columns exist in df and contain numeric values.\n",
        "df['Winning Factor'] = df[FACTORS].idxmax(axis=1)\n",
        "\n",
        "# Define the target variable based on rows retained in X.\n",
        "# The winning factor is encoded as a categorical variable.\n",
        "y = df['Winning Factor'].astype('category').cat.codes.loc[X.index]\n",
        "print(\"\\nIndices aligned:\", X.index.equals(y.index))\n",
        "\n",
        "# Ensure the data is sorted by date.\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Verify that all required columns exist.\n",
        "# Here, we require the FEATURES columns, the 'USA Standard (Large+Mid Cap)' column,\n",
        "# as well as all the FACTORS and the BENCHMARK columns.\n",
        "required_columns = FEATURES + FACTORS + BENCHMARK\n",
        "for col in required_columns:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Missing required column: {col}\")\n",
        "print(\"\\nParameters and dataset verified.\")\n",
        "\n",
        "# Compute the counts for each winning factor.\n",
        "winning_factor_counts = df['Winning Factor'].value_counts()\n",
        "\n",
        "# Compute total count and append it as the last row.\n",
        "total_counts = winning_factor_counts.sum()\n",
        "winning_factor_counts = pd.concat([winning_factor_counts, pd.Series({'Total': total_counts})])\n",
        "\n",
        "# Print the counts with \"Total\" as the last row.\n",
        "print(\"\\nWinning Factor counts:\")\n",
        "print(winning_factor_counts)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_regime_split:\n",
        "\n",
        "    # --- Regime Mapping & Conversion to Numeric Codes (Dynamic) ---\n",
        "\n",
        "    # Dynamically extract the unique values in the REGIMES_COLUMN.\n",
        "    unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "\n",
        "    # Convert the Regimes column to a categorical type with the unique values, ordered alphabetically.\n",
        "    df[REGIMES_COLUMN] = pd.Categorical(df[REGIMES_COLUMN], categories=sorted(unique_regimes), ordered=True)\n",
        "\n",
        "    # Create a dictionary mapping numeric codes to the regime names based on the unique values.\n",
        "    regime_mapping = {i: cat for i, cat in enumerate(df[REGIMES_COLUMN].cat.categories)}\n",
        "\n",
        "    # Now encode the Regimes column as numeric codes.\n",
        "    df[REGIMES_COLUMN] = df[REGIMES_COLUMN].cat.codes\n",
        "\n",
        "    # Create a mapping from numeric codes to original regime names.\n",
        "    regime_short_mapping = {code: name for code, name in regime_mapping.items()}\n",
        "\n",
        "    # Calculate the number of observations for each regime using value_counts (without reindexing).\n",
        "    obs_counts = df[REGIMES_COLUMN].value_counts(sort=False)\n",
        "\n",
        "    # Create a DataFrame preview of the regime mapping, including observation counts.\n",
        "    mapping_table_data = []\n",
        "    for code in regime_mapping.keys():\n",
        "        mapping_table_data.append({\n",
        "            \"Numeric Code\": code,\n",
        "            \"Original Name\": regime_mapping.get(code, \"N/A\"),\n",
        "            \"Observations\": obs_counts.get(code, 0)\n",
        "        })\n",
        "\n",
        "    # Append a row with the total observations.\n",
        "    total_obs = obs_counts.sum()\n",
        "    mapping_table_data.append({\n",
        "        \"Numeric Code\": \"\",\n",
        "        \"Original Name\": \"Total\",\n",
        "        \"Observations\": total_obs\n",
        "    })\n",
        "\n",
        "    # Create the DataFrame for regime mapping preview and print.\n",
        "    regime_mapping_df = pd.DataFrame(mapping_table_data)\n",
        "\n",
        "    from tabulate import tabulate\n",
        "    print(\"Preview of Dynamic Regime Mapping:\")\n",
        "    print(tabulate(regime_mapping_df, headers=\"keys\", tablefmt=\"psql\", showindex=False))\n"
      ],
      "metadata": {
        "id": "wYgvlvGRUUG4"
      },
      "id": "wYgvlvGRUUG4",
      "execution_count": 817,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "0wYzowb5Xdau"
      },
      "id": "0wYzowb5Xdau"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameters Table\n",
        "\n",
        "| Hyperparameter       | Purpose                                   | Common Choices                  |\n",
        "|----------------------|-------------------------------------------|---------------------------------|\n",
        "| `n_estimators`      | Number of trees                           | 100 (default), 200, 500         |\n",
        "| `max_depth`         | Max tree depth                            | `None` (default), 10, 20        |\n",
        "| `min_samples_split` | Min samples needed to split a node       | 2 (default), 10, 20             |\n",
        "| `min_samples_leaf`  | Min samples in a leaf                    | 1 (default), 5, 10              |\n",
        "| `max_features`      | Features per split                       | `'sqrt'` (default), `'log2'`, `None` |\n",
        "| `bootstrap`         | Use bootstrap sampling                    | `True` (default), `False`       |\n",
        "| `random_state`      | Set a random seed                         | `None`, 42, 0                   |\n",
        "| `criterion`         | Splitting method                          | `'gini'` (default), `'entropy'` |\n",
        "| `oob_score`        | Out-of-bag validation                     | `False` (default), `True`       |\n",
        "| `n_jobs`           | Parallel training                         | `None`, `-1` (all CPUs)         |\n"
      ],
      "metadata": {
        "id": "g3wGcKzuVZJ7"
      },
      "id": "g3wGcKzuVZJ7"
    },
    {
      "cell_type": "code",
      "source": [
        "# This removes any row with at least one NaN in any column\n",
        "df = df.dropna()\n",
        "\n",
        "# Optionally, reindex the rows\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "nFVXlc95MuWg"
      },
      "id": "nFVXlc95MuWg",
      "execution_count": 818,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dzDmh34EUy4R",
        "outputId": "2de4a57a-d32d-40e9-ecce-dacc1f2a43fe"
      },
      "id": "dzDmh34EUy4R",
      "execution_count": 819,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date  Us_standard      Size  Momentum  min_vola     value   Quality  \\\n",
              "0   1988-06-30     0.043323  0.043799  0.033579  0.032225  0.047154  0.037395   \n",
              "1   1988-07-30    -0.006198 -0.012095 -0.012041 -0.007055 -0.004743  0.001711   \n",
              "2   1988-08-30    -0.036689 -0.026526 -0.047391 -0.023115 -0.036281 -0.026872   \n",
              "3   1988-09-30     0.039493  0.038130  0.041126  0.054148  0.037351  0.051736   \n",
              "4   1988-10-30     0.027519  0.014227  0.017219  0.018087  0.027552  0.021731   \n",
              "..         ...          ...       ...       ...       ...       ...       ...   \n",
              "433 2024-07-30     0.011716  0.047299 -0.021593  0.036042  0.034542  0.002305   \n",
              "434 2024-08-30     0.022720  0.031729  0.028126  0.047736  0.015412  0.034802   \n",
              "435 2024-09-30     0.020355  0.016242  0.030033  0.003052  0.010191  0.008523   \n",
              "436 2024-10-30    -0.008114 -0.015492 -0.003074 -0.015629 -0.009584 -0.018075   \n",
              "437 2024-11-30     0.061165  0.060656  0.050177  0.048262  0.064180  0.043128   \n",
              "\n",
              "     GARCH_1M     CPI%  T10YFF  T10YFF_CHG        Amihud      LEI%  \\\n",
              "0    0.802078  0.42553    1.28       -0.17  1.312876e-13  0.003407   \n",
              "1    1.471245  0.42373    1.11       -0.78  9.325133e-15  0.006791   \n",
              "2    0.703486  0.42194    0.33       -0.07  6.857909e-14  0.000000   \n",
              "3    0.408904  0.42017    0.26        0.30  4.239543e-14  0.003373   \n",
              "4    0.733629  0.33473    0.56       -0.46  6.126809e-14  0.003361   \n",
              "..        ...      ...     ...         ...           ...       ...   \n",
              "433  1.087011  0.13892   -1.42        0.40  0.000000e+00 -0.001940   \n",
              "434  0.641443  0.18019   -1.02        0.47  0.000000e+00 -0.004859   \n",
              "435  0.396629  0.22920   -0.55        0.15  0.000000e+00 -0.002930   \n",
              "436  0.684819  0.22646   -0.40        0.65  0.000000e+00 -0.003918   \n",
              "437  0.499183  0.28045    0.25        0.00  0.000000e+00 -0.002950   \n",
              "\n",
              "    Winning Factor  \n",
              "0            value  \n",
              "1          Quality  \n",
              "2         min_vola  \n",
              "3         min_vola  \n",
              "4            value  \n",
              "..             ...  \n",
              "433           Size  \n",
              "434       min_vola  \n",
              "435           Size  \n",
              "436          value  \n",
              "437          value  \n",
              "\n",
              "[438 rows x 14 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-12671412-79c7-4283-b73e-9f8f20e0539e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Us_standard</th>\n",
              "      <th>Size</th>\n",
              "      <th>Momentum</th>\n",
              "      <th>min_vola</th>\n",
              "      <th>value</th>\n",
              "      <th>Quality</th>\n",
              "      <th>GARCH_1M</th>\n",
              "      <th>CPI%</th>\n",
              "      <th>T10YFF</th>\n",
              "      <th>T10YFF_CHG</th>\n",
              "      <th>Amihud</th>\n",
              "      <th>LEI%</th>\n",
              "      <th>Winning Factor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1988-06-30</td>\n",
              "      <td>0.043323</td>\n",
              "      <td>0.043799</td>\n",
              "      <td>0.033579</td>\n",
              "      <td>0.032225</td>\n",
              "      <td>0.047154</td>\n",
              "      <td>0.037395</td>\n",
              "      <td>0.802078</td>\n",
              "      <td>0.42553</td>\n",
              "      <td>1.28</td>\n",
              "      <td>-0.17</td>\n",
              "      <td>1.312876e-13</td>\n",
              "      <td>0.003407</td>\n",
              "      <td>value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1988-07-30</td>\n",
              "      <td>-0.006198</td>\n",
              "      <td>-0.012095</td>\n",
              "      <td>-0.012041</td>\n",
              "      <td>-0.007055</td>\n",
              "      <td>-0.004743</td>\n",
              "      <td>0.001711</td>\n",
              "      <td>1.471245</td>\n",
              "      <td>0.42373</td>\n",
              "      <td>1.11</td>\n",
              "      <td>-0.78</td>\n",
              "      <td>9.325133e-15</td>\n",
              "      <td>0.006791</td>\n",
              "      <td>Quality</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1988-08-30</td>\n",
              "      <td>-0.036689</td>\n",
              "      <td>-0.026526</td>\n",
              "      <td>-0.047391</td>\n",
              "      <td>-0.023115</td>\n",
              "      <td>-0.036281</td>\n",
              "      <td>-0.026872</td>\n",
              "      <td>0.703486</td>\n",
              "      <td>0.42194</td>\n",
              "      <td>0.33</td>\n",
              "      <td>-0.07</td>\n",
              "      <td>6.857909e-14</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>min_vola</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1988-09-30</td>\n",
              "      <td>0.039493</td>\n",
              "      <td>0.038130</td>\n",
              "      <td>0.041126</td>\n",
              "      <td>0.054148</td>\n",
              "      <td>0.037351</td>\n",
              "      <td>0.051736</td>\n",
              "      <td>0.408904</td>\n",
              "      <td>0.42017</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.30</td>\n",
              "      <td>4.239543e-14</td>\n",
              "      <td>0.003373</td>\n",
              "      <td>min_vola</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1988-10-30</td>\n",
              "      <td>0.027519</td>\n",
              "      <td>0.014227</td>\n",
              "      <td>0.017219</td>\n",
              "      <td>0.018087</td>\n",
              "      <td>0.027552</td>\n",
              "      <td>0.021731</td>\n",
              "      <td>0.733629</td>\n",
              "      <td>0.33473</td>\n",
              "      <td>0.56</td>\n",
              "      <td>-0.46</td>\n",
              "      <td>6.126809e-14</td>\n",
              "      <td>0.003361</td>\n",
              "      <td>value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>433</th>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>0.011716</td>\n",
              "      <td>0.047299</td>\n",
              "      <td>-0.021593</td>\n",
              "      <td>0.036042</td>\n",
              "      <td>0.034542</td>\n",
              "      <td>0.002305</td>\n",
              "      <td>1.087011</td>\n",
              "      <td>0.13892</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.001940</td>\n",
              "      <td>Size</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>434</th>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>0.022720</td>\n",
              "      <td>0.031729</td>\n",
              "      <td>0.028126</td>\n",
              "      <td>0.047736</td>\n",
              "      <td>0.015412</td>\n",
              "      <td>0.034802</td>\n",
              "      <td>0.641443</td>\n",
              "      <td>0.18019</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>0.47</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.004859</td>\n",
              "      <td>min_vola</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>435</th>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>0.020355</td>\n",
              "      <td>0.016242</td>\n",
              "      <td>0.030033</td>\n",
              "      <td>0.003052</td>\n",
              "      <td>0.010191</td>\n",
              "      <td>0.008523</td>\n",
              "      <td>0.396629</td>\n",
              "      <td>0.22920</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.002930</td>\n",
              "      <td>Size</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>436</th>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>-0.008114</td>\n",
              "      <td>-0.015492</td>\n",
              "      <td>-0.003074</td>\n",
              "      <td>-0.015629</td>\n",
              "      <td>-0.009584</td>\n",
              "      <td>-0.018075</td>\n",
              "      <td>0.684819</td>\n",
              "      <td>0.22646</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.65</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.003918</td>\n",
              "      <td>value</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>437</th>\n",
              "      <td>2024-11-30</td>\n",
              "      <td>0.061165</td>\n",
              "      <td>0.060656</td>\n",
              "      <td>0.050177</td>\n",
              "      <td>0.048262</td>\n",
              "      <td>0.064180</td>\n",
              "      <td>0.043128</td>\n",
              "      <td>0.499183</td>\n",
              "      <td>0.28045</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.002950</td>\n",
              "      <td>value</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>438 rows × 14 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-12671412-79c7-4283-b73e-9f8f20e0539e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-12671412-79c7-4283-b73e-9f8f20e0539e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-12671412-79c7-4283-b73e-9f8f20e0539e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-4e84517f-e11d-43de-82bf-b02b4b06503a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4e84517f-e11d-43de-82bf-b02b4b06503a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-4e84517f-e11d-43de-82bf-b02b4b06503a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_2ab01b5a-4e51-4613-b474-c44ac42e09da\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_2ab01b5a-4e51-4613-b474-c44ac42e09da button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 438,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1988-06-30 00:00:00\",\n        \"max\": \"2024-11-30 00:00:00\",\n        \"num_unique_values\": 438,\n        \"samples\": [\n          \"2018-10-30 00:00:00\",\n          \"1994-12-30 00:00:00\",\n          \"2011-08-30 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Us_standard\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04277676749386758,\n        \"min\": -0.17247092151428534,\n        \"max\": 0.13021867009232202,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.07046891607068873,\n          0.012444742853252588,\n          -0.0577273454765318\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03960168846817539,\n        \"min\": -0.18559157898431033,\n        \"max\": 0.11701007427132692,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.06034430255138201,\n          0.013087106519586333,\n          -0.038054539535069876\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Momentum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04625323057234239,\n        \"min\": -0.15837394203641963,\n        \"max\": 0.1439475982532752,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.09910880339680406,\n          0.01507912042745585,\n          -0.0587782000728283\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min_vola\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.033875393531583545,\n        \"min\": -0.14684136005156523,\n        \"max\": 0.09529696481347072,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.041482713798317694,\n          0.01898701237254863,\n          -0.0052616847191871985\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04386354828676989,\n        \"min\": -0.1804150610558456,\n        \"max\": 0.14375032299260648,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.05705292773744042,\n          0.011581151945796009,\n          -0.06281898558485688\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Quality\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04187052746539238,\n        \"min\": -0.15216966431608692,\n        \"max\": 0.14056177223552724,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          -0.07152955596675825,\n          0.006833474880773682,\n          -0.04327265730063501\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GARCH_1M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5796164726589572,\n        \"min\": 0.2060233522734674,\n        \"max\": 5.01881392087039,\n        \"num_unique_values\": 438,\n        \"samples\": [\n          1.6648177618407694,\n          0.3312976867341236,\n          2.8086772254272683\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPI%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27105347455029777,\n        \"min\": -1.77055,\n        \"max\": 1.37685,\n        \"num_unique_values\": 424,\n        \"samples\": [\n          0.05682,\n          0.26968,\n          -0.10805\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T10YFF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.360236043017669,\n        \"min\": -1.88,\n        \"max\": 4.04,\n        \"num_unique_values\": 294,\n        \"samples\": [\n          2.04,\n          -0.8,\n          2.99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T10YFF_CHG\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5322949704744593,\n        \"min\": -2.7,\n        \"max\": 1.97,\n        \"num_unique_values\": 169,\n        \"samples\": [\n          -0.57,\n          0.1,\n          0.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Amihud\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.81620846232997e-14,\n        \"min\": 0.0,\n        \"max\": 2.357760130064222e-13,\n        \"num_unique_values\": 392,\n        \"samples\": [\n          2.479172078396074e-15,\n          1.533110041356645e-15,\n          3.707221785587093e-15\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LEI%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007739091721748512,\n        \"min\": -0.05488372093023261,\n        \"max\": 0.022222222222222195,\n        \"num_unique_values\": 400,\n        \"samples\": [\n          -0.0041279669762642485,\n          0.009667024704618752,\n          0.0071684587813621095\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Winning Factor\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Quality\",\n          \"Size\",\n          \"value\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 819
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if RF_feature_seek:\n",
        "    import itertools\n",
        "    import os\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # --------------------------\n",
        "    # Parameters for Feature & Training Window Search\n",
        "    # --------------------------\n",
        "    min_features = 2               # minimum number of features in a subset\n",
        "    max_features = len(FEATURES)     # maximum number of features (or set to a smaller number if desired)\n",
        "    # Define fixed rolling window sizes (in years) to test (assuming monthly data)\n",
        "    training_window_years = [5, 10, 15, 20]\n",
        "    # Also run an expanding window experiment (like use_fixed_window = False)\n",
        "    run_expanding_window = True\n",
        "\n",
        "    output_filename = \"feature_subset_results.csv\"\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    # Ensure the data is sorted by date.\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # --------------------------\n",
        "    # Outer Loop: Fixed Rolling Window Modes\n",
        "    # --------------------------\n",
        "    for years in training_window_years:\n",
        "        # Convert years to number of observations (assume 12 obs per year)\n",
        "        rolling_window_size = years * 12\n",
        "        print(f\"\\n--- Testing fixed rolling window of {years} years ({rolling_window_size} observations) ---\")\n",
        "        outer_start_time = time.time()\n",
        "\n",
        "        # Inner loop over feature subset sizes\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            # Loop over all combinations of size r\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination: {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # Loop over test rows, starting when we have enough training data\n",
        "                for i in range(rolling_window_size, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    # Build fixed rolling training window (most recent rolling_window_size observations)\n",
        "                    train_window = df_sorted.iloc[i - rolling_window_size: i].copy()\n",
        "\n",
        "                    # Ensure the last training observation is strictly before test row date\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    # (Optional) Regime check if use_regime_split is True:\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                        if insufficient_regimes:\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "\n",
        "                    # Prepare training data for the current feature subset.\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    # Train the RandomForest model.\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    # Use the last row of the training window as test data.\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    # Map the predicted probabilities to the full set of FACTORS.\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    # Optional: Gather additional metrics.\n",
        "                    tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "                    avg_depth = np.mean(tree_depths)\n",
        "                    max_depth = np.max(tree_depths)\n",
        "                    months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                                    (Predicted_month.month - last_train_date.month))\n",
        "                    feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features}\n",
        "\n",
        "                    result = {\n",
        "                        'Features_used': str(current_features),\n",
        "                        'Predicted_month': Predicted_month,\n",
        "                        'Allocated_Return': allocated_return,\n",
        "                        'Predicted_Winner': predicted_winner,\n",
        "                        'Actual_Winner': test_row['Winning Factor'],\n",
        "                        'Num_Trees': rf_model.n_estimators,\n",
        "                        'Average_Tree_Depth': avg_depth,\n",
        "                        'Max_Tree_Depth': max_depth,\n",
        "                        'Prediction_Horizon_Months': months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                # End of inner test row loop for this feature combination.\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Results for combination {current_features} appended to CSV. Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed fixed rolling window of {years} years in {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Expanding Window Mode\n",
        "    # --------------------------\n",
        "    if run_expanding_window:\n",
        "        print(\"\\n--- Testing Expanding Window Mode ---\")\n",
        "        outer_start_time = time.time()\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination (expanding): {current_features}\")\n",
        "                results = []\n",
        "                # In expanding mode, the training window goes from the start until the test row.\n",
        "                for i in range(1, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "                    train_window = df_sorted.iloc[:i].copy()\n",
        "                    if train_window.empty:\n",
        "                        continue\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "                    tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "                    avg_depth = np.mean(tree_depths)\n",
        "                    max_depth = np.max(tree_depths)\n",
        "                    months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                                    (Predicted_month.month - last_train_date.month))\n",
        "                    feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features}\n",
        "                    result = {\n",
        "                        'Features_used': str(current_features),\n",
        "                        'Predicted_month': Predicted_month,\n",
        "                        'Allocated_Return': allocated_return,\n",
        "                        'Predicted_Winner': predicted_winner,\n",
        "                        'Actual_Winner': test_row['Winning Factor'],\n",
        "                        'Num_Trees': rf_model.n_estimators,\n",
        "                        'Average_Tree_Depth': avg_depth,\n",
        "                        'Max_Tree_Depth': max_depth,\n",
        "                        'Prediction_Horizon_Months': months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Expanding window: Results for combination {current_features} appended to CSV. Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed Expanding Window Mode in {minutes:02d}:{seconds:02d}\")\n"
      ],
      "metadata": {
        "id": "kraj1YkNhEq4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "ef9d8f3b-f236-451c-b1e3-c5662573cddb"
      },
      "id": "kraj1YkNhEq4",
      "execution_count": 820,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Testing fixed rolling window of 5 years (60 observations) ---\n",
            "\n",
            "Testing feature combination: ['CPI%', 'GARCH_1M']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-820-85e2753404a6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0;31m# Train the RandomForest model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                     \u001b[0mrf_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                     \u001b[0mrf_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m                     \u001b[0;31m# Use the last row of the training window as test data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    485\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m             trees = Parallel(\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"balanced\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m         tree._fit(\n\u001b[0m\u001b[1;32m    190\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_classification\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m     \"\"\"Ensure that target y is of a non-regression type.\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "MrSJ4xhmDuzE"
      },
      "id": "MrSJ4xhmDuzE"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_years_train = 5      # Min years of data needed\n",
        "    min_obs_regime = 50      # Min obs per regime if splitting\n",
        "    min_obs_train = 0        # Min total obs after dropping NAs\n",
        "    use_regime_split = False # Toggle regime-based training or not\n",
        "\n",
        "    # New toggle for training window type:\n",
        "    use_fixed_window = False  # Set to True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 120  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either a fixed-size (rolling) window or an expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check training years\n",
        "        training_years = (Predicted_month - train_window['Date'].iloc[0]).days / 365.25\n",
        "        if training_years < min_years_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training period ({training_years:.2f} years). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last date in training window\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks\n",
        "        if use_regime_split:\n",
        "            # Check each regime has enough data\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  🔴 Regime split active. Insufficient data in: {regime_str}. Skipping.\\n\")\n",
        "                continue\n",
        "\n",
        "            # Use only training data for the current regime\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  🔴 Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\\n\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Check last training date < test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Fit RandomForest\n",
        "        rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict using the last row in training window (previous month’s features)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\\n\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "        predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "        # Map probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "            try:\n",
        "                idx = FACTORS.index(cls)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass  # or print a warning\n",
        "\n",
        "        # Use the test_row's factor returns to compute the allocated return\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "        # Gather results\n",
        "        tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "        avg_depth = np.mean(tree_depths)\n",
        "        max_depth = np.max(tree_depths)\n",
        "\n",
        "        # Calculate how many months ahead the prediction is\n",
        "        months_ahead = (\n",
        "            (Predicted_month.year - last_train_date.year) * 12 +\n",
        "            (Predicted_month.month - last_train_date.month)\n",
        "        )\n",
        "\n",
        "        # Store the actual feature levels used in X_test\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        # Print the training window used for this test row\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        # Build the result row\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for RF\n",
        "    # -------------------\n",
        "    results_df_rf = pd.DataFrame(results)\n",
        "    print(\"Final results_df_rf columns:\", results_df_rf.columns.tolist())\n",
        "\n",
        "    # Show the last few rows\n",
        "    display(results_df_rf.tail(10))\n"
      ],
      "metadata": {
        "id": "YjHj_tiKSCYZ"
      },
      "id": "YjHj_tiKSCYZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient boosting\n"
      ],
      "metadata": {
        "id": "MSWv9xFlDbMz"
      },
      "id": "MSWv9xFlDbMz"
    },
    {
      "cell_type": "code",
      "source": [
        "if GB or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_years_train = 5      # Minimum years of data needed\n",
        "    min_obs_regime = 50      # Minimum observations per regime (if using regime split)\n",
        "    min_obs_train = 0        # Minimum observations after dropping NAs\n",
        "    use_regime_split = False # Toggle regime-based training or not\n",
        "\n",
        "    # Toggle for training window type:\n",
        "    use_fixed_window = True  # True for fixed (rolling) window; False for expanding window\n",
        "    rolling_window_size = 120  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either fixed-size (rolling) or expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check training years available\n",
        "        training_years = (Predicted_month - train_window['Date'].iloc[0]).days / 365.25\n",
        "        if training_years < min_years_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training period ({training_years:.2f} years). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last training dates\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks (if enabled)\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active. Insufficient data in: {regime_str}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Use only training data for the current regime\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure the last training date is strictly before the test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Convert y_train from strings to numeric codes and save mapping\n",
        "        y_train_cat = y_train.astype('category')\n",
        "        mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "        y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "        # Fit XGBoost gradient boosting classifier on numeric labels (full training, no early stopping)\n",
        "        xgb_model = XGBClassifier(n_estimators=100,\n",
        "                                  random_state=42,\n",
        "                                  eval_metric='mlogloss')\n",
        "        xgb_model.fit(X_train, y_train_numeric)\n",
        "\n",
        "        # Prepare test data (using the last row in the training window)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = xgb_model.predict_proba(X_test)[0]\n",
        "        # Get predicted numeric class and convert back to original factor name\n",
        "        predicted_numeric = xgb_model.classes_[predicted_probabilities.argmax()]\n",
        "        predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "        # Map predicted probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for code, prob in zip(xgb_model.classes_, predicted_probabilities):\n",
        "            factor_name = mapping[code]\n",
        "            try:\n",
        "                idx = FACTORS.index(factor_name)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass  # Skip if factor not found in FACTORS\n",
        "\n",
        "        # Compute allocated return using the test row's factor returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "        # Tree depth statistics are not required, so we set them to None\n",
        "        avg_depth = None\n",
        "        max_depth = None\n",
        "\n",
        "        # Calculate prediction horizon (months ahead)\n",
        "        months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month))\n",
        "\n",
        "        # Store the actual feature levels used in X_test\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        # Build the result dictionary for this iteration\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': xgb_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': xgb_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for GB\n",
        "    # -------------------\n",
        "    results_df_gb = pd.DataFrame(results)\n",
        "    print(\"Final results_df_gb columns:\", results_df_gb.columns.tolist())\n",
        "    display(results_df_gb.tail(10))\n"
      ],
      "metadata": {
        "id": "9sWAd_BnDVlB"
      },
      "id": "9sWAd_BnDVlB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid / tää säilyttää random forestin dataframen mut averagee painot ja laskee allocated returns nistä"
      ],
      "metadata": {
        "id": "0HZuBVFOW8qu"
      },
      "id": "0HZuBVFOW8qu"
    },
    {
      "cell_type": "code",
      "source": [
        "if Hybrid:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # Ensure that RF and GB results exist before merging\n",
        "    if 'results_df_rf' in locals() and 'results_df_gb' in locals():\n",
        "        # Create temporary DataFrames containing only the predicted weights from RF and GB\n",
        "        rf_subset = results_df_rf[['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "        gb_subset = results_df_gb[['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "\n",
        "        # Merge on Predicted_month\n",
        "        hybrid_temp = pd.merge(\n",
        "            rf_subset,\n",
        "            gb_subset,\n",
        "            on='Predicted_month',\n",
        "            suffixes=('_rf', '_gb')\n",
        "        )\n",
        "\n",
        "        # Compute the average predicted probabilities (ensemble)\n",
        "        hybrid_temp['Predicted_Probabilities'] = hybrid_temp.apply(\n",
        "            lambda row: (np.array(row['Predicted_Probabilities_rf']) + np.array(row['Predicted_Probabilities_gb'])) / 2,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Function to compute allocated return given a probability vector and factor returns\n",
        "        def compute_allocated_return(prob_vector, factor_returns):\n",
        "            return np.dot(prob_vector, factor_returns)\n",
        "\n",
        "        # Compute the allocated return for each row using the hybrid weights\n",
        "        hybrid_temp['Hybrid_Allocated_Return'] = hybrid_temp.apply(\n",
        "            lambda row: compute_allocated_return(\n",
        "                row['Predicted_Probabilities'],\n",
        "                df.loc[df['Date'] == row['Predicted_month'], FACTORS].values.flatten()\n",
        "            ) if not df.loc[df['Date'] == row['Predicted_month'], FACTORS].empty else np.nan,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Now merge the new hybrid values back into the full RF results so we keep all the RF columns\n",
        "        hybrid_df = results_df_rf.merge(\n",
        "            hybrid_temp[['Predicted_month', 'Predicted_Probabilities', 'Hybrid_Allocated_Return']],\n",
        "            on='Predicted_month',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Override the RF predicted probabilities and allocated return with the hybrid ones\n",
        "        # (the merge will create 'Predicted_Probabilities_x' from RF and 'Predicted_Probabilities_y' from hybrid_temp)\n",
        "        hybrid_df['Predicted_Probabilities'] = hybrid_df['Predicted_Probabilities_y']\n",
        "        hybrid_df['Allocated_Return'] = hybrid_df['Hybrid_Allocated_Return']\n",
        "\n",
        "        # Drop the temporary columns from the merge\n",
        "        hybrid_df.drop(columns=['Predicted_Probabilities_x', 'Predicted_Probabilities_y', 'Hybrid_Allocated_Return'], inplace=True)\n",
        "\n",
        "        print(\"Hybrid hybrid_df created:\")\n",
        "        display(hybrid_df.head())\n",
        "    else:\n",
        "        raise ValueError(\"Error: Both Random Forest and Gradient Boosting models must be run before Hybrid mode can be computed.\")\n"
      ],
      "metadata": {
        "id": "JKYtkk2MWPC0"
      },
      "id": "JKYtkk2MWPC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Convert results_df / Modelista riippumatta muutetaan results_df"
      ],
      "metadata": {
        "id": "A1BqWL_QXA8e"
      },
      "id": "A1BqWL_QXA8e"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF:\n",
        "    results_df = results_df_rf.copy()\n",
        "    print(\"Results from Random Forest assigned to results_df.\")\n",
        "\n",
        "elif GB:\n",
        "    results_df = results_df_gb.copy()\n",
        "    print(\"Results from Gradient Boosting assigned to results_df.\")\n",
        "\n",
        "elif Hybrid:\n",
        "    results_df = hybrid_df.copy()\n",
        "    print(\"Results from Hybrid Model assigned to results_df.\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Error: No valid model was selected. Ensure one of [RF, GB, Hybrid] is True.\")\n",
        "\n",
        "# Display the first few rows of the final results_df\n",
        "display(results_df.head())\n"
      ],
      "metadata": {
        "id": "FOzCGJrxXKUm"
      },
      "id": "FOzCGJrxXKUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Increase column width so no text is truncated\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# 1) Convert 'Predicted_month' to datetime if not already\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "\n",
        "# 2) Define the date range\n",
        "start_date = pd.to_datetime('1968-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# 3) Filter the DataFrame\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "].copy().sort_values('Predicted_month')\n",
        "\n",
        "# 4) Display the table with full column text\n",
        "display(filtered_results_df)\n",
        "\n",
        "# (Optional) Reset column width to default after display\n",
        "pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "id": "JmpzyRpGWP0n"
      },
      "id": "JmpzyRpGWP0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d0f495-1290-403b-86a2-7bd1c2c12814",
      "metadata": {
        "tags": [],
        "id": "88d0f495-1290-403b-86a2-7bd1c2c12814"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ----- Part A: Confusion Matrix & Overall Performance Metrics -----\n",
        "\n",
        "# Use FACTORS directly as labels\n",
        "labels = FACTORS\n",
        "\n",
        "# Extract actual and predicted winners\n",
        "all_true = results_df['Actual_Winner']\n",
        "all_pred = results_df['Predicted_Winner']\n",
        "\n",
        "# Get unique regimes if regime split is active\n",
        "if use_regime_split:\n",
        "    all_regimes = results_df['Regime'].unique()\n",
        "else:\n",
        "    all_regimes = []\n",
        "\n",
        "# Set number of subplots: one overall plus one per regime if needed\n",
        "num_cols = 1 if not use_regime_split else len(all_regimes) + 1\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_cols, figsize=(8 if num_cols == 1 else 19.5, 8))\n",
        "if num_cols == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# 1. Overall confusion matrix\n",
        "cm_total = confusion_matrix(all_true, all_pred, labels=labels)\n",
        "sns.heatmap(cm_total, annot=True, fmt='d', cmap=\"Blues\",\n",
        "            xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
        "axes[0].set_xlabel(\"Predicted Labels\")\n",
        "axes[0].set_ylabel(\"True Labels\")\n",
        "total_samples = len(all_true)\n",
        "axes[0].set_title(f\"Overall Confusion Matrix\\n({total_samples} samples)\")\n",
        "\n",
        "# Prepare a list for metrics summary\n",
        "metrics_summary = []\n",
        "\n",
        "# Overall metrics\n",
        "overall_metrics = {\n",
        "    \"Regime\": \"Overall\",\n",
        "    \"Accuracy\": accuracy_score(all_true, all_pred),\n",
        "    \"Precision\": precision_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"Recall\": recall_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"F1 Score\": f1_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"Samples\": total_samples\n",
        "}\n",
        "metrics_summary.append(overall_metrics)\n",
        "\n",
        "# 2. Per-regime confusion matrices and metrics (if regime split is active)\n",
        "if use_regime_split:\n",
        "    for i, regime in enumerate(all_regimes):\n",
        "        regime_mask = (results_df['Regime'] == regime)\n",
        "        regime_true = all_true[regime_mask]\n",
        "        regime_pred = all_pred[regime_mask]\n",
        "\n",
        "        if len(regime_true) == 0:\n",
        "            print(f\"\\nNo samples for regime '{regime}'. Skipping confusion matrix.\")\n",
        "            continue\n",
        "\n",
        "        cm_regime = confusion_matrix(regime_true, regime_pred, labels=labels)\n",
        "        sns.heatmap(cm_regime, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                    xticklabels=labels, yticklabels=labels, ax=axes[i+1])\n",
        "        axes[i+1].set_xlabel(\"Predicted Labels\")\n",
        "        axes[i+1].set_ylabel(\"True Labels\")\n",
        "        axes[i+1].set_title(f\"{regime} Regime\\n({len(regime_true)} samples)\")\n",
        "\n",
        "        regime_metrics = {\n",
        "            \"Regime\": regime,\n",
        "            \"Accuracy\": accuracy_score(regime_true, regime_pred),\n",
        "            \"Precision\": precision_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"Recall\": recall_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"F1 Score\": f1_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"Samples\": len(regime_true)\n",
        "        }\n",
        "        metrics_summary.append(regime_metrics)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Convert overall metrics summary to DataFrame and round values\n",
        "metrics_df = pd.DataFrame(metrics_summary)\n",
        "metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']] = metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']].round(4)\n",
        "\n",
        "# Build HTML table for overall performance metrics\n",
        "html_overall = f\"\"\"\n",
        "<h3>Overall Performance Metrics Summary</h3>\n",
        "<table border=\"1\" cellpadding=\"5\">\n",
        "    <tr>\n",
        "        <th>Regime</th>\n",
        "        <th>Accuracy</th>\n",
        "        <th>Precision</th>\n",
        "        <th>Recall</th>\n",
        "        <th>F1 Score</th>\n",
        "        <th>Samples</th>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "for _, row in metrics_df.iterrows():\n",
        "    html_overall += f\"\"\"\n",
        "    <tr>\n",
        "        <td>{row['Regime']}</td>\n",
        "        <td>{row['Accuracy']}</td>\n",
        "        <td>{row['Precision']}</td>\n",
        "        <td>{row['Recall']}</td>\n",
        "        <td>{row['F1 Score']}</td>\n",
        "        <td>{row['Samples']}</td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "html_overall += \"</table>\"\n",
        "\n",
        "explanations = {\n",
        "    \"Accuracy\": \"Proportion of correct predictions.\",\n",
        "    \"Precision\": \"How many predicted positives were actually correct?\",\n",
        "    \"Recall\": \"How many actual positives were correctly predicted?\",\n",
        "    \"F1 Score\": \"Harmonic mean of precision & recall.\",\n",
        "    \"Samples\": \"Number of test samples in this regime.\"\n",
        "}\n",
        "html_overall += \"<h4>Metric Explanations:</h4><ul>\"\n",
        "for metric, desc in explanations.items():\n",
        "    html_overall += f\"<li><strong>{metric}:</strong> {desc}</li>\"\n",
        "html_overall += \"</ul>\"\n",
        "\n",
        "display(HTML(html_overall))\n",
        "\n",
        "\n",
        "# ----- Part B: Factor-Level Accuracy from Overall Confusion Matrix -----\n",
        "factor_data = []\n",
        "for i, factor in enumerate(labels):\n",
        "    predicted_count = cm_total[:, i].sum()  # sum of column i\n",
        "    correct_count = cm_total[i, i]         # diagonal entry\n",
        "    if predicted_count > 0:\n",
        "        factor_accuracy = correct_count / predicted_count * 100\n",
        "    else:\n",
        "        factor_accuracy = 0.0\n",
        "\n",
        "    factor_data.append({\n",
        "        \"Factor\": factor,\n",
        "        \"Predicted Count\": int(predicted_count),\n",
        "        \"Correct Count\": int(correct_count),\n",
        "        \"Factor Accuracy (%)\": round(factor_accuracy, 2)\n",
        "    })\n",
        "\n",
        "factor_df = pd.DataFrame(factor_data)\n",
        "\n",
        "html_factor = f\"\"\"\n",
        "<h3>Factor-Level Accuracy (From Overall Confusion Matrix)</h3>\n",
        "<table border=\"1\" cellpadding=\"5\">\n",
        "    <tr>\n",
        "        <th>Factor</th>\n",
        "        <th>Predicted Count</th>\n",
        "        <th>Correct Count</th>\n",
        "        <th>Factor Accuracy (%)</th>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "for _, row in factor_df.iterrows():\n",
        "    html_factor += f\"\"\"\n",
        "    <tr>\n",
        "        <td>{row['Factor']}</td>\n",
        "        <td>{row['Predicted Count']}</td>\n",
        "        <td>{row['Correct Count']}</td>\n",
        "        <td>{row['Factor Accuracy (%)']}%</td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "html_factor += \"</table>\"\n",
        "\n",
        "html_factor += \"\"\"\n",
        "<h4>Definition</h4>\n",
        "<ul>\n",
        "  <li><strong>Predicted Count:</strong> Sum of the corresponding column in the confusion matrix (times predicted this factor).</li>\n",
        "  <li><strong>Correct Count:</strong> Diagonal entry for this factor in the matrix (times predicted factor = actual factor).</li>\n",
        "  <li><strong>Factor Accuracy:</strong> (Correct Count / Predicted Count) * 100.</li>\n",
        "</ul>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_factor))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance"
      ],
      "metadata": {
        "id": "-U5ovbRVPoPM"
      },
      "id": "-U5ovbRVPoPM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5",
      "metadata": {
        "tags": [],
        "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5"
      },
      "outputs": [],
      "source": [
        "# --- Code cell 26 ---\n",
        "# 2. Regime-Specific Feature Importances (Dynamic Version)\n",
        "# ========================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the unique regimes from results_df (already converted to string names)\n",
        "existing_regimes = results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "\n",
        "# Dynamically get the number of features from the data\n",
        "n_features = len(results_df['Feature_Importances'].iloc[0])  # Actual feature count\n",
        "\n",
        "# Robust feature name handling:\n",
        "try:\n",
        "    # Validate FEATURES list length matches actual features\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"⚠️ Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features})\")\n",
        "        print(\"Using auto-generated feature names instead\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate descriptive feature names if there's a mismatch or error\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features\")\n",
        "\n",
        "# Compute overall average feature importances across all predictions\n",
        "overall_avg_fi = np.vstack(results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# Calculate regime-specific average feature importances\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = results_df[results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# Sort features by overall importance in descending order\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # bounds check\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# 1) If we have more than one unique regime, make an overall + multiple regime subplots\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One for overall, one per regime\n",
        "    # Figure height depends on number of plots and number of features\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]  # ensure iterable\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Plots ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 2) Otherwise, if there's only zero or one regime, show only the overall chart\n",
        "else:\n",
        "    total_plots = 1\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Provide extra space on the left for feature labels\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Allocation chart"
      ],
      "metadata": {
        "id": "bhRxmOoFPtEI"
      },
      "id": "bhRxmOoFPtEI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8",
      "metadata": {
        "tags": [],
        "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Convert \"Predicted_month\" to datetime (assuming the format is '%Y-%m'):\n",
        "probability_dates = pd.to_datetime(\n",
        "    results_df[\"Predicted_month\"],\n",
        "    format='%Y-%m'\n",
        ")\n",
        "\n",
        "# 2) Drop rows with unparseable dates if needed\n",
        "if probability_dates.isna().any():\n",
        "    print(\"Warning: Some dates could not be parsed. Dropping those rows.\")\n",
        "    results_df = results_df.loc[~probability_dates.isna()].copy()\n",
        "    probability_dates = probability_dates.dropna()\n",
        "\n",
        "# 3) Extract the probability arrays and build a DataFrame\n",
        "full_probs = np.vstack(results_df[\"Predicted_Probabilities\"].values)\n",
        "probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "probability_df[\"Date\"] = probability_dates\n",
        "probability_df = probability_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# 4) Check date range\n",
        "print(\"Date Range:\", probability_df[\"Date\"].min(), \"to\", probability_df[\"Date\"].max())\n",
        "\n",
        "# 5) Plot the probabilities in a stack plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.stackplot(\n",
        "    probability_df[\"Date\"],\n",
        "    [probability_df[col] for col in FACTORS],\n",
        "    labels=FACTORS,\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "plt.title(\"Outperforming Probabilities of Each Factor (ML Prediction)\", fontsize=14)\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.ylabel(\"Probability\", fontsize=12)\n",
        "\n",
        "# Manually set y-axis ticks at 0, 0.25, 0.5, 0.75, 1\n",
        "plt.ylim(0, 1)\n",
        "plt.yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "\n",
        "plt.legend(loc='upper left', fontsize='small', frameon=False)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Factor weight analysis"
      ],
      "metadata": {
        "id": "PY247n9MPyWG"
      },
      "id": "PY247n9MPyWG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the date range for viewing\n",
        "start_date = \"1968-07-30\"\n",
        "end_date = \"2024-11-30\"\n",
        "\n",
        "# Ensure that the \"Date\" column is in datetime format if it's not already\n",
        "probability_df[\"Date\"] = pd.to_datetime(probability_df[\"Date\"])\n",
        "\n",
        "# Filter the dataframe to include only the rows between the set dates\n",
        "mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "filtered_df = probability_df.loc[mask]\n",
        "\n",
        "# Define the equal weight value (static equal weight for each factor)\n",
        "equal_weight = 1 / len(FACTORS)  # For example, if 5 factors then equal_weight = 0.20\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loop over each factor in FACTORS and create a separate chart\n",
        "for factor in FACTORS:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot the predicted probability for the current factor using the filtered data\n",
        "    plt.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "             label=f\"{factor} Predicted Probability\", color='blue', linewidth=2)\n",
        "\n",
        "    # Plot the static equal weight line\n",
        "    plt.axhline(equal_weight, color='black', linestyle='--',\n",
        "                label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "    # Shade the area where the predicted probability is above the equal weight (Overweight)\n",
        "    plt.fill_between(filtered_df[\"Date\"],\n",
        "                     filtered_df[factor],\n",
        "                     equal_weight,\n",
        "                     where=(filtered_df[factor] > equal_weight),\n",
        "                     interpolate=True, color='green', alpha=0.3, label='Overweight')\n",
        "\n",
        "    # Shade the area where the predicted probability is below the equal weight (Underweight)\n",
        "    plt.fill_between(filtered_df[\"Date\"],\n",
        "                     filtered_df[factor],\n",
        "                     equal_weight,\n",
        "                     where=(filtered_df[factor] < equal_weight),\n",
        "                     interpolate=True, color='red', alpha=0.3, label='Underweight')\n",
        "\n",
        "    # Set chart title and labels\n",
        "    plt.title(f\"Over/Under Weight for {factor}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5My7U6ny7GAJ"
      },
      "id": "5My7U6ny7GAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Total outperforming probabilities"
      ],
      "metadata": {
        "id": "6wkhRaZrP6aF"
      },
      "id": "6wkhRaZrP6aF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "avg_probs = probability_df[FACTORS].mean()\n",
        "avg_probs_df = avg_probs.reset_index()\n",
        "avg_probs_df.columns = ['Factor', 'Average Probability']\n",
        "\n",
        "\n",
        "print(\"Average Outperforming Probabilities Over Time:\")\n",
        "print(avg_probs_df)"
      ],
      "metadata": {
        "id": "7GvhTSes34Ci"
      },
      "id": "7GvhTSes34Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Start & end dates\n"
      ],
      "metadata": {
        "id": "M0sRKlsm6Yja"
      },
      "id": "M0sRKlsm6Yja"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure the 'Predicted_month' column is in datetime format\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "\n",
        "# Get the earliest date from the 'Predicted_month' column\n",
        "first_date = results_df['Predicted_month'].min()\n",
        "last_date = results_df['Predicted_month'].max()\n",
        "\n",
        "print(\"First date Predicted_month column in results_df:\", first_date)\n",
        "print(\"Last date in Predicted_month column results_df:\", last_date)"
      ],
      "metadata": {
        "id": "jxygXPuZ4NYc"
      },
      "id": "jxygXPuZ4NYc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns"
      ],
      "metadata": {
        "id": "aIJm8h02Pc0y"
      },
      "id": "aIJm8h02Pc0y"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Create local copies so that the original DataFrames remain unchanged\n",
        "results_df_local = results_df.copy()\n",
        "df_local = df.copy()\n",
        "\n",
        "# -------------------------------------\n",
        "# 1. Standardize date columns to month-end (local copies only)\n",
        "# -------------------------------------\n",
        "results_df_local['Predicted_month'] = pd.to_datetime(results_df_local['Predicted_month']) + pd.offsets.MonthEnd(0)\n",
        "df_local['Date'] = pd.to_datetime(df_local['Date']) + pd.offsets.MonthEnd(0)\n",
        "\n",
        "# -------------------------------------\n",
        "# Define your dynamic date range (month-end)\n",
        "# -------------------------------------\n",
        "start_date = pd.to_datetime('2010-01-30') + pd.offsets.MonthEnd(0)\n",
        "end_date   = pd.to_datetime('2024-12-30') + pd.offsets.MonthEnd(0)\n",
        "\n",
        "# -------------------------------------\n",
        "# Part 1: Process results_df for ML Allocated Strategy\n",
        "# -------------------------------------\n",
        "filtered_df = results_df_local.loc[\n",
        "    (results_df_local['Predicted_month'] >= start_date) & (results_df_local['Predicted_month'] <= end_date),\n",
        "    ['Predicted_month', 'Allocated_Return']\n",
        "].sort_values('Predicted_month').copy()\n",
        "\n",
        "filtered_df = filtered_df.rename(columns={'Allocated_Return': 'ML Allocated Strategy Return'})\n",
        "filtered_df['ML Allocated Strategy Return Shifted'] = filtered_df['ML Allocated Strategy Return'].shift(1, fill_value=0)\n",
        "filtered_df['ML Cumulative Allocated Return'] = (1 + filtered_df['ML Allocated Strategy Return Shifted']).cumprod()\n",
        "\n",
        "# -------------------------------------\n",
        "# Part 2: Process df for Benchmark, Equal Factor Weight, and Theoretical Maximum\n",
        "# -------------------------------------\n",
        "df_filtered = df_local.loc[\n",
        "    (df_local['Date'] >= start_date) & (df_local['Date'] <= end_date)\n",
        "].copy().sort_values('Date')\n",
        "\n",
        "# Process Benchmark returns (assuming BENCHMARK is defined, e.g., BENCHMARK = ['Us_standard'])\n",
        "benchmark_col = BENCHMARK[0]\n",
        "df_filtered[f'{benchmark_col} Shifted'] = df_filtered[benchmark_col].shift(1, fill_value=0)\n",
        "df_filtered['Benchmark Cumulative Returns'] = (1 + df_filtered[f'{benchmark_col} Shifted']).cumprod()\n",
        "\n",
        "# Calculate the Equal Factor Weight Strategy:\n",
        "if 'RF' in FACTORS:\n",
        "    # Calculate including RF\n",
        "    df_filtered['Equal Factor Weight Strategy w/ RF'] = df_filtered[FACTORS].mean(axis=1)\n",
        "    df_filtered['Equal Factor Weight Strategy w/ RF Shifted'] = df_filtered['Equal Factor Weight Strategy w/ RF'].shift(1, fill_value=0)\n",
        "    df_filtered['Equal Factor Weight Cumulative Return w/ RF'] = (1 + df_filtered['Equal Factor Weight Strategy w/ RF Shifted']).cumprod()\n",
        "\n",
        "    # Calculate excluding RF\n",
        "    factors_ex_rf = [f for f in FACTORS if f != 'RF']\n",
        "    df_filtered['Equal Factor Weight Strategy w/o RF'] = df_filtered[factors_ex_rf].mean(axis=1)\n",
        "    df_filtered['Equal Factor Weight Strategy w/o RF Shifted'] = df_filtered['Equal Factor Weight Strategy w/o RF'].shift(1, fill_value=0)\n",
        "    df_filtered['Equal Factor Weight Cumulative Return w/o RF'] = (1 + df_filtered['Equal Factor Weight Strategy w/o RF Shifted']).cumprod()\n",
        "else:\n",
        "    df_filtered['Equal Factor Weight Strategy'] = df_filtered[FACTORS].mean(axis=1)\n",
        "    df_filtered['Equal Factor Weight Strategy Shifted'] = df_filtered['Equal Factor Weight Strategy'].shift(1, fill_value=0)\n",
        "    df_filtered['Equal Factor Weight Cumulative Return'] = (1 + df_filtered['Equal Factor Weight Strategy Shifted']).cumprod()\n",
        "\n",
        "# Calculate the Theoretical Maximum Strategy by choosing the maximum return among FACTORS\n",
        "df_filtered['Theoretical Max Strategy'] = df_filtered[FACTORS].max(axis=1)\n",
        "df_filtered['Theoretical Max Strategy Shifted'] = df_filtered['Theoretical Max Strategy'].shift(1, fill_value=0)\n",
        "df_filtered['Theoretical Max Cumulative Return'] = (1 + df_filtered['Theoretical Max Strategy Shifted']).cumprod()\n",
        "\n",
        "# -------------------------------------\n",
        "# Part 3: Rebase Benchmark, Equal Factor Weight, and Theoretical Maximum Cumulative Returns\n",
        "# -------------------------------------\n",
        "ml_start_date = filtered_df['Predicted_month'].min()\n",
        "\n",
        "start_value_benchmark = df_filtered.loc[df_filtered['Date'] == ml_start_date, 'Benchmark Cumulative Returns'].values[0]\n",
        "df_filtered['Benchmark Cumulative Returns Rebased'] = df_filtered['Benchmark Cumulative Returns'] / start_value_benchmark\n",
        "\n",
        "if 'RF' in FACTORS:\n",
        "    start_value_equal_rf = df_filtered.loc[df_filtered['Date'] == ml_start_date, 'Equal Factor Weight Cumulative Return w/ RF'].values[0]\n",
        "    df_filtered['Equal Factor Weight Cumulative Return w/ RF Rebased'] = df_filtered['Equal Factor Weight Cumulative Return w/ RF'] / start_value_equal_rf\n",
        "\n",
        "    start_value_equal_no_rf = df_filtered.loc[df_filtered['Date'] == ml_start_date, 'Equal Factor Weight Cumulative Return w/o RF'].values[0]\n",
        "    df_filtered['Equal Factor Weight Cumulative Return w/o RF Rebased'] = df_filtered['Equal Factor Weight Cumulative Return w/o RF'] / start_value_equal_no_rf\n",
        "else:\n",
        "    start_value_equal = df_filtered.loc[df_filtered['Date'] == ml_start_date, 'Equal Factor Weight Cumulative Return'].values[0]\n",
        "    df_filtered['Equal Factor Weight Cumulative Return Rebased'] = df_filtered['Equal Factor Weight Cumulative Return'] / start_value_equal\n",
        "\n",
        "start_value_theoretical = df_filtered.loc[df_filtered['Date'] == ml_start_date, 'Theoretical Max Cumulative Return'].values[0]\n",
        "df_filtered['Theoretical Max Cumulative Return Rebased'] = df_filtered['Theoretical Max Cumulative Return'] / start_value_theoretical\n",
        "\n",
        "# -------------------------------------\n",
        "# Part 4: Merge the DataFrames and remove NaNs\n",
        "# -------------------------------------\n",
        "if 'RF' in FACTORS:\n",
        "    merged_df = pd.merge(\n",
        "        filtered_df,\n",
        "        df_filtered[['Date', 'Benchmark Cumulative Returns Rebased',\n",
        "                     'Equal Factor Weight Strategy w/ RF',\n",
        "                     'Equal Factor Weight Strategy w/o RF',\n",
        "                     'Theoretical Max Strategy',\n",
        "                     'Equal Factor Weight Cumulative Return w/ RF Rebased',\n",
        "                     'Equal Factor Weight Cumulative Return w/o RF Rebased',\n",
        "                     'Theoretical Max Cumulative Return Rebased']],\n",
        "        left_on='Predicted_month',\n",
        "        right_on='Date',\n",
        "        how='left'\n",
        "    )\n",
        "else:\n",
        "    merged_df = pd.merge(\n",
        "        filtered_df,\n",
        "        df_filtered[['Date', 'Benchmark Cumulative Returns Rebased',\n",
        "                     'Equal Factor Weight Strategy',\n",
        "                     'Theoretical Max Strategy',\n",
        "                     'Equal Factor Weight Cumulative Return Rebased',\n",
        "                     'Theoretical Max Cumulative Return Rebased']],\n",
        "        left_on='Predicted_month',\n",
        "        right_on='Date',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "merged_df.drop(columns=['Date'], inplace=True)\n",
        "merged_df = merged_df.dropna()\n",
        "\n",
        "# -------------------------------------\n",
        "# Split into Two Tables: Returns and Cumulative Returns\n",
        "# -------------------------------------\n",
        "if 'RF' in FACTORS:\n",
        "    returns_cols = ['Predicted_month', 'ML Allocated Strategy Return',\n",
        "                    'Equal Factor Weight Strategy w/ RF', 'Equal Factor Weight Strategy w/o RF',\n",
        "                    'Theoretical Max Strategy']\n",
        "    cumulative_cols = ['Predicted_month', 'ML Cumulative Allocated Return',\n",
        "                       'Benchmark Cumulative Returns Rebased',\n",
        "                       'Equal Factor Weight Cumulative Return w/ RF Rebased', 'Equal Factor Weight Cumulative Return w/o RF Rebased',\n",
        "                       'Theoretical Max Cumulative Return Rebased']\n",
        "else:\n",
        "    returns_cols = ['Predicted_month', 'ML Allocated Strategy Return',\n",
        "                    'Equal Factor Weight Strategy', 'Theoretical Max Strategy']\n",
        "    cumulative_cols = ['Predicted_month', 'ML Cumulative Allocated Return',\n",
        "                       'Benchmark Cumulative Returns Rebased',\n",
        "                       'Equal Factor Weight Cumulative Return Rebased', 'Theoretical Max Cumulative Return Rebased']\n",
        "\n",
        "returns_df = merged_df[returns_cols].copy()\n",
        "cumulative_df = merged_df[cumulative_cols].copy()\n",
        "\n",
        "# Round numeric columns to 3 decimals\n",
        "for df_temp in [returns_df, cumulative_df]:\n",
        "    numeric_cols = df_temp.select_dtypes(include=[np.number]).columns\n",
        "    df_temp[numeric_cols] = df_temp[numeric_cols].round(3)\n",
        "\n",
        "# -------------------------------------\n",
        "# Display Results in Two Separate Tables\n",
        "# -------------------------------------\n",
        "display(HTML(\"<h3>Returns Table</h3>\"))\n",
        "display(returns_df)\n",
        "\n",
        "display(HTML(\"<h3>Cumulative Returns Table</h3>\"))\n",
        "display(cumulative_df)\n"
      ],
      "metadata": {
        "id": "J-0WFHrz2c5F"
      },
      "id": "J-0WFHrz2c5F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns chart"
      ],
      "metadata": {
        "id": "W-uq2G9iPQwD"
      },
      "id": "W-uq2G9iPQwD"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "# Ensure the DataFrame is sorted by Predicted_month\n",
        "merged_df = merged_df.sort_values('Predicted_month')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot ML Cumulative Allocated Return\n",
        "plt.plot(\n",
        "    merged_df['Predicted_month'],\n",
        "    merged_df['ML Cumulative Allocated Return'],\n",
        "    label='ML Cumulative Allocated Return'\n",
        ")\n",
        "\n",
        "# Plot Equal Factor Weight Cumulative Return(s)\n",
        "if 'RF' in FACTORS:\n",
        "    # Plot the version including RF\n",
        "    plt.plot(\n",
        "        merged_df['Predicted_month'],\n",
        "        merged_df['Equal Factor Weight Cumulative Return w/ RF Rebased'],\n",
        "        label='Equal Factor Weight Cumulative Return (w/ RF)'\n",
        "    )\n",
        "    # Plot the version excluding RF\n",
        "    plt.plot(\n",
        "        merged_df['Predicted_month'],\n",
        "        merged_df['Equal Factor Weight Cumulative Return w/o RF Rebased'],\n",
        "        label='Equal Factor Weight Cumulative Return (w/o RF)'\n",
        "    )\n",
        "else:\n",
        "    plt.plot(\n",
        "        merged_df['Predicted_month'],\n",
        "        merged_df['Equal Factor Weight Cumulative Return Rebased'],\n",
        "        label='Equal Factor Weight Cumulative Return (Rebased)'\n",
        "    )\n",
        "\n",
        "# Only plot Benchmark if show_benchmark is True\n",
        "if show_benchmark:\n",
        "    plt.plot(\n",
        "        merged_df['Predicted_month'],\n",
        "        merged_df['Benchmark Cumulative Returns Rebased'],\n",
        "        label='Benchmark Cumulative Returns (Rebased)'\n",
        "    )\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.title('Cumulative Returns Comparison')\n",
        "\n",
        "# Set y-axis to log scale for better visualization\n",
        "plt.yscale('log')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wqfmAhDk3KBx"
      },
      "id": "wqfmAhDk3KBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##PERFORMANCE METRICS"
      ],
      "metadata": {
        "id": "-YjFAG1ePN-9"
      },
      "id": "-YjFAG1ePN-9"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# --- PERFORMANCE METRICS ---\n",
        "\n",
        "# For the ML Allocated Strategy:\n",
        "ml_returns = merged_df['ML Allocated Strategy Return']\n",
        "ml_mean_monthly = ml_returns.mean()\n",
        "ml_std_monthly  = ml_returns.std()\n",
        "ml_annualized_return = ml_mean_monthly * 12\n",
        "ml_annualized_vol    = ml_std_monthly * np.sqrt(12)\n",
        "ml_sharpe            = ml_annualized_return / ml_annualized_vol\n",
        "\n",
        "print(\"\\nPERFORMANCE METRICS:\\n\")\n",
        "print(\"ML Allocated Strategy:\")\n",
        "print(f\"  Annualized Return:      {ml_annualized_return:.2%}\")\n",
        "print(f\"  Annualized Volatility:  {ml_annualized_vol:.2%}\")\n",
        "print(f\"  Sharpe Ratio:           {ml_sharpe:.2f}\\n\")\n",
        "\n",
        "# For the Equal Factor Weight Strategy:\n",
        "if 'RF' in FACTORS:\n",
        "    # With RF\n",
        "    equal_returns_with_rf = merged_df['Equal Factor Weight Strategy w/ RF']\n",
        "    equal_mean_monthly_with_rf = equal_returns_with_rf.mean()\n",
        "    equal_std_monthly_with_rf  = equal_returns_with_rf.std()\n",
        "    equal_annualized_return_with_rf = equal_mean_monthly_with_rf * 12\n",
        "    equal_annualized_vol_with_rf    = equal_std_monthly_with_rf * np.sqrt(12)\n",
        "    equal_sharpe_with_rf            = equal_annualized_return_with_rf / equal_annualized_vol_with_rf\n",
        "\n",
        "    print(\"Equal Factor Weight Strategy (with RF):\")\n",
        "    print(f\"  Annualized Return:      {equal_annualized_return_with_rf:.2%}\")\n",
        "    print(f\"  Annualized Volatility:  {equal_annualized_vol_with_rf:.2%}\")\n",
        "    print(f\"  Sharpe Ratio:           {equal_sharpe_with_rf:.2f}\\n\")\n",
        "\n",
        "    # Without RF\n",
        "    equal_returns_without_rf = merged_df['Equal Factor Weight Strategy w/o RF']\n",
        "    equal_mean_monthly_without_rf = equal_returns_without_rf.mean()\n",
        "    equal_std_monthly_without_rf  = equal_returns_without_rf.std()\n",
        "    equal_annualized_return_without_rf = equal_mean_monthly_without_rf * 12\n",
        "    equal_annualized_vol_without_rf    = equal_std_monthly_without_rf * np.sqrt(12)\n",
        "    equal_sharpe_without_rf            = equal_annualized_return_without_rf / equal_annualized_vol_without_rf\n",
        "\n",
        "    print(\"Equal Factor Weight Strategy (without RF):\")\n",
        "    print(f\"  Annualized Return:      {equal_annualized_return_without_rf:.2%}\")\n",
        "    print(f\"  Annualized Volatility:  {equal_annualized_vol_without_rf:.2%}\")\n",
        "    print(f\"  Sharpe Ratio:           {equal_sharpe_without_rf:.2f}\\n\")\n",
        "else:\n",
        "    equal_returns = merged_df['Equal Factor Weight Strategy']\n",
        "    equal_mean_monthly = equal_returns.mean()\n",
        "    equal_std_monthly  = equal_returns.std()\n",
        "    equal_annualized_return = equal_mean_monthly * 12\n",
        "    equal_annualized_vol    = equal_std_monthly * np.sqrt(12)\n",
        "    equal_sharpe            = equal_annualized_return / equal_annualized_vol\n",
        "\n",
        "    print(\"Equal Factor Weight Strategy:\")\n",
        "    print(f\"  Annualized Return:      {equal_annualized_return:.2%}\")\n",
        "    print(f\"  Annualized Volatility:  {equal_annualized_vol:.2%}\")\n",
        "    print(f\"  Sharpe Ratio:           {equal_sharpe:.2f}\\n\")\n",
        "\n",
        "# --- MAXIMUM DRAWDOWN CALCULATIONS ---\n",
        "# Compute wealth indexes starting at 1 for each strategy.\n",
        "wealth_ml = (1 + merged_df['ML Allocated Strategy Return']).cumprod()\n",
        "\n",
        "if 'RF' in FACTORS:\n",
        "    wealth_equal_with_rf = (1 + merged_df['Equal Factor Weight Strategy w/ RF']).cumprod()\n",
        "    wealth_equal_without_rf = (1 + merged_df['Equal Factor Weight Strategy w/o RF']).cumprod()\n",
        "else:\n",
        "    wealth_equal = (1 + merged_df['Equal Factor Weight Strategy']).cumprod()\n",
        "\n",
        "# Compute drawdown series for each strategy:\n",
        "drawdown_ml = wealth_ml / wealth_ml.cummax() - 1\n",
        "\n",
        "if 'RF' in FACTORS:\n",
        "    drawdown_equal_with_rf = wealth_equal_with_rf / wealth_equal_with_rf.cummax() - 1\n",
        "    drawdown_equal_without_rf = wealth_equal_without_rf / wealth_equal_without_rf.cummax() - 1\n",
        "else:\n",
        "    drawdown_equal = wealth_equal / wealth_equal.cummax() - 1\n",
        "\n",
        "print(\"MAXIMUM DRAWDOWNS:\\n\")\n",
        "print(\"ML Allocated Strategy Max Drawdown:       \", f\"{drawdown_ml.min():.2%}\")\n",
        "if 'RF' in FACTORS:\n",
        "    print(\"Equal Factor Weight Strategy (with RF) Max Drawdown:  \", f\"{drawdown_equal_with_rf.min():.2%}\")\n",
        "    print(\"Equal Factor Weight Strategy (without RF) Max Drawdown:  \", f\"{drawdown_equal_without_rf.min():.2%}\")\n",
        "else:\n",
        "    print(\"Equal Factor Weight Strategy Max Drawdown:  \", f\"{drawdown_equal.min():.2%}\")\n"
      ],
      "metadata": {
        "id": "1o0tq56K64bL"
      },
      "id": "1o0tq56K64bL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drawdown chart"
      ],
      "metadata": {
        "id": "YXD3cJKFO_sE"
      },
      "id": "YXD3cJKFO_sE"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# --- Toggle Options (set these before any calculations) ---\n",
        "show_benchmark_drawdown = True\n",
        "\n",
        "# For Equal Factor Weight Drawdown toggles:\n",
        "if 'RF' in FACTORS:\n",
        "    show_equal_weight_drawdown_rf = True    # Toggle for Equal Weight drawdown (with RF)\n",
        "    show_equal_weight_drawdown_no_rf = True   # Toggle for Equal Weight drawdown (without RF)\n",
        "else:\n",
        "    show_equal_weight_drawdown = True         # Toggle for Equal Weight drawdown\n",
        "\n",
        "# --- Compute Drawdowns for Each Strategy ---\n",
        "# ML Allocated Strategy Drawdown (using the ML Cumulative Allocated Return)\n",
        "merged_df['ML Drawdown'] = merged_df['ML Cumulative Allocated Return'] / merged_df['ML Cumulative Allocated Return'].cummax() - 1\n",
        "\n",
        "# Benchmark Drawdown (using the Benchmark Cumulative Returns Rebased)\n",
        "merged_df['Benchmark Drawdown'] = merged_df['Benchmark Cumulative Returns Rebased'] / merged_df['Benchmark Cumulative Returns Rebased'].cummax() - 1\n",
        "\n",
        "if 'RF' in FACTORS:\n",
        "    # Equal Factor Weight Drawdowns for strategies with and without RF:\n",
        "    merged_df['Equal Weight Drawdown w/ RF'] = merged_df['Equal Factor Weight Cumulative Return w/ RF Rebased'] / \\\n",
        "        merged_df['Equal Factor Weight Cumulative Return w/ RF Rebased'].cummax() - 1\n",
        "    merged_df['Equal Weight Drawdown w/o RF'] = merged_df['Equal Factor Weight Cumulative Return w/o RF Rebased'] / \\\n",
        "        merged_df['Equal Factor Weight Cumulative Return w/o RF Rebased'].cummax() - 1\n",
        "else:\n",
        "    merged_df['Equal Weight Drawdown'] = merged_df['Equal Factor Weight Cumulative Return Rebased'] / \\\n",
        "        merged_df['Equal Factor Weight Cumulative Return Rebased'].cummax() - 1\n",
        "\n",
        "# --- Define the date range for plotting ---\n",
        "start_date = pd.to_datetime('2000-01-01')  # change as needed\n",
        "end_date = pd.to_datetime('2024-12-31')      # change as needed\n",
        "\n",
        "# Filter the DataFrame for the specified timeframe\n",
        "plot_df = merged_df[(merged_df['Predicted_month'] >= start_date) & (merged_df['Predicted_month'] <= end_date)]\n",
        "\n",
        "# --- Plot the Drawdowns ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# ML drawdown is always plotted.\n",
        "plt.plot(plot_df['Predicted_month'], plot_df['ML Drawdown'], label='ML Drawdown')\n",
        "\n",
        "if show_benchmark_drawdown:\n",
        "    plt.plot(plot_df['Predicted_month'], plot_df['Benchmark Drawdown'], label='Benchmark Drawdown')\n",
        "\n",
        "if 'RF' in FACTORS:\n",
        "    if show_equal_weight_drawdown_rf:\n",
        "        plt.plot(plot_df['Predicted_month'], plot_df['Equal Weight Drawdown w/ RF'], label='Equal Weight Drawdown (w/ RF)')\n",
        "    if show_equal_weight_drawdown_no_rf:\n",
        "        plt.plot(plot_df['Predicted_month'], plot_df['Equal Weight Drawdown w/o RF'], label='Equal Weight Drawdown (w/o RF)')\n",
        "else:\n",
        "    if show_equal_weight_drawdown:\n",
        "        plt.plot(plot_df['Predicted_month'], plot_df['Equal Weight Drawdown'], label='Equal Weight Drawdown')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Drawdown')\n",
        "plt.title('Drawdowns of Strategies')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FK-XFzpv4SXo"
      },
      "id": "FK-XFzpv4SXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ----------------------------\n",
        "# 1. Load the FF5 data and rename columns\n",
        "# ----------------------------\n",
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "df_factors = xls_file.parse(\"FF5\")\n",
        "\n",
        "# Expected order: Date, Mkt-RF, SMB, HML, RMW, CMA, RF\n",
        "# Adjust these names if your Excel sheet uses different labels\n",
        "df_factors.columns = [\"Date\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]\n",
        "\n",
        "# Convert commas to dots, convert to float, and divide by 100\n",
        "for col in [\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]:\n",
        "    df_factors[col] = df_factors[col].astype(str).str.replace(\",\", \".\")\n",
        "    df_factors[col] = pd.to_numeric(df_factors[col], errors=\"coerce\") / 100.0\n",
        "\n",
        "# Convert the Date column to datetime\n",
        "df_factors[\"Date\"] = pd.to_datetime(df_factors[\"Date\"])\n",
        "\n",
        "# ----------------------------\n",
        "# 2. Merge FF5 data into merged_df\n",
        "# ----------------------------\n",
        "# Ensure merged_df['Predicted_month'] is datetime (and ideally at month-end)\n",
        "# For a clean merge, both date columns should have the same frequency/format.\n",
        "merged_df = merged_df.merge(\n",
        "    df_factors,\n",
        "    left_on=\"Predicted_month\",\n",
        "    right_on=\"Date\",\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# Drop the duplicate Date column from the merge\n",
        "merged_df.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "# ----------------------------\n",
        "# 3. Check that the RF column is present\n",
        "# ----------------------------\n",
        "print(\"Merged DataFrame columns:\", merged_df.columns)\n",
        "\n",
        "if \"RF\" not in merged_df.columns:\n",
        "    # Try to find a column that might be the risk-free rate (case-insensitive)\n",
        "    for col in merged_df.columns:\n",
        "        if col.lower().strip() == \"rf\":\n",
        "            merged_df.rename(columns={col: \"RF\"}, inplace=True)\n",
        "            print(f\"Renamed column {col} to 'RF'\")\n",
        "            break\n",
        "    if \"RF\" not in merged_df.columns:\n",
        "        raise KeyError(\"Risk-free rate column 'RF' not found in merged_df. \"\n",
        "                       \"Please verify your FF5 sheet and column names.\")\n",
        "\n",
        "# ----------------------------\n",
        "# 4. Create Excess Returns (subtract RF from strategy returns)\n",
        "# ----------------------------\n",
        "merged_df[\"ML Excess Return\"] = merged_df[\"ML Allocated Strategy Return\"] - merged_df[\"RF\"]\n",
        "merged_df[\"Equal Factor Weight Excess Return\"] = merged_df[\"Equal Factor Weight Strategy\"] - merged_df[\"RF\"]\n",
        "\n",
        "# ----------------------------\n",
        "# 5. Prepare the regression data for ML Allocated Strategy\n",
        "# ----------------------------\n",
        "X_ml = merged_df[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
        "X_ml = sm.add_constant(X_ml)\n",
        "y_ml = merged_df[\"ML Excess Return\"]\n",
        "\n",
        "# Remove any rows with NaN or infinite values\n",
        "df_reg_ml = pd.concat([y_ml, X_ml], axis=1)\n",
        "df_reg_ml.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_reg_ml.dropna(inplace=True)\n",
        "y_ml_clean = df_reg_ml[\"ML Excess Return\"]\n",
        "X_ml_clean = df_reg_ml.drop(columns=[\"ML Excess Return\"])\n",
        "\n",
        "model_ml = sm.OLS(y_ml_clean, X_ml_clean).fit()\n",
        "print(\"Regression Results: ML Allocated Strategy (Excess) vs FF5 Factors\")\n",
        "print(model_ml.summary())\n",
        "\n",
        "# ----------------------------\n",
        "# 6. Prepare the regression data for Equal Factor Weight Strategy\n",
        "# ----------------------------\n",
        "X_eq = merged_df[[\"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\"]]\n",
        "X_eq = sm.add_constant(X_eq)\n",
        "y_eq = merged_df[\"Equal Factor Weight Excess Return\"]\n",
        "\n",
        "df_reg_eq = pd.concat([y_eq, X_eq], axis=1)\n",
        "df_reg_eq.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df_reg_eq.dropna(inplace=True)\n",
        "y_eq_clean = df_reg_eq[\"Equal Factor Weight Excess Return\"]\n",
        "X_eq_clean = df_reg_eq.drop(columns=[\"Equal Factor Weight Excess Return\"])\n",
        "\n",
        "model_eq = sm.OLS(y_eq_clean, X_eq_clean).fit()\n",
        "print(\"\\nRegression Results: Equal Factor Weight Strategy (Excess) vs FF5 Factors\")\n",
        "print(model_eq.summary())\n",
        "\n",
        "# ----------------------------\n",
        "# 7. Annualize alpha and report t-stat and p-value\n",
        "# ----------------------------\n",
        "alpha_ml = model_ml.params[\"const\"]\n",
        "annualized_alpha_ml = alpha_ml * 12\n",
        "t_alpha_ml = model_ml.tvalues[\"const\"]\n",
        "p_alpha_ml = model_ml.pvalues[\"const\"]\n",
        "\n",
        "alpha_eq = model_eq.params[\"const\"]\n",
        "annualized_alpha_eq = alpha_eq * 12\n",
        "t_alpha_eq = model_eq.tvalues[\"const\"]\n",
        "p_alpha_eq = model_eq.pvalues[\"const\"]\n",
        "\n",
        "print(\"\\nAnnualized Alpha Metrics:\")\n",
        "print(\"ML Allocated Strategy:\")\n",
        "print(f\"  Annualized Alpha: {annualized_alpha_ml:.2%}\")\n",
        "print(f\"  T-statistic:      {t_alpha_ml:.2f}\")\n",
        "print(f\"  P-value:          {p_alpha_ml:.4f}\")\n",
        "\n",
        "print(\"\\nEqual Factor Weight Strategy:\")\n",
        "print(f\"  Annualized Alpha: {annualized_alpha_eq:.2%}\")\n",
        "print(f\"  T-statistic:      {t_alpha_eq:.2f}\")\n",
        "print(f\"  P-value:          {p_alpha_eq:.4f}\")"
      ],
      "metadata": {
        "id": "MOb-rnOzH_4W"
      },
      "id": "MOb-rnOzH_4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Create a Year column from the 'Predicted_month'\n",
        "merged_df['Year'] = merged_df['Predicted_month'].dt.year\n",
        "\n",
        "def annual_sharpe(group, col):\n",
        "    \"\"\"\n",
        "    Calculate the annual Sharpe ratio for a given return column within a group (year).\n",
        "    Annual Return is computed as the product of (1 + monthly return) minus 1.\n",
        "    Annual Volatility is computed as the standard deviation of monthly returns * sqrt(12).\n",
        "    The Sharpe ratio is then annual return divided by annual volatility.\n",
        "    \"\"\"\n",
        "    # Compound monthly returns to get the annual return:\n",
        "    annual_return = (1 + group[col]).prod() - 1\n",
        "    # Annualize the monthly volatility:\n",
        "    annual_vol = group[col].std() * np.sqrt(12)\n",
        "    # Compute Sharpe ratio (assume risk-free rate = 0)\n",
        "    sharpe = annual_return / annual_vol if annual_vol != 0 else np.nan\n",
        "    return sharpe\n",
        "\n",
        "# Calculate annual Sharpe ratios for each strategy by grouping on 'Year'\n",
        "sharpe_ml = merged_df.groupby('Year', group_keys=False).apply(lambda grp: annual_sharpe(grp, 'ML Allocated Strategy Return'))\n",
        "sharpe_bench = merged_df.groupby('Year', group_keys=False).apply(lambda grp: annual_sharpe(grp, 'Mkt-RF'))\n",
        "sharpe_equal = merged_df.groupby('Year', group_keys=False).apply(lambda grp: annual_sharpe(grp, 'Equal Factor Weight Strategy'))\n",
        "\n",
        "# Combine the Sharpe ratios into one DataFrame\n",
        "annual_sharpe_table = pd.DataFrame({\n",
        "    'ML Sharpe Ratio': sharpe_ml,\n",
        "    'Benchmark Sharpe Ratio': sharpe_bench,\n",
        "    'Equal Weight Sharpe Ratio': sharpe_equal\n",
        "})\n",
        "\n",
        "# Optionally format the table for clearer display (2 decimal places)\n",
        "annual_sharpe_table = annual_sharpe_table.applymap(lambda x: f\"{x:.2f}\" if pd.notnull(x) else x)\n",
        "\n",
        "print(\"Yearly Sharpe Ratios:\")\n",
        "print(annual_sharpe_table)"
      ],
      "metadata": {
        "id": "5Cmu8Mcc8tuI"
      },
      "id": "5Cmu8Mcc8tuI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Prepare the Data: Create a Year Column from 'Predicted_month' ---\n",
        "merged_df['Year'] = merged_df['Predicted_month'].dt.year\n",
        "\n",
        "# --- Function to Calculate Annual Return ---\n",
        "def annual_return(group, col):\n",
        "    \"\"\"\n",
        "    Compute the annual return as the compounded return over the year.\n",
        "    \"\"\"\n",
        "    return (1 + group[col]).prod() - 1\n",
        "\n",
        "# --- Compute Annual Returns for Each Strategy by Grouping on 'Year' ---\n",
        "annual_return_ml = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, 'ML Allocated Strategy Return'))\n",
        "annual_return_equal = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, 'Equal Factor Weight Strategy'))\n",
        "\n",
        "# --- Compute Excess Return (ML - Equal Weight) ---\n",
        "excess_return_ml = annual_return_ml - annual_return_equal\n",
        "\n",
        "# --- Combine into a DataFrame ---\n",
        "annual_metrics = pd.DataFrame({\n",
        "    'ML Annual Return': annual_return_ml,\n",
        "    'Equal Annual Return': annual_return_equal,\n",
        "    'Excess Return (ML-EQ)': excess_return_ml\n",
        "})\n",
        "\n",
        "# --- Optionally Format the Metrics for Display ---\n",
        "annual_metrics_formatted = annual_metrics.applymap(lambda x: f\"{x:.2f}\" if pd.notnull(x) else x)\n",
        "print(\"Annual Metrics:\")\n",
        "print(annual_metrics_formatted)\n",
        "\n",
        "# --- Plot Only the Excess Returns ---\n",
        "fig, ax = plt.subplots(figsize=(12,6))\n",
        "\n",
        "ax.plot(annual_metrics.index, annual_metrics['Excess Return (ML-EQ)'],\n",
        "        label='Excess Return (ML-EQ)', color='red', marker='s')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Excess Return (ML - Equal Weight)')\n",
        "ax.set_title('Annual Excess Return: ML Allocated Strategy vs Equal Weight Strategy')\n",
        "ax.legend()\n",
        "ax.grid(True, linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "E0cAJzowyqZn"
      },
      "id": "E0cAJzowyqZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Prepare the Data: Create a Year Column from 'Predicted_month'\n",
        "merged_df['Year'] = merged_df['Predicted_month'].dt.year\n",
        "\n",
        "# --- Function to Calculate Annual Return ---\n",
        "def annual_return(group, col):\n",
        "    \"\"\"\n",
        "    Compute the annual return as the compounded return over the year.\n",
        "    \"\"\"\n",
        "    return (1 + group[col]).prod() - 1\n",
        "\n",
        "# --- Compute Annual Returns for the ML Allocated Strategy ---\n",
        "annual_return_ml = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, 'ML Allocated Strategy Return'))\n",
        "\n",
        "# --- Compute Annual Returns for Each Factor ---\n",
        "factor_names = FACTORS\n",
        "annual_returns_factors = {}\n",
        "excess_returns = {}\n",
        "\n",
        "for factor in factor_names:\n",
        "    annual_returns_factors[factor] = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, factor))\n",
        "    # Compute excess return: ML strategy annual return minus factor's annual return.\n",
        "    excess_returns[factor] = annual_return_ml - annual_returns_factors[factor]\n",
        "\n",
        "# Combine the computed metrics into a DataFrame for reference (optional)\n",
        "annual_metrics = pd.DataFrame({'ML Annual Return': annual_return_ml})\n",
        "for factor in factor_names:\n",
        "    annual_returns_factors[factor] = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, factor))\n",
        "    excess_returns[factor] = annual_return_ml - annual_returns_factors[factor]\n",
        "\n",
        "print(\"Annual Metrics:\")\n",
        "print(annual_metrics.round(4))\n",
        "\n",
        "# --- Plot Excess Returns for Each Factor ---\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14,10), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, factor in enumerate(factor_names):\n",
        "    ax = axes[idx]\n",
        "    # Plot excess return: ML annual return minus factor annual return.\n",
        "    ax.plot(excess_returns[factor].index, excess_returns[factor],\n",
        "            label=f'Excess Return (ML - {factor})', marker='o')\n",
        "    ax.set_title(f'Excess Return (ML - {factor})')\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.set_ylabel('Excess Return')\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lo1wj3nU3ceD"
      },
      "id": "Lo1wj3nU3ceD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======= USER-DEFINED DATE RANGE =======\n",
        "# Adjust these dates to view feature importances for a specific period\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date   = pd.to_datetime('2022-12-31')\n",
        "\n",
        "# ======= Filter the Data =======\n",
        "# Filter the results_df for the specified date range based on the 'Predicted_month' column\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "]\n",
        "\n",
        "# ======= Get Unique Regimes and Feature Count =======\n",
        "existing_regimes = filtered_results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "n_features = len(filtered_results_df['Feature_Importances'].iloc[0])  # Assumes each entry is a vector\n",
        "\n",
        "# ======= Robust Feature Naming =======\n",
        "try:\n",
        "    # Validate if the predefined FEATURES list matches the actual feature count\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"⚠️ Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features}).\")\n",
        "        print(\"Using auto-generated feature names instead.\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate default feature names if there's a mismatch or if FEATURES is undefined\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features.\")\n",
        "\n",
        "# ======= Compute Overall Average Feature Importances =======\n",
        "overall_avg_fi = np.vstack(filtered_results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# ======= Compute Regime-Specific Average Feature Importances =======\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = filtered_results_df[filtered_results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# ======= Sort Features by Overall Importance (Descending) =======\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # Ensure index bounds\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# ======= Plotting =======\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One overall plot plus one for each regime\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Feature Importances ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    # If zero or one regime, show only the overall chart\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Extra space for feature labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B60Pa4LKEGxo"
      },
      "id": "B60Pa4LKEGxo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ======================\n",
        "# --- Part 1: Process results_df (ML Strategy) ---\n",
        "# ======================\n",
        "\n",
        "# Ensure 'Predicted_month' is in datetime format\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "\n",
        "# Define your dynamic date range (adjust as needed)\n",
        "start_date = pd.to_datetime('1973-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Filter results_df for the date range and select the desired columns\n",
        "filtered_df = results_df.loc[\n",
        "    (results_df['Predicted_month'] >= start_date) & (results_df['Predicted_month'] <= end_date),\n",
        "    ['Predicted_month', 'Allocated_Return']\n",
        "].sort_values('Predicted_month')\n",
        "\n",
        "# Rename 'Allocated_Return' to 'ML Allocated Strategy Return'\n",
        "filtered_df = filtered_df.rename(columns={'Allocated_Return': 'ML Allocated Strategy Return'})\n",
        "\n",
        "# Calculate cumulative returns for the ML allocated strategy using compound returns (wealth index starting at 1)\n",
        "filtered_df['ML Cumulative Allocated Return'] = (1 + filtered_df['ML Allocated Strategy Return']).cumprod()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# --- Part 2: Process the second DataFrame (df) (Benchmark & Equal Factor Weight) ---\n",
        "# ======================\n",
        "\n",
        "# Create a filtered copy of df for the same date range using the 'Date' column.\n",
        "df_filtered = df.loc[\n",
        "    (pd.to_datetime(df['Date']) >= start_date) & (pd.to_datetime(df['Date']) <= end_date)\n",
        "].copy()\n",
        "\n",
        "# Ensure the 'Date' column is datetime for correct sorting and merging\n",
        "df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
        "\n",
        "# Sort by 'Date'\n",
        "df_filtered = df_filtered.sort_values('Date')\n",
        "\n",
        "# Calculate cumulative returns for the RMW benchmark using 'RMW'\n",
        "df_filtered['RMW Cumulative'] = (1 + df_filtered['RMW']).cumprod()\n",
        "\n",
        "# Calculate the Equal Factor Weight Strategy returns by taking an equal-weight average of 4 factors (excluding MOM)\n",
        "df_filtered['Equal Factor Weight Strategy'] = df_filtered[FACTORS].mean(axis=1)\n",
        "\n",
        "\n",
        "# Calculate cumulative returns for the Equal Factor Weight Strategy using compound returns (wealth index starting at 1)\n",
        "df_filtered['Equal Factor Weight Cumulative Return'] = (1 + df_filtered['Equal Factor Weight Strategy']).cumprod()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# --- Part 3: Merge the two DataFrames ---\n",
        "# ======================\n",
        "\n",
        "# Merge on the date columns:\n",
        "#   - In filtered_df: 'Predicted_month'\n",
        "#   - In df_filtered: 'Date'\n",
        "merged_df = pd.merge(\n",
        "    filtered_df,\n",
        "    df_filtered[['Date', 'RMW Cumulative', 'Equal Factor Weight Strategy', 'Equal Factor Weight Cumulative Return']],\n",
        "    left_on='Predicted_month',\n",
        "    right_on='Date',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop the duplicate 'Date' column from the merge\n",
        "merged_df.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# ======================\n",
        "# --- Part 4: Define 5-Year Periods and Compute Cumulative Returns for Each Block ---\n",
        "# ======================\n",
        "\n",
        "def define_5_year_period(date, start_year=1973):\n",
        "    \"\"\"\n",
        "    Assign each date to a discrete 5-year block starting at 'start_year'.\n",
        "    For example, a date in 1973-1977 will be labeled \"1973-1977\",\n",
        "    in 1978-1982 as \"1978-1982\", etc.\n",
        "    \"\"\"\n",
        "    if pd.isnull(date):\n",
        "        return None\n",
        "    year = date.year\n",
        "    # Determine the block index from start_year\n",
        "    block_index = (year - start_year) // 5\n",
        "    block_start = start_year + 5 * block_index\n",
        "    block_end = block_start + 4\n",
        "    return f\"{block_start}-{block_end}\"\n",
        "\n",
        "# Apply the function to create a new column for 5-year period labels\n",
        "merged_df['5_Year_Period'] = merged_df['Predicted_month'].apply(lambda x: define_5_year_period(x, start_year=1973))\n",
        "\n",
        "def five_year_return(group, col):\n",
        "    \"\"\"\n",
        "    Compute the total return over the 5-year block for the given cumulative return column.\n",
        "    This is done by taking the ratio of the last value to the first value of the block, minus 1.\n",
        "    \"\"\"\n",
        "    # Ensure the group is sorted by date\n",
        "    group = group.sort_values(by='Predicted_month')\n",
        "    start_val = group[col].iloc[0]\n",
        "    end_val   = group[col].iloc[-1]\n",
        "    return (end_val / start_val) - 1\n",
        "\n",
        "# Group by the 5_Year_Period and compute returns for each strategy\n",
        "five_year_results = merged_df.groupby('5_Year_Period').apply(\n",
        "    lambda g: pd.Series({\n",
        "        'ML_5yr_return'    : five_year_return(g, 'ML Cumulative Allocated Return'),\n",
        "        'Equal_5yr_return' : five_year_return(g, 'Equal Factor Weight Cumulative Return'),\n",
        "        'RMW_5yr_return'   : five_year_return(g, 'RMW Cumulative')\n",
        "    })\n",
        ").reset_index()\n",
        "\n",
        "# Calculate the excess return of ML strategy over the Equal Factor Weight strategy\n",
        "five_year_results['Excess_return'] = five_year_results['ML_5yr_return'] - five_year_results['Equal_5yr_return']\n",
        "\n",
        "# ======================\n",
        "# (Optional) Remove partial 5-year blocks if desired.\n",
        "# For example, you might require at least 5 full years of data.\n",
        "# Uncomment the following block if needed.\n",
        "# ======================\n",
        "# def is_full_5_years(group):\n",
        "#     date_range = group['Predicted_month'].max() - group['Predicted_month'].min()\n",
        "#     return date_range.days >= 1825  # Approximately 5 years in days\n",
        "#\n",
        "# full_periods = []\n",
        "# for period, grp in merged_df.groupby('5_Year_Period'):\n",
        "#     if is_full_5_years(grp):\n",
        "#         full_periods.append(period)\n",
        "#\n",
        "# five_year_results = five_year_results[five_year_results['5_Year_Period'].isin(full_periods)]\n",
        "\n",
        "# ======================\n",
        "# --- Part 5: Display the 5-Year Cumulative Returns ---\n",
        "# ======================\n",
        "\n",
        "print(\"5-Year Cumulative Returns Comparison:\")\n",
        "print(five_year_results)"
      ],
      "metadata": {
        "id": "_KfEgK83EKLT"
      },
      "id": "_KfEgK83EKLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corr Heat map & regiimi sharpet\n"
      ],
      "metadata": {
        "id": "WnQxAZGgWuKt"
      },
      "id": "WnQxAZGgWuKt"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Regime-wise Correlation Heatmaps\n",
        "#\n",
        "# For the selected return columns, compute and plot the correlation matrix\n",
        "# for each market regime as a heatmap.\n",
        "\n",
        "# %%\n",
        "# Use the global FACTORS instead of redefining returns_columns\n",
        "unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "for regime in unique_regimes:\n",
        "    regime_data = df[df[REGIMES_COLUMN] == regime][FACTORS]\n",
        "    corr = regime_data.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "    plt.title(f\"Return Correlation Heatmap - {regime}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4GbzDKk2FZYH"
      },
      "id": "4GbzDKk2FZYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Sharpe Ratios by Market Regime\n",
        "#\n",
        "# Compute and visualize Sharpe ratios for selected factors across each regime,\n",
        "# as well as the unconditional (all-data) values, using a bar chart.\n",
        "# The numeric regime codes are converted back to their original names using the regime_mapping,\n",
        "# and then further shortened using regime_short_mapping.\n",
        "\n",
        "# %%\n",
        "# Define factors and regime columns (using global variables if already defined)\n",
        "factors_columns = FACTORS\n",
        "regimes_column = REGIMES_COLUMN   # Assumes REGIMES_COLUMN was defined earlier\n",
        "\n",
        "# Use the previously created regime_short_mapping to convert numeric codes back to short names.\n",
        "# (If a code is not in regime_short_mapping, it will default to \"Regime <code>\")\n",
        "regime_short_names = {reg: regime_short_mapping.get(reg, f\"Regime {reg}\")\n",
        "                      for reg in df[regimes_column].unique()}\n",
        "\n",
        "sharpe_ratios = {\n",
        "    regime_short_names[regime]: (\n",
        "        df[df[regimes_column] == regime][factors_columns].mean() /\n",
        "        df[df[regimes_column] == regime][factors_columns].std()\n",
        "    )\n",
        "    for regime in df[regimes_column].unique()\n",
        "}\n",
        "\n",
        "# Calculate the \"Unconditional\" Sharpe ratios (using all data)\n",
        "sharpe_ratios[\"Unconditional\"] = df[factors_columns].mean() / df[factors_columns].std()\n",
        "\n",
        "# Convert the dictionary to a DataFrame and set column names\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios).T\n",
        "sharpe_ratios_df.columns = factors_columns\n",
        "\n",
        "# Plot the Sharpe ratios using the same styling as before.\n",
        "plt.figure(figsize=(14, 8))\n",
        "sharpe_ratios_df.plot(\n",
        "    kind=\"bar\",\n",
        "    grid=True,\n",
        "    colormap=\"viridis\",\n",
        "    title=\"Sharpe Ratios by Regime and Unconditional\",\n",
        "    figsize=(14, 8)\n",
        ")\n",
        "plt.ylabel(\"Sharpe Ratio\", fontsize=12)\n",
        "plt.xlabel(\"Market Regimes\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(title=\"Factors\", fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCkBikW6A2o0"
      },
      "id": "GCkBikW6A2o0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XgeSIiAu-M2b"
      },
      "id": "XgeSIiAu-M2b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}