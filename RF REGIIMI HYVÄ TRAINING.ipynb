{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IDEAT: KMEANS VIX, Cap max weight for factor\n",
        "\n",
        "<div style=\"font-size:14px;\">\n",
        "<strong>TO DO:</strong><br><br>\n",
        "Tarkistaa ovatko regiimit oikein, exp. antaa ainoana kaikille neg sharpet<br>\n",
        "Katsoa vielä financial turbulence koodi<br>\n",
        "Data varmistukset (ei dataa tulevaisuudesta)<br><br>\n",
        "\n",
        "<strong>Lisää features:</strong><br>\n",
        "RSI<br>\n",
        "Yield spread<br>\n",
        "Muita??<br><br>\n",
        "\n",
        "<strong>Muuta:</strong><br>\n",
        "regiimi testaus drawdowneilla?<br><br>\n",
        "regiimi specifi model ennustus?\n",
        "\n",
        "\n",
        "<strong>Mallin kehitys:</strong><br>\n",
        "1. Feature eliminointi<br>\n",
        "2. Training interval<br>\n",
        "3. Hyperparametrit\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "mqpPtrCOkXAO"
      },
      "id": "mqpPtrCOkXAO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model settings"
      ],
      "metadata": {
        "id": "k1mEzhhWXup6"
      },
      "id": "k1mEzhhWXup6"
    },
    {
      "cell_type": "code",
      "source": [
        "use_regime_split = False\n",
        "\n",
        "#Default models\n",
        "RF = False # perus random forest\n",
        "GB = False # perus gradient boost\n",
        "Hybrid = False\n",
        "\n",
        "#Looping models\n",
        "RF_feature_seek = False # random forest all combinations\n",
        "seek_all = False\n",
        "gb_loop = True\n",
        "\n",
        "#DATA\n",
        "FF5 = True\n",
        "FF5_long = False\n",
        "MSCI = False\n",
        "\n",
        "local = False #ajetaanko colab vai oma kone\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-AF3FdwvaLPp"
      },
      "id": "-AF3FdwvaLPp",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if sum([RF, GB, RF_feature_seek, Hybrid, seek_all, gb_loop]) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [RF, GB, RF_feature_seek, Hybrid, seek_all, gb_loop] must be True.\")\n",
        "\n",
        "# Check subgroup 2: Exactly one of [FF5, FF5_long, MSCI] must be True\n",
        "if sum([FF5, FF5_long, MSCI]) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [FF5, FF5_long, MSCI] must be True.\")\n",
        "\n",
        "print(\"Toggles are correctly set.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ktRzXgrQHYX",
        "outputId": "58caaa3e-90d0-47f9-addd-e6c06c5267b2"
      },
      "id": "7ktRzXgrQHYX",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Toggles are correctly set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4085d55c-e568-465c-9bcc-6013281c105d",
      "metadata": {
        "tags": [],
        "id": "4085d55c-e568-465c-9bcc-6013281c105d"
      },
      "outputs": [],
      "source": [
        "# # Import Required Libraries\n",
        "#\n",
        "# Import all necessary libraries for data manipulation, visualization,\n",
        "# machine learning, and regression analysis.\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from tabulate import tabulate\n",
        "\n",
        "from IPython.display import display, HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not local:\n",
        "\n",
        "  %cd /content\n",
        "  !rm -rf Gradu\n",
        "  !git clone https://github.com/Elkkujou/Gradu.git\n",
        "  %cd /content/Gradu\n",
        "  !ls\n",
        "  xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "\n",
        "else:\n",
        "\n",
        "\n",
        "\n",
        "    repo_url = \"https://github.com/Elkkujou/Gradu.git\"\n",
        "    repo_name = \"Gradu\"  # Name of the cloned folder\n",
        "\n",
        "    # Check if the directory already exists\n",
        "    if os.path.exists(repo_name):\n",
        "        print(f\"Folder '{repo_name}' already exists. Pulling latest changes...\")\n",
        "        # Change to the existing repo folder and pull the latest updates\n",
        "        subprocess.run([\"git\", \"-C\", repo_name, \"pull\"], check=True)\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{repo_name}'...\")\n",
        "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "    # List contents of the cloned repository\n",
        "    subprocess.run([\"ls\", repo_name], check=True)\n",
        "    xls_file = pd.ExcelFile(\"Gradu/THE_2ND_latest.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j2fmaZCMluYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78b3b649-5b65-43a0-e794-f5ddf1612599"
      },
      "id": "j2fmaZCMluYf",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Gradu'...\n",
            "remote: Enumerating objects: 734, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 734 (delta 150), reused 124 (delta 124), pack-reused 566 (from 3)\u001b[K\n",
            "Receiving objects: 100% (734/734), 146.98 MiB | 31.05 MiB/s, done.\n",
            "Resolving deltas: 100% (351/351), done.\n",
            "/content/Gradu\n",
            " chatti_RF.ipynb\t\t      regime_prediction_msci.ipynb\n",
            " data+regimes.xlsx\t\t      regime_pred.txt\n",
            " Fama_french_XGBOOST.ipynb\t      RF_Gradu.ipynb\n",
            "'Financial turbulence.ipynb'\t     'RF REGIIMI HYVÄ TRAINING.ipynb'\n",
            " FT_source.xlsx\t\t\t     'RF_REGIIMI_HYVÄ_TRAINING (MSCI).ipynb'\n",
            " Gradient_boost_malli.ipynb\t     'RF_regime (3).ipynb'\n",
            " MSCI_XGBOOST.ipynb\t\t      THE_2ND_latest.xlsx\n",
            " Regiimi_prediction.ipynb\t      THE_2ND.xlsx\n",
            " regime_prediction_famafrench.ipynb   THE_ONE.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5:\n",
        "  SHEET_NAME = \"ajodata_FF5\"\n",
        "  FEATURES = ['CPI%', 'T10YFF', 'LEI%', 'Amihud', 'GARCH_1M']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = False"
      ],
      "metadata": {
        "id": "R_dU9PNF-Puv"
      },
      "id": "R_dU9PNF-Puv",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5_long:\n",
        "  SHEET_NAME = \"ajodata_FF5_long\"\n",
        "  FEATURES = ['CPI%', 'GARCH_1M', 'T10YFF', 'LEI%', 'Amihud']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "wJo52ErY-v0Q"
      },
      "id": "wJo52ErY-v0Q",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MSCI:\n",
        "  SHEET_NAME = \"ajodata_MSCI\"\n",
        "  FEATURES = ['CPI%', 'GARCH_1M', 'T10YFF', 'LEI%', 'Amihud']\n",
        "  FACTORS = [\n",
        "    'Size',\n",
        "    'value',\n",
        "    'Quality',\n",
        "    'min_vola']\n",
        "  BENCHMARK = ['Us_standard']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "AT6Cf_ge_ApN"
      },
      "id": "AT6Cf_ge_ApN",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data"
      ],
      "metadata": {
        "id": "ZwuAM8venlx5"
      },
      "id": "ZwuAM8venlx5"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
      "metadata": {
        "tags": [],
        "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
        "outputId": "f185f1e4-d305-449d-8b58-031a08f06eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers in the 'ajodata_FF5' sheet:\n",
            "Index(['Date', 'SMB', 'HML', 'RMW', 'CMA', 'Mkt', 'RF', 'Mkt-RF', 'GARCH_1M',\n",
            "       'CPI%', 'T10YFF', 'Amihud', 'LEI%', 'Cape', 'Cape %'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table class=\"dataframe table table-striped\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>First observation date</td>\n",
              "      <td>1963-07-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Last observation date</td>\n",
              "      <td>2024-11-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Total number of observations</td>\n",
              "      <td>737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = xls_file.parse(SHEET_NAME)\n",
        "df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "# Print headers dynamically\n",
        "print(f\"Headers in the '{SHEET_NAME}' sheet:\")\n",
        "print(df.columns)\n",
        "\n",
        "REGIMES_COLUMN = 'Predicted_reg'\n",
        "\n",
        "# Convert the leftmost column (assumed to be the date column) to datetime\n",
        "date_column = df.columns[0]\n",
        "df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "# Retrieve first and last observation dates and count observations\n",
        "first_date = df[date_column].iloc[0]\n",
        "last_date = df[date_column].iloc[-1]\n",
        "n_observations = len(df)\n",
        "\n",
        "# Create a DataFrame with the information\n",
        "info_df = pd.DataFrame({\n",
        "    \"Description\": [\"First observation date\", \"Last observation date\", \"Total number of observations\"],\n",
        "    \"Value\": [first_date, last_date, n_observations]\n",
        "})\n",
        "\n",
        "# Display the results as a neat HTML table\n",
        "display(HTML(info_df.to_html(index=False, classes=\"table table-striped\", border=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Define Helper Functions ---\n",
        "def annualized_return(returns):\n",
        "    \"\"\"Compute the compounded annualized return (assuming monthly returns).\"\"\"\n",
        "    return np.prod(1 + returns)**(12 / len(returns)) - 1\n",
        "\n",
        "def compute_metrics(returns):\n",
        "    \"\"\"\n",
        "    Compute key metrics for a returns series:\n",
        "      - Annualized Return\n",
        "      - Annualized Volatility (assuming monthly returns)\n",
        "      - Total Cumulative Return\n",
        "    \"\"\"\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    total_cum_return = cumulative_returns.iloc[-1] - 1\n",
        "    ann_ret = annualized_return(returns)\n",
        "    ann_vol = np.std(returns) * np.sqrt(12)\n",
        "    return ann_ret, ann_vol, total_cum_return\n",
        "\n",
        "# --- Compute Metrics for Benchmark and Each Factor ---\n",
        "metrics = []\n",
        "\n",
        "# Compute metrics for the benchmark.\n",
        "benchmark_returns = df[BENCHMARK[0]]\n",
        "bench_ann_ret, bench_ann_vol, bench_cum_return = compute_metrics(benchmark_returns)\n",
        "metrics.append({\n",
        "    \"Strategy\": \"Benchmark\",\n",
        "    \"Annualized Return\": f\"{bench_ann_ret*100:.2f}%\",\n",
        "    \"Annualized Volatility\": f\"{bench_ann_vol*100:.2f}%\",\n",
        "    \"Total Cumulative Return\": f\"{bench_cum_return*100:.2f}%\"\n",
        "})\n",
        "\n",
        "# Compute metrics for each factor in FACTORS.\n",
        "for factor in FACTORS:\n",
        "    factor_returns = df[factor]\n",
        "    factor_ann_ret, factor_ann_vol, factor_cum_return = compute_metrics(factor_returns)\n",
        "    metrics.append({\n",
        "        \"Strategy\": factor,\n",
        "        \"Annualized Return\": f\"{factor_ann_ret*100:.2f}%\",\n",
        "        \"Annualized Volatility\": f\"{factor_ann_vol*100:.2f}%\",\n",
        "        \"Total Cumulative Return\": f\"{factor_cum_return*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the metrics.\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# --- Display the Results as an HTML Table ---\n",
        "display(HTML(metrics_df.to_html(index=False)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "phBSNpYXviHe",
        "outputId": "c9959bf1-a84e-41ac-da94-935f640a0070",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "phBSNpYXviHe",
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Strategy</th>\n",
              "      <th>Annualized Return</th>\n",
              "      <th>Annualized Volatility</th>\n",
              "      <th>Total Cumulative Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Benchmark</td>\n",
              "      <td>10.71%</td>\n",
              "      <td>15.46%</td>\n",
              "      <td>51691.71%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>SMB</td>\n",
              "      <td>1.89%</td>\n",
              "      <td>10.55%</td>\n",
              "      <td>216.75%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>HML</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>10.37%</td>\n",
              "      <td>473.19%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CMA</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>7.18%</td>\n",
              "      <td>472.94%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>RMW</td>\n",
              "      <td>3.14%</td>\n",
              "      <td>7.67%</td>\n",
              "      <td>568.93%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
      "metadata": {
        "tags": [],
        "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
        "outputId": "5078fb0d-e1ba-4afb-a221-c8daabfc33c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows before cleaning: 737\n",
            "\n",
            "Missing values in feature columns before dropping NA:\n",
            "CPI%        0\n",
            "T10YFF      0\n",
            "LEI%        0\n",
            "Amihud      0\n",
            "GARCH_1M    0\n",
            "dtype: int64\n",
            "\n",
            "No missing values found in feature columns. Data is clean.\n",
            "\n",
            "Indices aligned: True\n",
            "\n",
            "Parameters and dataset verified.\n",
            "\n",
            "Winning Factor counts:\n",
            "SMB      227\n",
            "RMW      217\n",
            "HML      175\n",
            "CMA      118\n",
            "Total    737\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# # Prepare Data for Model Training – Data Cleaning & Verification\n",
        "\n",
        "# we keep all rows and compute the winning factor as the factor (from FACTORS) with the highest value in each row.\n",
        "\n",
        "# Print the initial number of rows.\n",
        "initial_rows = len(df)\n",
        "print(f\"Total number of rows before cleaning: {initial_rows}\")\n",
        "\n",
        "# Check missing values in feature columns (FEATURES) before dropping NAs.\n",
        "missing_counts = df[FEATURES].isna().sum()\n",
        "print(\"\\nMissing values in feature columns before dropping NA:\")\n",
        "print(missing_counts)\n",
        "\n",
        "# Save the number of rows before dropping NA and then drop rows with missing values in FEATURES.\n",
        "initial_rows_features = len(df)\n",
        "X = df[FEATURES].dropna()\n",
        "rows_after_drop = len(X)\n",
        "dropped_rows = initial_rows_features - rows_after_drop\n",
        "\n",
        "if dropped_rows > 0:\n",
        "    print(f\"\\nDropped {dropped_rows} rows due to missing values in feature columns.\")\n",
        "else:\n",
        "    print(\"\\nNo missing values found in feature columns. Data is clean.\")\n",
        "\n",
        "# Compute the Winning Factor by taking the column (from FACTORS) that has the maximum value in each row.\n",
        "# This assumes that the FACTORS columns exist in df and contain numeric values.\n",
        "df['Winning Factor'] = df[FACTORS].idxmax(axis=1)\n",
        "\n",
        "# Define the target variable based on rows retained in X.\n",
        "# The winning factor is encoded as a categorical variable.\n",
        "y = df['Winning Factor'].astype('category').cat.codes.loc[X.index]\n",
        "print(\"\\nIndices aligned:\", X.index.equals(y.index))\n",
        "\n",
        "# Ensure the data is sorted by date.\n",
        "df = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Verify that all required columns exist.\n",
        "# Here, we require the FEATURES columns, the 'USA Standard (Large+Mid Cap)' column,\n",
        "# as well as all the FACTORS and the BENCHMARK columns.\n",
        "required_columns = FEATURES + FACTORS + BENCHMARK\n",
        "for col in required_columns:\n",
        "    if col not in df.columns:\n",
        "        raise ValueError(f\"Missing required column: {col}\")\n",
        "print(\"\\nParameters and dataset verified.\")\n",
        "\n",
        "# Compute the counts for each winning factor.\n",
        "winning_factor_counts = df['Winning Factor'].value_counts()\n",
        "\n",
        "# Compute total count and append it as the last row.\n",
        "total_counts = winning_factor_counts.sum()\n",
        "winning_factor_counts = pd.concat([winning_factor_counts, pd.Series({'Total': total_counts})])\n",
        "\n",
        "# Print the counts with \"Total\" as the last row.\n",
        "print(\"\\nWinning Factor counts:\")\n",
        "print(winning_factor_counts)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_regime_split:\n",
        "\n",
        "    # --- Regime Mapping & Conversion to Numeric Codes (Dynamic) ---\n",
        "\n",
        "    # Dynamically extract the unique values in the REGIMES_COLUMN.\n",
        "    unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "\n",
        "    # Convert the Regimes column to a categorical type with the unique values, ordered alphabetically.\n",
        "    df[REGIMES_COLUMN] = pd.Categorical(df[REGIMES_COLUMN], categories=sorted(unique_regimes), ordered=True)\n",
        "\n",
        "    # Create a dictionary mapping numeric codes to the regime names based on the unique values.\n",
        "    regime_mapping = {i: cat for i, cat in enumerate(df[REGIMES_COLUMN].cat.categories)}\n",
        "\n",
        "    # Now encode the Regimes column as numeric codes.\n",
        "    df[REGIMES_COLUMN] = df[REGIMES_COLUMN].cat.codes\n",
        "\n",
        "    # Create a mapping from numeric codes to original regime names.\n",
        "    regime_short_mapping = {code: name for code, name in regime_mapping.items()}\n",
        "\n",
        "    # Calculate the number of observations for each regime using value_counts (without reindexing).\n",
        "    obs_counts = df[REGIMES_COLUMN].value_counts(sort=False)\n",
        "\n",
        "    # Create a DataFrame preview of the regime mapping, including observation counts.\n",
        "    mapping_table_data = []\n",
        "    for code in regime_mapping.keys():\n",
        "        mapping_table_data.append({\n",
        "            \"Numeric Code\": code,\n",
        "            \"Original Name\": regime_mapping.get(code, \"N/A\"),\n",
        "            \"Observations\": obs_counts.get(code, 0)\n",
        "        })\n",
        "\n",
        "    # Append a row with the total observations.\n",
        "    total_obs = obs_counts.sum()\n",
        "    mapping_table_data.append({\n",
        "        \"Numeric Code\": \"\",\n",
        "        \"Original Name\": \"Total\",\n",
        "        \"Observations\": total_obs\n",
        "    })\n",
        "\n",
        "    # Create the DataFrame for regime mapping preview and print.\n",
        "    regime_mapping_df = pd.DataFrame(mapping_table_data)\n",
        "\n",
        "    from tabulate import tabulate\n",
        "    print(\"Preview of Dynamic Regime Mapping:\")\n",
        "    print(tabulate(regime_mapping_df, headers=\"keys\", tablefmt=\"psql\", showindex=False))\n"
      ],
      "metadata": {
        "id": "wYgvlvGRUUG4"
      },
      "id": "wYgvlvGRUUG4",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "0wYzowb5Xdau"
      },
      "id": "0wYzowb5Xdau"
    },
    {
      "cell_type": "code",
      "source": [
        "# This removes any row with at least one NaN in any column\n",
        "df = df.dropna()\n",
        "\n",
        "# Optionally, reindex the rows\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "nFVXlc95MuWg"
      },
      "id": "nFVXlc95MuWg",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "dzDmh34EUy4R",
        "outputId": "9f5765ed-3f78-4e18-f94f-259cb840a897"
      },
      "id": "dzDmh34EUy4R",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Date     SMB     HML     RMW     CMA     Mkt      RF  Mkt-RF  \\\n",
              "0   1963-07-30 -0.0041 -0.0097  0.0068 -0.0118 -0.0012  0.0027 -0.0039   \n",
              "1   1963-08-30 -0.0080  0.0180  0.0036 -0.0035  0.0532  0.0025  0.0507   \n",
              "2   1963-09-30 -0.0052  0.0013 -0.0071  0.0029 -0.0130  0.0027 -0.0157   \n",
              "3   1963-10-30 -0.0139 -0.0010  0.0280 -0.0201  0.0282  0.0029  0.0253   \n",
              "4   1963-11-30 -0.0088  0.0175 -0.0051  0.0224 -0.0058  0.0027 -0.0085   \n",
              "..         ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "732 2024-07-30  0.0828  0.0574  0.0022  0.0043  0.0169  0.0045  0.0124   \n",
              "733 2024-08-30 -0.0365 -0.0113  0.0085  0.0086  0.0209  0.0048  0.0161   \n",
              "734 2024-09-30 -0.0102 -0.0259  0.0004 -0.0026  0.0214  0.0040  0.0174   \n",
              "735 2024-10-30 -0.0088  0.0089 -0.0138  0.0103 -0.0058  0.0039 -0.0097   \n",
              "736 2024-11-30  0.0478 -0.0005 -0.0262 -0.0217  0.0691  0.0040  0.0651   \n",
              "\n",
              "     GARCH_1M     CPI%  T10YFF        Amihud      LEI%      Cape    Cape %  \\\n",
              "0    0.409987  0.26135    0.52  5.582783e-12  0.003413  1.096642  0.000000   \n",
              "1    0.281066  0.19550    0.57  6.112556e-13  0.006803  1.106774  0.009239   \n",
              "2    0.431188 -0.09756    0.65  8.801949e-13  0.003378  1.116794  0.009053   \n",
              "3    0.444504  0.09766    0.58  6.240603e-12  0.010101  1.122111  0.004761   \n",
              "4    1.526543  0.09756    0.89  5.778983e-12  0.006667  1.130949  0.007877   \n",
              "..        ...      ...     ...           ...       ...       ...       ...   \n",
              "732  1.087011  0.13892   -1.42  0.000000e+00 -0.001940  1.257904  0.004980   \n",
              "733  0.641443  0.18019   -1.02  0.000000e+00 -0.004859  1.264477  0.005225   \n",
              "734  0.396629  0.22920   -0.55  0.000000e+00 -0.002930  1.269959  0.004335   \n",
              "735  0.684819  0.22646   -0.40  0.000000e+00 -0.003918  1.271925  0.001548   \n",
              "736  0.499183  0.28045    0.25  0.000000e+00 -0.002950  1.276066  0.003256   \n",
              "\n",
              "    Winning Factor  \n",
              "0              RMW  \n",
              "1              HML  \n",
              "2              CMA  \n",
              "3              RMW  \n",
              "4              CMA  \n",
              "..             ...  \n",
              "732            SMB  \n",
              "733            CMA  \n",
              "734            RMW  \n",
              "735            CMA  \n",
              "736            SMB  \n",
              "\n",
              "[737 rows x 16 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-56d93d7f-f252-4135-81fb-e64e84f316b9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>SMB</th>\n",
              "      <th>HML</th>\n",
              "      <th>RMW</th>\n",
              "      <th>CMA</th>\n",
              "      <th>Mkt</th>\n",
              "      <th>RF</th>\n",
              "      <th>Mkt-RF</th>\n",
              "      <th>GARCH_1M</th>\n",
              "      <th>CPI%</th>\n",
              "      <th>T10YFF</th>\n",
              "      <th>Amihud</th>\n",
              "      <th>LEI%</th>\n",
              "      <th>Cape</th>\n",
              "      <th>Cape %</th>\n",
              "      <th>Winning Factor</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1963-07-30</td>\n",
              "      <td>-0.0041</td>\n",
              "      <td>-0.0097</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>-0.0118</td>\n",
              "      <td>-0.0012</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>0.409987</td>\n",
              "      <td>0.26135</td>\n",
              "      <td>0.52</td>\n",
              "      <td>5.582783e-12</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>1.096642</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>RMW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1963-08-30</td>\n",
              "      <td>-0.0080</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.0532</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0507</td>\n",
              "      <td>0.281066</td>\n",
              "      <td>0.19550</td>\n",
              "      <td>0.57</td>\n",
              "      <td>6.112556e-13</td>\n",
              "      <td>0.006803</td>\n",
              "      <td>1.106774</td>\n",
              "      <td>0.009239</td>\n",
              "      <td>HML</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1963-09-30</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>-0.0071</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>-0.0130</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>-0.0157</td>\n",
              "      <td>0.431188</td>\n",
              "      <td>-0.09756</td>\n",
              "      <td>0.65</td>\n",
              "      <td>8.801949e-13</td>\n",
              "      <td>0.003378</td>\n",
              "      <td>1.116794</td>\n",
              "      <td>0.009053</td>\n",
              "      <td>CMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1963-10-30</td>\n",
              "      <td>-0.0139</td>\n",
              "      <td>-0.0010</td>\n",
              "      <td>0.0280</td>\n",
              "      <td>-0.0201</td>\n",
              "      <td>0.0282</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0253</td>\n",
              "      <td>0.444504</td>\n",
              "      <td>0.09766</td>\n",
              "      <td>0.58</td>\n",
              "      <td>6.240603e-12</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>1.122111</td>\n",
              "      <td>0.004761</td>\n",
              "      <td>RMW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1963-11-30</td>\n",
              "      <td>-0.0088</td>\n",
              "      <td>0.0175</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0224</td>\n",
              "      <td>-0.0058</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>-0.0085</td>\n",
              "      <td>1.526543</td>\n",
              "      <td>0.09756</td>\n",
              "      <td>0.89</td>\n",
              "      <td>5.778983e-12</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>1.130949</td>\n",
              "      <td>0.007877</td>\n",
              "      <td>CMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>732</th>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>0.0828</td>\n",
              "      <td>0.0574</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0169</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0124</td>\n",
              "      <td>1.087011</td>\n",
              "      <td>0.13892</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.001940</td>\n",
              "      <td>1.257904</td>\n",
              "      <td>0.004980</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>733</th>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>-0.0365</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0209</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0161</td>\n",
              "      <td>0.641443</td>\n",
              "      <td>0.18019</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.004859</td>\n",
              "      <td>1.264477</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>CMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>734</th>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>-0.0102</td>\n",
              "      <td>-0.0259</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>0.0214</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>0.396629</td>\n",
              "      <td>0.22920</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.002930</td>\n",
              "      <td>1.269959</td>\n",
              "      <td>0.004335</td>\n",
              "      <td>RMW</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>735</th>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>-0.0088</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>-0.0138</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>-0.0058</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>-0.0097</td>\n",
              "      <td>0.684819</td>\n",
              "      <td>0.22646</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.003918</td>\n",
              "      <td>1.271925</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>CMA</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736</th>\n",
              "      <td>2024-11-30</td>\n",
              "      <td>0.0478</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0262</td>\n",
              "      <td>-0.0217</td>\n",
              "      <td>0.0691</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0651</td>\n",
              "      <td>0.499183</td>\n",
              "      <td>0.28045</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>-0.002950</td>\n",
              "      <td>1.276066</td>\n",
              "      <td>0.003256</td>\n",
              "      <td>SMB</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>737 rows × 16 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56d93d7f-f252-4135-81fb-e64e84f316b9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-56d93d7f-f252-4135-81fb-e64e84f316b9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-56d93d7f-f252-4135-81fb-e64e84f316b9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c467096c-de6f-4286-8595-47c37c428be5\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c467096c-de6f-4286-8595-47c37c428be5')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c467096c-de6f-4286-8595-47c37c428be5 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_f427c08b-2d99-4697-866e-d863ff7e2fe0\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f427c08b-2d99-4697-866e-d863ff7e2fe0 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 737,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"1963-07-30 00:00:00\",\n        \"max\": \"2024-11-30 00:00:00\",\n        \"num_unique_values\": 737,\n        \"samples\": [\n          \"2019-04-30 00:00:00\",\n          \"1966-04-30 00:00:00\",\n          \"2009-04-30 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SMB\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.030482775206935448,\n        \"min\": -0.1532,\n        \"max\": 0.18280000000000002,\n        \"num_unique_values\": 530,\n        \"samples\": [\n          -0.0005,\n          -0.0146,\n          0.0013\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"HML\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.029944868166812095,\n        \"min\": -0.1388,\n        \"max\": 0.128,\n        \"num_unique_values\": 513,\n        \"samples\": [\n          0.0034999999999999996,\n          -0.0176,\n          0.0096\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RMW\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022170609643194675,\n        \"min\": -0.1865,\n        \"max\": 0.1307,\n        \"num_unique_values\": 446,\n        \"samples\": [\n          0.0212,\n          0.008199999999999999,\n          -0.0278\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CMA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.02072678890190611,\n        \"min\": -0.07200000000000001,\n        \"max\": 0.0907,\n        \"num_unique_values\": 457,\n        \"samples\": [\n          0.0166,\n          -0.0662,\n          0.06559999999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mkt\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0446664550928636,\n        \"min\": -0.2264,\n        \"max\": 0.1661,\n        \"num_unique_values\": 635,\n        \"samples\": [\n          -0.0618,\n          -0.0012000000000000005,\n          -0.0565\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002636746845490088,\n        \"min\": 0.0,\n        \"max\": 0.013500000000000002,\n        \"num_unique_values\": 106,\n        \"samples\": [\n          0.0001,\n          0.0038,\n          0.0026\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Mkt-RF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.04480690876610601,\n        \"min\": -0.2324,\n        \"max\": 0.161,\n        \"num_unique_values\": 582,\n        \"samples\": [\n          0.0434,\n          -0.0229,\n          -0.011399999999999999\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GARCH_1M\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5494643426602868,\n        \"min\": 0.0922015472219609,\n        \"max\": 5.944321567175251,\n        \"num_unique_values\": 737,\n        \"samples\": [\n          0.3340229340188635,\n          0.331529766956197,\n          1.8862635272176047\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"CPI%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.31761354006539066,\n        \"min\": -1.77055,\n        \"max\": 1.80995,\n        \"num_unique_values\": 703,\n        \"samples\": [\n          0.4133,\n          0.32733,\n          0.2924\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T10YFF\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.7465363607675652,\n        \"min\": -9.57,\n        \"max\": 4.04,\n        \"num_unique_values\": 416,\n        \"samples\": [\n          1.15,\n          2.46,\n          -1.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Amihud\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.325093632084421e-12,\n        \"min\": 0.0,\n        \"max\": 2.635503775878702e-11,\n        \"num_unique_values\": 691,\n        \"samples\": [\n          6.282382688746791e-15,\n          5.867787123803853e-16,\n          1.701972681642527e-13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"LEI%\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007516271141122038,\n        \"min\": -0.05488372093023261,\n        \"max\": 0.022222222222222195,\n        \"num_unique_values\": 655,\n        \"samples\": [\n          0.008823529411764622,\n          0.005221932114882581,\n          0.0012406947890819917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cape\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.24407381910624673,\n        \"min\": 0.11987320963374827,\n        \"max\": 1.5811418052109967,\n        \"num_unique_values\": 737,\n        \"samples\": [\n          1.4324771717688594,\n          1.3360519226730834,\n          0.21336999005711468\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cape %\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.05629320122725271,\n        \"min\": -0.40979259687140107,\n        \"max\": 1.0281056896545262,\n        \"num_unique_values\": 737,\n        \"samples\": [\n          -0.010591898111154092,\n          0.0015500539796250714,\n          -0.17767127052258894\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Winning Factor\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"HML\",\n          \"SMB\",\n          \"RMW\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature seek"
      ],
      "metadata": {
        "id": "KkFWbyO6XlRZ"
      },
      "id": "KkFWbyO6XlRZ"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF_feature_seek:\n",
        "    import itertools\n",
        "    import os\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # --------------------------\n",
        "    # Parameters for Feature & Training Window Search\n",
        "    # --------------------------\n",
        "    min_features = 2                  # minimum number of features in a subset\n",
        "    max_features = len(FEATURES)      # maximum number of features (or set to a smaller number if desired)\n",
        "\n",
        "    # Define fixed rolling window sizes (in years) to test (assuming monthly data)\n",
        "    training_window_years = [5, 10, 15, 20]\n",
        "\n",
        "    # Also run an expanding window experiment\n",
        "    run_expanding_window = True\n",
        "\n",
        "    # Independent variable: minimum number of observations required for making a prediction.\n",
        "    # This is now decoupled from the training window calculation.\n",
        "    min_obs_for_prediction = 60  # adjust this value as desired\n",
        "\n",
        "    output_filename = \"feature_subset_results.csv\"\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    # Ensure the data is sorted by date.\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # --------------------------\n",
        "    # Outer Loop: Fixed Rolling Window Modes\n",
        "    # --------------------------\n",
        "    for years in training_window_years:\n",
        "        # Convert years to number of observations (assume 12 obs per year)\n",
        "        rolling_window_size = years * 12\n",
        "        # Ensure predictions start only after both the rolling window and the independent minimum are met.\n",
        "        start_index = max(min_obs_for_prediction, rolling_window_size)\n",
        "        print(f\"\\n--- Testing fixed rolling window of {years} years \"\n",
        "              f\"({rolling_window_size} observations, starting predictions at index {start_index}) ---\")\n",
        "        outer_start_time = time.time()\n",
        "\n",
        "        # Inner loop over feature subset sizes\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            # Loop over all combinations of size r\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination: {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # Loop over test rows, starting when we have enough training data\n",
        "                for i in range(start_index, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    # Build fixed rolling training window (most recent rolling_window_size observations)\n",
        "                    train_window = df_sorted.iloc[i - rolling_window_size : i].copy()\n",
        "\n",
        "                    # Ensure the last training observation is strictly before test row date\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    # (Optional) Regime check if use_regime_split is True:\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                        if insufficient_regimes:\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "\n",
        "                    # Prepare training data for the current feature subset.\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    # Train the RandomForest model.\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    # Use the last row of the training window as test data.\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    # Map the predicted probabilities to the full set of FACTORS.\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    # months_ahead: how many months ahead the prediction is (optional usage)\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    # Collect feature levels for logging\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    # Create a result row\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": years,   # <--- Record the training window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                # End of inner test row loop for this feature combination.\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Results for combination {current_features} appended to CSV. \"\n",
        "                          f\"Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed fixed rolling window of {years} years in {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Expanding Window Mode\n",
        "    # --------------------------\n",
        "    if run_expanding_window:\n",
        "        print(\"\\n--- Testing Expanding Window Mode ---\")\n",
        "        outer_start_time = time.time()\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination (expanding): {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # In expanding mode, the training window goes from the start until the test row.\n",
        "                # Start predictions only after the minimum observation threshold is met.\n",
        "                for i in range(min_obs_for_prediction, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "                    train_window = df_sorted.iloc[:i].copy()\n",
        "                    if train_window.empty:\n",
        "                        continue\n",
        "\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": \"expanding\",  # <--- Indicate expanding window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Expanding window: Results for combination {current_features} \"\n",
        "                          f\"appended to CSV. Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed Expanding Window Mode in {minutes:02d}:{seconds:02d}\")\n"
      ],
      "metadata": {
        "id": "kraj1YkNhEq4"
      },
      "id": "kraj1YkNhEq4",
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seek all"
      ],
      "metadata": {
        "id": "QkeGs9s9dExF"
      },
      "id": "QkeGs9s9dExF"
    },
    {
      "cell_type": "code",
      "source": [
        "if seek_all: #updated\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import time\n",
        "    from itertools import product, combinations\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from datetime import datetime\n",
        "\n",
        "    use_regime_split = False\n",
        "    min_months_train = 60\n",
        "    min_obs_regime = 50\n",
        "    min_obs_train = 0\n",
        "    use_fixed_window = True\n",
        "\n",
        "    min_features = len(FEATURES)\n",
        "    max_features = len(FEATURES)\n",
        "\n",
        "    training_windows_list = [60]\n",
        "\n",
        "    hyperparameter_grid = {\n",
        "        \"n_estimators\": [100],\n",
        "        \"max_depth\": [None],\n",
        "        \"max_features\": ['sqrt'],\n",
        "        \"min_samples_split\": [2],\n",
        "        \"min_samples_leaf\": [1],\n",
        "        \"bootstrap\": [False]\n",
        "    }\n",
        "\n",
        "    hyperparameter_combinations = [\n",
        "        dict(zip(hyperparameter_grid.keys(), values))\n",
        "        for values in product(*hyperparameter_grid.values())\n",
        "    ]\n",
        "\n",
        "    feature_combinations = [\n",
        "        list(combo)\n",
        "        for r in range(min_features, max_features + 1)\n",
        "        for combo in combinations(FEATURES, r)\n",
        "    ]\n",
        "\n",
        "    total_iterations = len(training_windows_list) * len(feature_combinations) * len(hyperparameter_combinations)\n",
        "    print(f\"Total iterations to run: {total_iterations}\")\n",
        "\n",
        "    log_entries = []\n",
        "    iteration_results_log = []\n",
        "    iteration_number = 0\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    for train_window_size in training_windows_list:\n",
        "        for current_features in feature_combinations:\n",
        "            for rf_params in hyperparameter_combinations:\n",
        "                iteration_number += 1\n",
        "                iter_start_time = time.time()\n",
        "                results = []\n",
        "\n",
        "                for i in range(1, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    if use_fixed_window:\n",
        "                        start_idx = max(0, i - train_window_size)\n",
        "                        train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "                    else:\n",
        "                        train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "                    if len(train_window) < min_months_train:\n",
        "                        continue\n",
        "\n",
        "                    train_start_date = train_window['Date'].iloc[0]\n",
        "                    train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        if any(regime_counts < min_obs_regime):\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "                        regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                    else:\n",
        "                        regime_used = 'NoRegime'\n",
        "\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[current_features].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < min_obs_train:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "                        rf_model.fit(X_train, y_train)\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    X_test = train_window[current_features].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    try:\n",
        "                        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    except:\n",
        "                        continue\n",
        "\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "                    equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "                    results.append({\n",
        "                        'Predicted_month': Predicted_month,\n",
        "                        'Allocated_Return': allocated_return,\n",
        "                        'Equal_Weight_Return': equal_weight_return,\n",
        "                        'Train_Start_Date': train_start_date,\n",
        "                        'Train_End_Date': train_end_date\n",
        "                    })\n",
        "\n",
        "                results_df = pd.DataFrame(results)\n",
        "                if results_df.empty:\n",
        "                    cum_return_allocated = np.nan\n",
        "                    cum_return_equal = np.nan\n",
        "                    cum_return_allocated_total = np.nan\n",
        "                    cum_return_equal_total = np.nan\n",
        "                    first_pred_date = None\n",
        "                    last_pred_date = None\n",
        "                    winning_months_str = None\n",
        "                    sharpe_ratio = np.nan\n",
        "                    win_months = np.nan\n",
        "                    total_months = np.nan\n",
        "                else:\n",
        "                    # NEW: Actual prediction date range\n",
        "                    first_pred_date = results_df.iloc[0]['Predicted_month']\n",
        "                    last_pred_date = results_df.iloc[-1]['Predicted_month']\n",
        "\n",
        "                    # Performance from 2000 onward\n",
        "                    filtered = results_df[results_df['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "                    if not filtered.empty:\n",
        "                        cum_return_allocated = (1 + filtered['Allocated_Return']).prod() - 1\n",
        "                        cum_return_equal = (1 + filtered['Equal_Weight_Return']).prod() - 1\n",
        "                        win_months = (filtered['Allocated_Return'] > filtered['Equal_Weight_Return']).sum()\n",
        "                        total_months = len(filtered)\n",
        "                        winning_months_str = f\"{win_months}/{total_months}\"\n",
        "                        std_alloc = filtered['Allocated_Return'].std()\n",
        "                        sharpe_ratio = filtered['Allocated_Return'].mean() / std_alloc if std_alloc != 0 else np.nan\n",
        "                    else:\n",
        "                        cum_return_allocated = np.nan\n",
        "                        cum_return_equal = np.nan\n",
        "                        winning_months_str = None\n",
        "                        sharpe_ratio = np.nan\n",
        "                        win_months = np.nan\n",
        "                        total_months = np.nan\n",
        "\n",
        "                    cum_return_allocated_total = (1 + results_df['Allocated_Return']).prod() - 1\n",
        "                    cum_return_equal_total = (1 + results_df['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "                iter_time = time.time() - iter_start_time\n",
        "                time_str = f\"{int(iter_time // 60):02d}:{int(iter_time % 60):02d}\"\n",
        "\n",
        "                iteration_results_log.append({\n",
        "                    'Iteration': iteration_number,\n",
        "                    'Cum_Return_Total': cum_return_allocated_total,\n",
        "                    'Cum_Return_Post2000': cum_return_allocated,\n",
        "                    'Sharpe_Ratio_Post2000': sharpe_ratio,\n",
        "                    'Win_Months_Count': win_months\n",
        "                })\n",
        "\n",
        "                valid_total = [x['Cum_Return_Total'] for x in iteration_results_log if pd.notna(x['Cum_Return_Total'])]\n",
        "                valid_post2000 = [x['Cum_Return_Post2000'] for x in iteration_results_log if pd.notna(x['Cum_Return_Post2000'])]\n",
        "                valid_sharpe = [x['Sharpe_Ratio_Post2000'] for x in iteration_results_log if pd.notna(x['Sharpe_Ratio_Post2000'])]\n",
        "                valid_win = [x['Win_Months_Count'] for x in iteration_results_log if pd.notna(x.get('Win_Months_Count'))]\n",
        "\n",
        "                total_rank = sorted(valid_total, reverse=True).index(cum_return_allocated_total) + 1 if pd.notna(cum_return_allocated_total) else None\n",
        "                post2000_rank = sorted(valid_post2000, reverse=True).index(cum_return_allocated) + 1 if pd.notna(cum_return_allocated) else None\n",
        "                sharpe_rank = sorted(valid_sharpe, reverse=True).index(sharpe_ratio) + 1 if pd.notna(sharpe_ratio) else None\n",
        "                win_months_rank = sorted(valid_win, reverse=True).index(win_months) + 1 if pd.notna(win_months) else None\n",
        "\n",
        "                log_entry = {\n",
        "                    'Iteration': iteration_number,\n",
        "                    'Training_Window': train_window_size,\n",
        "                    'Features': ','.join(current_features),\n",
        "                    'Hyperparameters': ','.join(f\"{k}={v}\" for k, v in rf_params.items()),\n",
        "                    'First_Prediction_Date': first_pred_date,\n",
        "                    'Last_Prediction_Date': last_pred_date,\n",
        "                    'Cumulative_Return_Post2000': cum_return_allocated,\n",
        "                    'Cumulative_Return_Total': cum_return_allocated_total,\n",
        "                    'Cumulative_Return_Equal_Post2000': cum_return_equal,\n",
        "                    'Cumulative_Return_Equal_Total': cum_return_equal_total,\n",
        "                    'Winning_Months': winning_months_str,\n",
        "                    'Winning_Months_Rank': win_months_rank,\n",
        "                    'Sharpe_Ratio_Post2000': sharpe_ratio\n",
        "                }\n",
        "                log_entries.append(log_entry)\n",
        "\n",
        "                # UPDATED: Print first and last prediction dates\n",
        "                print(f\"Iteration {iteration_number}/{total_iterations} | Duration: {time_str}\")\n",
        "                print(f\"Prediction Dates: {first_pred_date.date() if first_pred_date else 'NA'} → {last_pred_date.date() if last_pred_date else 'NA'}\")\n",
        "                print(f\"Total Cum Return (Allocated): {cum_return_allocated_total:.4f} | Post-2000: {cum_return_allocated:.4f}\")\n",
        "                print(f\"Total Cum Return (Equal): {cum_return_equal_total:.4f} | Post-2000: {cum_return_equal:.4f}\")\n",
        "                print(f\"Winning Months (Allocated > Equal): {winning_months_str} | Winning Months Rank: {win_months_rank}\")\n",
        "                print(f\"Sharpe Ratio (Post-2000): {sharpe_ratio:.4f} | Sharpe Rank: {sharpe_rank}\")\n",
        "                print(f\"Rank (Total): {total_rank} | Rank (Post-2000): {post2000_rank}\\n\")\n",
        "\n",
        "    log_df = pd.DataFrame(log_entries)\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
        "    filename = f\"seek_all_results({timestamp}).csv\"\n",
        "    log_df.to_csv(filename, sep=\";\", index=False)\n",
        "    print(f\"Logged all results to '{filename}'.\")"
      ],
      "metadata": {
        "id": "b_ac9qZ7ir9I"
      },
      "id": "b_ac9qZ7ir9I",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "MrSJ4xhmDuzE"
      },
      "id": "MrSJ4xhmDuzE"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Min obs per regime if splitting\n",
        "    min_obs_train = 0         # Min total obs after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # New toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    # -------------------\n",
        "    # New: Tunable hyperparameter for parallel jobs\n",
        "    # -------------------\n",
        "    n_jobs = -1  # Set to -1 to use all available cores; adjust as needed\n",
        "\n",
        "    # -------------------\n",
        "    # Hyperparameter Settings for Random Forest\n",
        "    # -------------------\n",
        "    if default_hyperparameters:\n",
        "        # DEFAULTS!!!!\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,      # Number of trees in the forest\n",
        "            'max_depth': None,        # Maximum depth of each tree (None = no limit)\n",
        "            'max_features': 'sqrt',   # Number of features to consider when looking for the best split\n",
        "            'min_samples_split': 2,   # Minimum number of samples required to split an internal node\n",
        "            'min_samples_leaf': 1,    # Minimum number of samples required to be at a leaf node\n",
        "            'bootstrap': True,        # Whether bootstrap samples are used when building trees\n",
        "            'n_jobs': n_jobs          # Tunable: Number of parallel jobs\n",
        "        }\n",
        "    else:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,      # Number of trees in the forest\n",
        "            'max_depth': None,        # Maximum depth of each tree (None = no limit)\n",
        "            'max_features': None,     # Number of features to consider when looking for the best split\n",
        "            'min_samples_split': 2,   # Minimum number of samples required to split an internal node\n",
        "            'min_samples_leaf': 5,    # Minimum number of samples required to be at a leaf node\n",
        "            'bootstrap': False,       # Whether bootstrap samples are used when building trees\n",
        "            'n_jobs': n_jobs          # Tunable: Number of parallel jobs\n",
        "        }\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either a fixed-size (rolling) window or an expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check that we have enough training rows (i.e., months)\n",
        "        if len(train_window) < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training rows ({len(train_window)} rows). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last date in training window\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  🔴 Regime split active. Insufficient data in: {regime_str}. Skipping.\\n\")\n",
        "                continue\n",
        "\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  🔴 Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\\n\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Check that the last training date is strictly before the test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Fit RandomForest using the hyperparameter settings\n",
        "        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict using the last row in training window (previous month’s features)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\\n\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "        predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "        # Map probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "            try:\n",
        "                idx = FACTORS.index(cls)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # Calculate the strategy returns:\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Gather additional info\n",
        "        tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "        avg_depth = np.mean(tree_depths)\n",
        "        max_depth_val = np.max(tree_depths)\n",
        "        months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month))\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth_val,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for RF\n",
        "    # -------------------\n",
        "    results_df_rf = pd.DataFrame(results)\n",
        "    print(\"Final results_df_rf columns:\", results_df_rf.columns.tolist())\n",
        "    display(results_df_rf.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # 4) Calculate and Print Cumulative Returns (Filtered: from 1 Jan 2000 onwards)\n",
        "    # -------------------\n",
        "    filtered_results = results_df_rf[results_df_rf['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered_results.empty:\n",
        "        cum_return_allocated = (1 + filtered_results['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal = (1 + filtered_results['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_pred_month = filtered_results.iloc[0]['Predicted_month']\n",
        "        last_pred_month = filtered_results.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_pred_month.date(), last_pred_month.date(),\n",
        "            cum_return_allocated, cum_return_equal))\n",
        "    else:\n",
        "        print(\"No predictions from 1 Jan 2000 onwards.\")\n",
        "\n",
        "    # -------------------\n",
        "    # 5) Calculate and Print Cumulative Returns for Total Time\n",
        "    # -------------------\n",
        "    if not results_df_rf.empty:\n",
        "        cum_return_allocated_total = (1 + results_df_rf['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal_total = (1 + results_df_rf['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_total_month = results_df_rf.iloc[0]['Predicted_month']\n",
        "        last_total_month = results_df_rf.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_total_month.date(), last_total_month.date(),\n",
        "            cum_return_allocated_total, cum_return_equal_total))\n",
        "    else:\n",
        "        print(\"No predictions available for total time.\")\n"
      ],
      "metadata": {
        "id": "YjHj_tiKSCYZ"
      },
      "id": "YjHj_tiKSCYZ",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient boosting\n"
      ],
      "metadata": {
        "id": "MSWv9xFlDbMz"
      },
      "id": "MSWv9xFlDbMz"
    },
    {
      "cell_type": "code",
      "source": [
        "if GB or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Minimum observations per regime if splitting\n",
        "    min_obs_train = 0         # Minimum total observations after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # Toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either fixed-size (rolling) or expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "            # Ensure fixed window has the full required length\n",
        "            if len(train_window) < rolling_window_size:\n",
        "                print(f\"Test row date: {Predicted_month.date()} - Insufficient fixed window length ({len(train_window)} rows), required: {rolling_window_size}. Skipping.\")\n",
        "                continue\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check training period in months\n",
        "        start_date = train_window['Date'].iloc[0]\n",
        "        training_months = (Predicted_month.year - start_date.year) * 12 + (Predicted_month.month - start_date.month)\n",
        "        if training_months < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training period ({training_months} months). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last training dates\n",
        "        train_start_date = start_date\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks (if enabled)\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active. Insufficient data in: {regime_str}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Use only training data for the current regime\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure the last training date is strictly before the test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Convert y_train from strings to numeric codes and save mapping\n",
        "        y_train_cat = y_train.astype('category')\n",
        "        mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "        y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "        # -------------------\n",
        "        # Set hyperparameters based on default_hyperparameters flag\n",
        "        # -------------------\n",
        "        if default_hyperparameters:\n",
        "            xgb_params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 3,\n",
        "                'learning_rate': 0.1,\n",
        "                'subsample': 1.0,\n",
        "                'colsample_bytree': 1.0,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss'\n",
        "            }\n",
        "        else:\n",
        "            # Use manually defined hyperparameters (customize as needed)\n",
        "            xgb_params = {\n",
        "                'n_estimators': 500,\n",
        "                'max_depth': 10,\n",
        "                'learning_rate': 0.01,\n",
        "                'subsample': 1,\n",
        "                'colsample_bytree': 1,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss'\n",
        "            }\n",
        "\n",
        "        # Fit XGBoost gradient boosting classifier on numeric labels (full training, no early stopping)\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X_train, y_train_numeric)\n",
        "\n",
        "        # Prepare test data (using the last row in the training window)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = xgb_model.predict_proba(X_test)[0]\n",
        "        # Get predicted numeric class and convert back to original factor name\n",
        "        predicted_numeric = xgb_model.classes_[predicted_probabilities.argmax()]\n",
        "        predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "        # Map predicted probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for code, prob in zip(xgb_model.classes_, predicted_probabilities):\n",
        "            factor_name = mapping[code]\n",
        "            try:\n",
        "                idx = FACTORS.index(factor_name)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass  # Skip if factor not found in FACTORS\n",
        "\n",
        "        # Compute allocated return using the test row's factor returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "        # Tree depth statistics are not required, so we set them to None\n",
        "        avg_depth = None\n",
        "        max_depth = None\n",
        "\n",
        "        # Calculate prediction horizon (months ahead)\n",
        "        months_ahead = (Predicted_month.year - last_train_date.year) * 12 + (Predicted_month.month - last_train_date.month)\n",
        "\n",
        "        # Store the actual feature levels used in X_test\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        # Build the result dictionary for this iteration\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': xgb_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': xgb_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for GB\n",
        "    # -------------------\n",
        "    results_df_gb = pd.DataFrame(results)\n",
        "    print(\"Final results_df_gb columns:\", results_df_gb.columns.tolist())\n",
        "    display(results_df_gb.tail(10))\n"
      ],
      "metadata": {
        "id": "9sWAd_BnDVlB"
      },
      "id": "9sWAd_BnDVlB",
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if gb_loop:\n",
        "  import optuna\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  from xgboost import XGBClassifier\n",
        "\n",
        "  # ----- Assumed Predefined Variables -----\n",
        "  # df_sorted: your DataFrame sorted by 'Date'\n",
        "  # FEATURES: list of feature column names\n",
        "  # FACTORS: list of factor names (which are also columns in df)\n",
        "  # REGIMES_COLUMN: name of the regime column if using regime splitting (else not used)\n",
        "  # regime_short_mapping: dict mapping regime values to short strings\n",
        "  # min_months_train, min_obs_regime, min_obs_train, use_regime_split, use_fixed_window, rolling_window_size\n",
        "  # (set these variables as in your provided code snippet)\n",
        "\n",
        "  # For example, you might have:\n",
        "  # min_months_train = 60\n",
        "  # rolling_window_size = 60\n",
        "  # use_fixed_window = True\n",
        "  # use_regime_split = False\n",
        "\n",
        "  def objective(trial):\n",
        "      # Define hyperparameters for this trial:\n",
        "      xgb_params = {\n",
        "          'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "          'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "          'learning_rate': trial.suggest_loguniform('learning_rate', 0.005, 0.2),\n",
        "          'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
        "          'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
        "          'random_state': 42,\n",
        "          'eval_metric': 'mlogloss'\n",
        "      }\n",
        "\n",
        "      cumulative_return = 0.0\n",
        "\n",
        "      # Loop over test rows in df_sorted starting from 1.1.2000 onward\n",
        "      for i in range(1, len(df_sorted)):\n",
        "          test_row = df_sorted.iloc[i]\n",
        "          Predicted_month = test_row['Date']\n",
        "          # Only process dates on or after January 1, 2000:\n",
        "          if Predicted_month < pd.Timestamp('2000-01-01'):\n",
        "              continue\n",
        "\n",
        "          # Build training window: fixed (rolling) or expanding\n",
        "          if use_fixed_window:\n",
        "              start_idx = max(0, i - rolling_window_size)\n",
        "              train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "              if len(train_window) < rolling_window_size:\n",
        "                  print(f\"Test row {Predicted_month.date()} - Insufficient window ({len(train_window)} rows), skipping.\")\n",
        "                  continue\n",
        "          else:\n",
        "              train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "          # Check that the training period is long enough\n",
        "          start_date = train_window['Date'].iloc[0]\n",
        "          training_months = (Predicted_month.year - start_date.year) * 12 + (Predicted_month.month - start_date.month)\n",
        "          if training_months < min_months_train:\n",
        "              print(f\"Test row {Predicted_month.date()} - Training period too short ({training_months} months), skipping.\")\n",
        "              continue\n",
        "\n",
        "          # (Optional) If using regime-based splits:\n",
        "          if use_regime_split:\n",
        "              regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "              insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "              if insufficient_regimes:\n",
        "                  print(f\"Test row {Predicted_month.date()} - Insufficient regime data, skipping.\")\n",
        "                  continue\n",
        "              current_regime = test_row[REGIMES_COLUMN]\n",
        "              train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "              if len(train_window) < min_obs_regime:\n",
        "                  print(f\"Test row {Predicted_month.date()} - Not enough observations in regime, skipping.\")\n",
        "                  continue\n",
        "              regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "          else:\n",
        "              regime_used = 'NoRegime'\n",
        "\n",
        "          # Ensure the last training date is strictly before the test date\n",
        "          last_train_date = train_window['Date'].iloc[-1]\n",
        "          if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "              print(f\"Test row {Predicted_month.date()} - Last training date not before test date, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # Prepare training data\n",
        "          X_train = train_window[FEATURES].dropna()\n",
        "          y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "          if len(X_train) < min_obs_train:\n",
        "              print(f\"Test row {Predicted_month.date()} - Insufficient training observations after dropna, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # Convert categorical target to numeric codes\n",
        "          y_train_cat = y_train.astype('category')\n",
        "          mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "          y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "          # Train the XGBoost model with the trial's hyperparameters\n",
        "          xgb_model = XGBClassifier(**xgb_params)\n",
        "          xgb_model.fit(X_train, y_train_numeric)\n",
        "\n",
        "          # Prepare test data: use the last row from the training window as test features\n",
        "          X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "          if X_test.empty:\n",
        "              print(f\"Test row {Predicted_month.date()} - Test features empty, skipping.\")\n",
        "              continue\n",
        "\n",
        "          # Get prediction probabilities and determine the predicted winning factor\n",
        "          predicted_probabilities = xgb_model.predict_proba(X_test)[0]\n",
        "          predicted_numeric = xgb_model.classes_[predicted_probabilities.argmax()]\n",
        "          predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "          # Map the predicted probabilities onto the full set of FACTORS\n",
        "          full_probs = np.zeros(len(FACTORS))\n",
        "          for code, prob in zip(xgb_model.classes_, predicted_probabilities):\n",
        "              factor_name = mapping[code]\n",
        "              try:\n",
        "                  idx = FACTORS.index(factor_name)\n",
        "                  full_probs[idx] = prob\n",
        "              except ValueError:\n",
        "                  continue\n",
        "\n",
        "          # Compute allocated return as the weighted sum of the test row’s factor returns\n",
        "          allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "          cumulative_return += allocated_return\n",
        "\n",
        "          # Print progress for this test row\n",
        "          print(f\"Test row {Predicted_month.date()} -> Allocated Return: {allocated_return:.4f}, Cumulative Return: {cumulative_return:.4f}\")\n",
        "\n",
        "      # End of trial: print and return the cumulative return achieved\n",
        "      print(f\"Trial finished with Cumulative Return: {cumulative_return:.4f}\")\n",
        "      return cumulative_return\n",
        "\n",
        "  # Define a callback to print trial info after each trial completes\n",
        "  def print_trial_info(study, trial):\n",
        "      print(f\"Trial {trial.number} finished with value: {trial.value:.4f}. Current best: {study.best_value:.4f}\")\n",
        "\n",
        "  # Create and run the Optuna study\n",
        "  study = optuna.create_study(direction='maximize')\n",
        "  study.optimize(objective, n_trials=30, callbacks=[print_trial_info])\n"
      ],
      "metadata": {
        "id": "K6JoZwTdQ8lL",
        "outputId": "ad76ac5f-12ca-4ef2-a923-aeff4331890e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "id": "K6JoZwTdQ8lL",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'optuna'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-cdc1e076ea96>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mgb_loop\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0;32mimport\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'optuna'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid / tää säilyttää random forestin dataframen mut averagee painot ja laskee allocated returns nistä"
      ],
      "metadata": {
        "id": "0HZuBVFOW8qu"
      },
      "id": "0HZuBVFOW8qu"
    },
    {
      "cell_type": "code",
      "source": [
        "if Hybrid:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "\n",
        "    # Ensure that RF and GB results exist before merging\n",
        "    if 'results_df_rf' in locals() and 'results_df_gb' in locals():\n",
        "        # Create temporary DataFrames containing only the predicted weights from RF and GB\n",
        "        rf_subset = results_df_rf[['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "        gb_subset = results_df_gb[['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "\n",
        "        # Merge on Predicted_month\n",
        "        hybrid_temp = pd.merge(\n",
        "            rf_subset,\n",
        "            gb_subset,\n",
        "            on='Predicted_month',\n",
        "            suffixes=('_rf', '_gb')\n",
        "        )\n",
        "\n",
        "        # Compute the average predicted probabilities (ensemble)\n",
        "        hybrid_temp['Predicted_Probabilities'] = hybrid_temp.apply(\n",
        "            lambda row: (np.array(row['Predicted_Probabilities_rf']) + np.array(row['Predicted_Probabilities_gb'])) / 2,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Function to compute allocated return given a probability vector and factor returns\n",
        "        def compute_allocated_return(prob_vector, factor_returns):\n",
        "            return np.dot(prob_vector, factor_returns)\n",
        "\n",
        "        # Compute the allocated return for each row using the hybrid weights\n",
        "        hybrid_temp['Hybrid_Allocated_Return'] = hybrid_temp.apply(\n",
        "            lambda row: compute_allocated_return(\n",
        "                row['Predicted_Probabilities'],\n",
        "                df.loc[df['Date'] == row['Predicted_month'], FACTORS].values.flatten()\n",
        "            ) if not df.loc[df['Date'] == row['Predicted_month'], FACTORS].empty else np.nan,\n",
        "            axis=1\n",
        "        )\n",
        "\n",
        "        # Now merge the new hybrid values back into the full RF results so we keep all the RF columns\n",
        "        hybrid_df = results_df_rf.merge(\n",
        "            hybrid_temp[['Predicted_month', 'Predicted_Probabilities', 'Hybrid_Allocated_Return']],\n",
        "            on='Predicted_month',\n",
        "            how='left'\n",
        "        )\n",
        "\n",
        "        # Override the RF predicted probabilities and allocated return with the hybrid ones\n",
        "        # (the merge will create 'Predicted_Probabilities_x' from RF and 'Predicted_Probabilities_y' from hybrid_temp)\n",
        "        hybrid_df['Predicted_Probabilities'] = hybrid_df['Predicted_Probabilities_y']\n",
        "        hybrid_df['Allocated_Return'] = hybrid_df['Hybrid_Allocated_Return']\n",
        "\n",
        "        # Drop the temporary columns from the merge\n",
        "        hybrid_df.drop(columns=['Predicted_Probabilities_x', 'Predicted_Probabilities_y', 'Hybrid_Allocated_Return'], inplace=True)\n",
        "\n",
        "        print(\"Hybrid hybrid_df created:\")\n",
        "        display(hybrid_df.head())\n",
        "    else:\n",
        "        raise ValueError(\"Error: Both Random Forest and Gradient Boosting models must be run before Hybrid mode can be computed.\")\n"
      ],
      "metadata": {
        "id": "JKYtkk2MWPC0"
      },
      "id": "JKYtkk2MWPC0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model evaluation"
      ],
      "metadata": {
        "id": "A1BqWL_QXA8e"
      },
      "id": "A1BqWL_QXA8e"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF:\n",
        "    results_df = results_df_rf.copy()\n",
        "    print(\"Results from Random Forest assigned to results_df.\")\n",
        "\n",
        "elif GB:\n",
        "    results_df = results_df_gb.copy()\n",
        "    print(\"Results from Gradient Boosting assigned to results_df.\")\n",
        "\n",
        "elif Hybrid:\n",
        "    results_df = hybrid_df.copy()\n",
        "    print(\"Results from Hybrid Model assigned to results_df.\")\n",
        "\n",
        "else:\n",
        "    raise ValueError(\"Error: No valid model was selected. Ensure one of [RF, GB, Hybrid] is True.\")\n",
        "\n",
        "# Display the first few rows of the final results_df\n",
        "display(results_df.head())\n"
      ],
      "metadata": {
        "id": "FOzCGJrxXKUm"
      },
      "id": "FOzCGJrxXKUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Increase column width so no text is truncated\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# 1) Convert 'Predicted_month' to datetime if not already\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "\n",
        "# 2) Define the date range\n",
        "start_date = pd.to_datetime('1968-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# 3) Filter the DataFrame\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "].copy().sort_values('Predicted_month')\n",
        "\n",
        "# 4) Display the table with full column text\n",
        "display(filtered_results_df)\n",
        "\n",
        "# (Optional) Reset column width to default after display\n",
        "pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "id": "JmpzyRpGWP0n"
      },
      "id": "JmpzyRpGWP0n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88d0f495-1290-403b-86a2-7bd1c2c12814",
      "metadata": {
        "tags": [],
        "id": "88d0f495-1290-403b-86a2-7bd1c2c12814"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ----- Part A: Confusion Matrix & Overall Performance Metrics -----\n",
        "\n",
        "# Use FACTORS directly as labels\n",
        "labels = FACTORS\n",
        "\n",
        "# Extract actual and predicted winners\n",
        "all_true = results_df['Actual_Winner']\n",
        "all_pred = results_df['Predicted_Winner']\n",
        "\n",
        "# Get unique regimes if regime split is active\n",
        "if use_regime_split:\n",
        "    all_regimes = results_df['Regime'].unique()\n",
        "else:\n",
        "    all_regimes = []\n",
        "\n",
        "# Set number of subplots: one overall plus one per regime if needed\n",
        "num_cols = 1 if not use_regime_split else len(all_regimes) + 1\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_cols, figsize=(8 if num_cols == 1 else 19.5, 8))\n",
        "if num_cols == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "# 1. Overall confusion matrix\n",
        "cm_total = confusion_matrix(all_true, all_pred, labels=labels)\n",
        "sns.heatmap(cm_total, annot=True, fmt='d', cmap=\"Blues\",\n",
        "            xticklabels=labels, yticklabels=labels, ax=axes[0])\n",
        "axes[0].set_xlabel(\"Predicted Labels\")\n",
        "axes[0].set_ylabel(\"True Labels\")\n",
        "total_samples = len(all_true)\n",
        "axes[0].set_title(f\"Overall Confusion Matrix\\n({total_samples} samples)\")\n",
        "\n",
        "# Prepare a list for metrics summary\n",
        "metrics_summary = []\n",
        "\n",
        "# Overall metrics\n",
        "overall_metrics = {\n",
        "    \"Regime\": \"Overall\",\n",
        "    \"Accuracy\": accuracy_score(all_true, all_pred),\n",
        "    \"Precision\": precision_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"Recall\": recall_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"F1 Score\": f1_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "    \"Samples\": total_samples\n",
        "}\n",
        "metrics_summary.append(overall_metrics)\n",
        "\n",
        "# 2. Per-regime confusion matrices and metrics (if regime split is active)\n",
        "if use_regime_split:\n",
        "    for i, regime in enumerate(all_regimes):\n",
        "        regime_mask = (results_df['Regime'] == regime)\n",
        "        regime_true = all_true[regime_mask]\n",
        "        regime_pred = all_pred[regime_mask]\n",
        "\n",
        "        if len(regime_true) == 0:\n",
        "            print(f\"\\nNo samples for regime '{regime}'. Skipping confusion matrix.\")\n",
        "            continue\n",
        "\n",
        "        cm_regime = confusion_matrix(regime_true, regime_pred, labels=labels)\n",
        "        sns.heatmap(cm_regime, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                    xticklabels=labels, yticklabels=labels, ax=axes[i+1])\n",
        "        axes[i+1].set_xlabel(\"Predicted Labels\")\n",
        "        axes[i+1].set_ylabel(\"True Labels\")\n",
        "        axes[i+1].set_title(f\"{regime} Regime\\n({len(regime_true)} samples)\")\n",
        "\n",
        "        regime_metrics = {\n",
        "            \"Regime\": regime,\n",
        "            \"Accuracy\": accuracy_score(regime_true, regime_pred),\n",
        "            \"Precision\": precision_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"Recall\": recall_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"F1 Score\": f1_score(regime_true, regime_pred, average='weighted', zero_division=0),\n",
        "            \"Samples\": len(regime_true)\n",
        "        }\n",
        "        metrics_summary.append(regime_metrics)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(wspace=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Convert overall metrics summary to DataFrame and round values\n",
        "metrics_df = pd.DataFrame(metrics_summary)\n",
        "metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']] = metrics_df[['Accuracy', 'Precision', 'Recall', 'F1 Score']].round(4)\n",
        "\n",
        "# Build HTML table for overall performance metrics\n",
        "html_overall = f\"\"\"\n",
        "<h3>Overall Performance Metrics Summary</h3>\n",
        "<table border=\"1\" cellpadding=\"5\">\n",
        "    <tr>\n",
        "        <th>Regime</th>\n",
        "        <th>Accuracy</th>\n",
        "        <th>Precision</th>\n",
        "        <th>Recall</th>\n",
        "        <th>F1 Score</th>\n",
        "        <th>Samples</th>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "for _, row in metrics_df.iterrows():\n",
        "    html_overall += f\"\"\"\n",
        "    <tr>\n",
        "        <td>{row['Regime']}</td>\n",
        "        <td>{row['Accuracy']}</td>\n",
        "        <td>{row['Precision']}</td>\n",
        "        <td>{row['Recall']}</td>\n",
        "        <td>{row['F1 Score']}</td>\n",
        "        <td>{row['Samples']}</td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "html_overall += \"</table>\"\n",
        "\n",
        "explanations = {\n",
        "    \"Accuracy\": \"Proportion of correct predictions.\",\n",
        "    \"Precision\": \"How many predicted positives were actually correct?\",\n",
        "    \"Recall\": \"How many actual positives were correctly predicted?\",\n",
        "    \"F1 Score\": \"Harmonic mean of precision & recall.\",\n",
        "    \"Samples\": \"Number of test samples in this regime.\"\n",
        "}\n",
        "html_overall += \"<h4>Metric Explanations:</h4><ul>\"\n",
        "for metric, desc in explanations.items():\n",
        "    html_overall += f\"<li><strong>{metric}:</strong> {desc}</li>\"\n",
        "html_overall += \"</ul>\"\n",
        "\n",
        "display(HTML(html_overall))\n",
        "\n",
        "\n",
        "# ----- Part B: Factor-Level Accuracy from Overall Confusion Matrix -----\n",
        "factor_data = []\n",
        "for i, factor in enumerate(labels):\n",
        "    predicted_count = cm_total[:, i].sum()  # sum of column i\n",
        "    correct_count = cm_total[i, i]         # diagonal entry\n",
        "    if predicted_count > 0:\n",
        "        factor_accuracy = correct_count / predicted_count * 100\n",
        "    else:\n",
        "        factor_accuracy = 0.0\n",
        "\n",
        "    factor_data.append({\n",
        "        \"Factor\": factor,\n",
        "        \"Predicted Count\": int(predicted_count),\n",
        "        \"Correct Count\": int(correct_count),\n",
        "        \"Factor Accuracy (%)\": round(factor_accuracy, 2)\n",
        "    })\n",
        "\n",
        "factor_df = pd.DataFrame(factor_data)\n",
        "\n",
        "html_factor = f\"\"\"\n",
        "<h3>Factor-Level Accuracy (From Overall Confusion Matrix)</h3>\n",
        "<table border=\"1\" cellpadding=\"5\">\n",
        "    <tr>\n",
        "        <th>Factor</th>\n",
        "        <th>Predicted Count</th>\n",
        "        <th>Correct Count</th>\n",
        "        <th>Factor Accuracy (%)</th>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "for _, row in factor_df.iterrows():\n",
        "    html_factor += f\"\"\"\n",
        "    <tr>\n",
        "        <td>{row['Factor']}</td>\n",
        "        <td>{row['Predicted Count']}</td>\n",
        "        <td>{row['Correct Count']}</td>\n",
        "        <td>{row['Factor Accuracy (%)']}%</td>\n",
        "    </tr>\n",
        "\"\"\"\n",
        "html_factor += \"</table>\"\n",
        "\n",
        "html_factor += \"\"\"\n",
        "<h4>Definition</h4>\n",
        "<ul>\n",
        "  <li><strong>Predicted Count:</strong> Sum of the corresponding column in the confusion matrix (times predicted this factor).</li>\n",
        "  <li><strong>Correct Count:</strong> Diagonal entry for this factor in the matrix (times predicted factor = actual factor).</li>\n",
        "  <li><strong>Factor Accuracy:</strong> (Correct Count / Predicted Count) * 100.</li>\n",
        "</ul>\n",
        "\"\"\"\n",
        "\n",
        "display(HTML(html_factor))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance"
      ],
      "metadata": {
        "id": "-U5ovbRVPoPM"
      },
      "id": "-U5ovbRVPoPM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5",
      "metadata": {
        "tags": [],
        "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5"
      },
      "outputs": [],
      "source": [
        "# --- Code cell 26 ---\n",
        "# 2. Regime-Specific Feature Importances (Dynamic Version)\n",
        "# ========================================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get the unique regimes from results_df (already converted to string names)\n",
        "existing_regimes = results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "\n",
        "# Dynamically get the number of features from the data\n",
        "n_features = len(results_df['Feature_Importances'].iloc[0])  # Actual feature count\n",
        "\n",
        "# Robust feature name handling:\n",
        "try:\n",
        "    # Validate FEATURES list length matches actual features\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"⚠️ Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features})\")\n",
        "        print(\"Using auto-generated feature names instead\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate descriptive feature names if there's a mismatch or error\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features\")\n",
        "\n",
        "# Compute overall average feature importances across all predictions\n",
        "overall_avg_fi = np.vstack(results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# Calculate regime-specific average feature importances\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = results_df[results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# Sort features by overall importance in descending order\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # bounds check\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# 1) If we have more than one unique regime, make an overall + multiple regime subplots\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One for overall, one per regime\n",
        "    # Figure height depends on number of plots and number of features\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]  # ensure iterable\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Plots ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 2) Otherwise, if there's only zero or one regime, show only the overall chart\n",
        "else:\n",
        "    total_plots = 1\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Provide extra space on the left for feature labels\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Allocation chart"
      ],
      "metadata": {
        "id": "bhRxmOoFPtEI"
      },
      "id": "bhRxmOoFPtEI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8",
      "metadata": {
        "tags": [],
        "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Convert \"Predicted_month\" to datetime (assuming the format is '%Y-%m'):\n",
        "probability_dates = pd.to_datetime(\n",
        "    results_df[\"Predicted_month\"],\n",
        "    format='%Y-%m'\n",
        ")\n",
        "\n",
        "# 2) Drop rows with unparseable dates if needed\n",
        "if probability_dates.isna().any():\n",
        "    print(\"Warning: Some dates could not be parsed. Dropping those rows.\")\n",
        "    results_df = results_df.loc[~probability_dates.isna()].copy()\n",
        "    probability_dates = probability_dates.dropna()\n",
        "\n",
        "# 3) Extract the probability arrays and build a DataFrame\n",
        "full_probs = np.vstack(results_df[\"Predicted_Probabilities\"].values)\n",
        "probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "probability_df[\"Date\"] = probability_dates\n",
        "probability_df = probability_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# 4) Check date range\n",
        "print(\"Date Range:\", probability_df[\"Date\"].min(), \"to\", probability_df[\"Date\"].max())\n",
        "\n",
        "# 5) Plot the probabilities in a stack plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.stackplot(\n",
        "    probability_df[\"Date\"],\n",
        "    [probability_df[col] for col in FACTORS],\n",
        "    labels=FACTORS,\n",
        "    alpha=0.8\n",
        ")\n",
        "\n",
        "plt.title(\"Outperforming Probabilities of Each Factor (ML Prediction)\", fontsize=14)\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.ylabel(\"Probability\", fontsize=12)\n",
        "\n",
        "# Manually set y-axis ticks at 0, 0.25, 0.5, 0.75, 1\n",
        "plt.ylim(0, 1)\n",
        "plt.yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "\n",
        "plt.legend(loc='upper left', fontsize='small', frameon=False)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Factor weight analysis"
      ],
      "metadata": {
        "id": "PY247n9MPyWG"
      },
      "id": "PY247n9MPyWG"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the date range for viewing\n",
        "start_date = \"1968-07-30\"\n",
        "end_date = \"2024-11-30\"\n",
        "\n",
        "# Ensure that the \"Date\" column is in datetime format if it's not already\n",
        "probability_df[\"Date\"] = pd.to_datetime(probability_df[\"Date\"])\n",
        "\n",
        "# Filter the dataframe to include only the rows between the set dates\n",
        "mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "filtered_df = probability_df.loc[mask]\n",
        "\n",
        "# Define the equal weight value (static equal weight for each factor)\n",
        "equal_weight = 1 / len(FACTORS)  # For example, if 5 factors then equal_weight = 0.20\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Loop over each factor in FACTORS and create a separate chart\n",
        "for factor in FACTORS:\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot the predicted probability for the current factor using the filtered data\n",
        "    plt.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "             label=f\"{factor} Predicted Probability\", color='blue', linewidth=2)\n",
        "\n",
        "    # Plot the static equal weight line\n",
        "    plt.axhline(equal_weight, color='black', linestyle='--',\n",
        "                label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "    # Shade the area where the predicted probability is above the equal weight (Overweight)\n",
        "    plt.fill_between(filtered_df[\"Date\"],\n",
        "                     filtered_df[factor],\n",
        "                     equal_weight,\n",
        "                     where=(filtered_df[factor] > equal_weight),\n",
        "                     interpolate=True, color='green', alpha=0.3, label='Overweight')\n",
        "\n",
        "    # Shade the area where the predicted probability is below the equal weight (Underweight)\n",
        "    plt.fill_between(filtered_df[\"Date\"],\n",
        "                     filtered_df[factor],\n",
        "                     equal_weight,\n",
        "                     where=(filtered_df[factor] < equal_weight),\n",
        "                     interpolate=True, color='red', alpha=0.3, label='Underweight')\n",
        "\n",
        "    # Set chart title and labels\n",
        "    plt.title(f\"Over/Under Weight for {factor}\")\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Probability\")\n",
        "    plt.legend(loc='best')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5My7U6ny7GAJ"
      },
      "id": "5My7U6ny7GAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Total outperforming probabilities"
      ],
      "metadata": {
        "id": "6wkhRaZrP6aF"
      },
      "id": "6wkhRaZrP6aF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "avg_probs = probability_df[FACTORS].mean()\n",
        "avg_probs_df = avg_probs.reset_index()\n",
        "avg_probs_df.columns = ['Factor', 'Average Probability']\n",
        "\n",
        "\n",
        "print(\"Average Outperforming Probabilities Over Time:\")\n",
        "print(avg_probs_df)"
      ],
      "metadata": {
        "id": "7GvhTSes34Ci"
      },
      "id": "7GvhTSes34Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Return data"
      ],
      "metadata": {
        "id": "M0sRKlsm6Yja"
      },
      "id": "M0sRKlsm6Yja"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Ensure 'Predicted_month' is datetime and merge results with df_sorted on Date\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "results_df_local = results_df.copy().merge(df_sorted, left_on='Predicted_month', right_on='Date', how='left')\n",
        "\n",
        "# Print row-match info\n",
        "print(f\"Initial results_df rows: {len(results_df)}\")\n",
        "print(f\"Initial df_sorted rows: {len(df_sorted)}\")\n",
        "print(f\"Matched rows: {results_df_local['Date'].notna().sum()}\")\n",
        "print(f\"Unmatched rows: {results_df_local['Date'].isna().sum()}\")\n",
        "\n",
        "# Define columns to keep (with 'Predicted_month' first)\n",
        "cols = ['Predicted_month', 'Allocated_Return', 'Equal_Weight_Return', 'Mkt', 'RF', 'Mkt-RF', 'Us_standard', 'Predicted_Winner'] + FACTORS\n",
        "results_df_local = results_df_local[[c for c in cols if c in results_df_local.columns]] \\\n",
        "                    .sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "display(results_df_local)\n",
        "print(\"\\nFirst date in 'Predicted_month':\", results_df['Predicted_month'].min())\n",
        "print(\"Last date in 'Predicted_month':\", results_df['Predicted_month'].max())\n"
      ],
      "metadata": {
        "id": "rA982P5EGhj4"
      },
      "id": "rA982P5EGhj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns table"
      ],
      "metadata": {
        "id": "E3Xi_BH1iwiv"
      },
      "id": "E3Xi_BH1iwiv"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Filter the DataFrame between specified dates\n",
        "start_date = pd.to_datetime('1950-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "df_filtered = results_df_local[\n",
        "    (results_df_local['Predicted_month'] >= start_date) &\n",
        "    (results_df_local['Predicted_month'] <= end_date)\n",
        "].copy()\n",
        "\n",
        "# Rename columns if present\n",
        "rename_dict = {}\n",
        "if 'Allocated_Return' in df_filtered.columns:\n",
        "    rename_dict['Allocated_Return'] = 'ML Allocated Strategy Return'\n",
        "if BENCHMARK[0] in df_filtered.columns:\n",
        "    rename_dict[BENCHMARK[0]] = 'Benchmark Return'\n",
        "df_filtered.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Calculate equal-weighted returns based on FACTORS (e.g. ['SMB','HML','CMA','RMW'])\n",
        "factors_exRF = [f for f in FACTORS if f in df_filtered.columns]\n",
        "if 'RF' in df_filtered.columns:\n",
        "    factors_inRF = factors_exRF + ['RF']\n",
        "    df_filtered['Equal Factor Weight Strategy Return ExRF']  = df_filtered[factors_exRF].mean(axis=1)\n",
        "    df_filtered['Equal Factor Weight Strategy Return InclRF'] = df_filtered[factors_inRF].mean(axis=1)\n",
        "else:\n",
        "    df_filtered['Equal Factor Weight Strategy Return'] = df_filtered[factors_exRF].mean(axis=1)\n",
        "\n",
        "# ===== NEW CODE: 50% weight on predicted winner =====\n",
        "def calc_winner_strategy(row):\n",
        "    pred = row['Predicted_Winner']\n",
        "    if pred in factors_exRF:\n",
        "        other_factors = [f for f in factors_exRF if f != pred]\n",
        "        if other_factors:\n",
        "            return 0.5 * row[pred] + 0.5 * row[other_factors].mean()\n",
        "        else:\n",
        "            return row[pred]\n",
        "    else:\n",
        "        return row[factors_exRF].mean()\n",
        "\n",
        "df_filtered['Predicted Winner Weighted Strategy Return'] = df_filtered.apply(calc_winner_strategy, axis=1)\n",
        "# ===== END NEW CODE =====\n",
        "\n",
        "# Build monthly returns table\n",
        "if 'RF' in df_filtered.columns:\n",
        "    base_cols = [\n",
        "        'Predicted_month', 'ML Allocated Strategy Return',\n",
        "        'Equal Factor Weight Strategy Return ExRF',\n",
        "        'Equal Factor Weight Strategy Return InclRF',\n",
        "        'Benchmark Return', 'Predicted Winner Weighted Strategy Return'\n",
        "    ]\n",
        "else:\n",
        "    base_cols = [\n",
        "        'Predicted_month', 'ML Allocated Strategy Return',\n",
        "        'Equal Factor Weight Strategy Return',\n",
        "        'Benchmark Return', 'Predicted Winner Weighted Strategy Return'\n",
        "    ]\n",
        "returns_table = df_filtered[[c for c in base_cols if c in df_filtered.columns]].copy()\n",
        "\n",
        "# Compute cumulative returns using cumprod and subtract 1\n",
        "cum = returns_table.copy()\n",
        "if 'ML Allocated Strategy Return' in cum.columns:\n",
        "    cum['ML Cumulative Allocated Return'] = (1 + cum['ML Allocated Strategy Return']).cumprod() - 1\n",
        "if 'Equal Factor Weight Strategy Return ExRF' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return ExRF']  = (1 + cum['Equal Factor Weight Strategy Return ExRF']).cumprod() - 1\n",
        "    cum['Equal Factor Weight Cumulative Return InclRF'] = (1 + cum['Equal Factor Weight Strategy Return InclRF']).cumprod() - 1\n",
        "elif 'Equal Factor Weight Strategy Return' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return'] = (1 + cum['Equal Factor Weight Strategy Return']).cumprod() - 1\n",
        "if 'Benchmark Return' in cum.columns:\n",
        "    cum['Benchmark Cumulative Return'] = (1 + cum['Benchmark Return']).cumprod() - 1\n",
        "# ===== NEW CODE: Cumulative for Predicted Winner Weighted Strategy =====\n",
        "if 'Predicted Winner Weighted Strategy Return' in cum.columns:\n",
        "    cum['Predicted Winner Weighted Cumulative Return'] = (\n",
        "        1 + cum['Predicted Winner Weighted Strategy Return']\n",
        "    ).cumprod() - 1\n",
        "# ===== END NEW CODE =====\n",
        "\n",
        "# Keep only desired cumulative columns\n",
        "if 'Equal Factor Weight Strategy Return ExRF' in cum.columns:\n",
        "    desired = [\n",
        "        'Predicted_month', 'ML Cumulative Allocated Return',\n",
        "        'Equal Factor Weight Cumulative Return ExRF',\n",
        "        'Equal Factor Weight Cumulative Return InclRF',\n",
        "        'Benchmark Cumulative Return',\n",
        "        'Predicted Winner Weighted Cumulative Return'\n",
        "    ]\n",
        "else:\n",
        "    desired = [\n",
        "        'Predicted_month', 'ML Cumulative Allocated Return',\n",
        "        'Equal Factor Weight Cumulative Return',\n",
        "        'Benchmark Cumulative Return',\n",
        "        'Predicted Winner Weighted Cumulative Return'\n",
        "    ]\n",
        "cumulative_table = cum[[c for c in desired if c in cum.columns]].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "print(\"Returns Table:\")\n",
        "display(returns_table)\n",
        "print(\"Cumulative Returns Table:\")\n",
        "display(cumulative_table)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# BUILD A FACTOR CUMULATIVE TABLE & FIND THE BEST FACTOR\n",
        "# ---------------------------------------------------------------------\n",
        "factor_cum = pd.DataFrame({'Predicted_month': df_filtered['Predicted_month']})\n",
        "\n",
        "for fac in FACTORS:\n",
        "    if fac in df_filtered.columns:\n",
        "        # Build cumulative return for that factor\n",
        "        factor_cum[fac + ' Cumulative'] = (1 + df_filtered[fac]).cumprod() - 1\n",
        "\n",
        "# Identify the factor with the highest final cumulative return\n",
        "best_factor  = None\n",
        "best_final   = float('-inf')\n",
        "best_colname = None\n",
        "\n",
        "for fac in FACTORS:\n",
        "    col_cum = fac + ' Cumulative'\n",
        "    if col_cum in factor_cum.columns:\n",
        "        final_val = factor_cum[col_cum].iloc[-1]  # last row's value\n",
        "        if final_val > best_final:\n",
        "            best_final = final_val\n",
        "            best_factor = fac\n",
        "            best_colname = col_cum\n",
        "\n",
        "if best_factor is not None:\n",
        "    # Rename it to 'Best Factor Cumulative' for easy plotting\n",
        "    factor_cum.rename(columns={best_colname: 'Best Factor Cumulative'}, inplace=True)\n",
        "\n",
        "# We store factor_cum and best_factor for usage in the next cell\n",
        "# (Because we want to add it to the same chart.)\n",
        "# If you're in Jupyter, a global variable is enough:\n",
        "BEST_FACTOR = best_factor\n",
        "FACTOR_CUM  = factor_cum  # We'll read them in the next cell\n",
        "print(f\"\\nBest factor for {start_date.date()} to {end_date.date()} is: {best_factor} with final = {best_final:.2f}\")"
      ],
      "metadata": {
        "id": "wm06N1gjGjVr"
      },
      "id": "wm06N1gjGjVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns chart"
      ],
      "metadata": {
        "id": "W-uq2G9iPQwD"
      },
      "id": "W-uq2G9iPQwD"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toggle: Show 50%/50% strategy line or not\n",
        "show_50_50_strategy = True  # Set to False to hide it\n",
        "\n",
        "# Ensure cumulative_table is up-to-date\n",
        "df_plot = cumulative_table.copy()\n",
        "\n",
        "start_date = df_plot['Predicted_month'].min()\n",
        "end_date   = df_plot['Predicted_month'].max()\n",
        "print(f\"Updated plotting range: {start_date} to {end_date}\")\n",
        "print(\"Columns available for plotting:\", df_plot.columns.tolist())\n",
        "\n",
        "if start_date == end_date:\n",
        "    print(\"⚠ Warning: The dataset might not have updated properly. Try rerunning the previous cell!\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.clf()  # Clears the figure\n",
        "\n",
        "# --- Plot your existing lines ---\n",
        "if 'ML Cumulative Allocated Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['ML Cumulative Allocated Return'],\n",
        "             label='ML Cumulative Allocated Return')\n",
        "\n",
        "if 'Equal Factor Weight Cumulative Return ExRF' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Equal Factor Weight Cumulative Return ExRF'],\n",
        "             label='Equal Factor Weight Cumulative (w/o RF)')\n",
        "\n",
        "if 'RF' in FACTORS and 'Equal Factor Weight Cumulative Return InclRF' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Equal Factor Weight Cumulative Return InclRF'],\n",
        "             label='Equal Factor Weight Cumulative (w/ RF)')\n",
        "\n",
        "if show_benchmark and 'Benchmark Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Benchmark Cumulative Return'],\n",
        "             label='Benchmark Cumulative Return')\n",
        "\n",
        "if show_50_50_strategy and 'Predicted Winner Weighted Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Predicted Winner Weighted Cumulative Return'],\n",
        "             label='50%/50% Predicted Winner Strategy')\n",
        "\n",
        "# --- Plot the best factor if we found one ---\n",
        "if BEST_FACTOR is not None and 'Best Factor Cumulative' in FACTOR_CUM.columns:\n",
        "    # Just to ensure date range lines up, we can do a quick merge if needed\n",
        "    # but if factor_cum has the same 'Predicted_month' indexing, we can plot directly:\n",
        "    plt.plot(FACTOR_CUM['Predicted_month'], FACTOR_CUM['Best Factor Cumulative'],\n",
        "             label=f'Best Factor: {BEST_FACTOR}')\n",
        "\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.title('Cumulative Returns Comparison')\n",
        "plt.xlim(df_plot['Predicted_month'].min(), df_plot['Predicted_month'].max())\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wqfmAhDk3KBx"
      },
      "id": "wqfmAhDk3KBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PERFORMANCE METRICS"
      ],
      "metadata": {
        "id": "-YjFAG1ePN-9"
      },
      "id": "-YjFAG1ePN-9"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# 1) FILTER & BUILD returns_table\n",
        "#    (Your snippet code, mostly unchanged)\n",
        "# -----------------------------\n",
        "\n",
        "start_date = pd.to_datetime('1950-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "\n",
        "df_filtered = results_df_local[(results_df_local['Predicted_month'] >= start_date) &\n",
        "                               (results_df_local['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "rename_dict = {}\n",
        "if 'Allocated_Return' in df_filtered.columns:\n",
        "    rename_dict['Allocated_Return'] = 'ML Allocated Strategy Return'\n",
        "if BENCHMARK[0] in df_filtered.columns:\n",
        "    rename_dict[BENCHMARK[0]] = 'Benchmark Return'\n",
        "df_filtered.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# Calculate equal-weighted returns:\n",
        "factors_exRF = [f for f in FACTORS if f in df_filtered.columns]\n",
        "if 'RF' in df_filtered.columns:\n",
        "    factors_inRF = factors_exRF + ['RF']\n",
        "    df_filtered['Equal Factor Weight Strategy Return ExRF']  = df_filtered[factors_exRF].mean(axis=1)\n",
        "    df_filtered['Equal Factor Weight Strategy Return InclRF'] = df_filtered[factors_inRF].mean(axis=1)\n",
        "else:\n",
        "    df_filtered['Equal Factor Weight Strategy Return'] = df_filtered[factors_exRF].mean(axis=1)\n",
        "\n",
        "# 50% Winner Weighted Strategy\n",
        "def calc_winner_strategy(row):\n",
        "    pred = row['Predicted_Winner']\n",
        "    if pred in factors_exRF:\n",
        "        other_factors = [f for f in factors_exRF if f != pred]\n",
        "        if len(other_factors) > 0:\n",
        "            return 0.5 * row[pred] + 0.5 * row[other_factors].mean()\n",
        "        else:\n",
        "            return row[pred]\n",
        "    else:\n",
        "        return row[factors_exRF].mean()\n",
        "\n",
        "df_filtered['Predicted Winner Weighted Strategy Return'] = df_filtered.apply(calc_winner_strategy, axis=1)\n",
        "\n",
        "# Keep only the strategy columns we want\n",
        "if 'RF' in df_filtered.columns:\n",
        "    base_cols = [\n",
        "        'Predicted_month',\n",
        "        'ML Allocated Strategy Return',\n",
        "        'Equal Factor Weight Strategy Return ExRF',\n",
        "        'Equal Factor Weight Strategy Return InclRF',\n",
        "        'Benchmark Return',\n",
        "        'Predicted Winner Weighted Strategy Return'\n",
        "    ]\n",
        "else:\n",
        "    base_cols = [\n",
        "        'Predicted_month',\n",
        "        'ML Allocated Strategy Return',\n",
        "        'Equal Factor Weight Strategy Return',\n",
        "        'Benchmark Return',\n",
        "        'Predicted Winner Weighted Strategy Return'\n",
        "    ]\n",
        "returns_table = df_filtered[[c for c in base_cols if c in df_filtered.columns]].copy()\n",
        "\n",
        "# Build cumulative returns (and store in cumulative_table)\n",
        "cum = returns_table.copy()\n",
        "if 'ML Allocated Strategy Return' in cum.columns:\n",
        "    cum['ML Cumulative Allocated Return'] = (1 + cum['ML Allocated Strategy Return']).cumprod() - 1\n",
        "if 'Equal Factor Weight Strategy Return ExRF' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return ExRF']  = (1 + cum['Equal Factor Weight Strategy Return ExRF']).cumprod() - 1\n",
        "    cum['Equal Factor Weight Cumulative Return InclRF'] = (1 + cum['Equal Factor Weight Strategy Return InclRF']).cumprod() - 1\n",
        "elif 'Equal Factor Weight Strategy Return' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return'] = (1 + cum['Equal Factor Weight Strategy Return']).cumprod() - 1\n",
        "if 'Benchmark Return' in cum.columns:\n",
        "    cum['Benchmark Cumulative Return'] = (1 + cum['Benchmark Return']).cumprod() - 1\n",
        "if 'Predicted Winner Weighted Strategy Return' in cum.columns:\n",
        "    cum['Predicted Winner Weighted Cumulative Return'] = (1 + cum['Predicted Winner Weighted Strategy Return']).cumprod() - 1\n",
        "\n",
        "if 'Equal Factor Weight Strategy Return ExRF' in cum.columns:\n",
        "    desired_cols = [\n",
        "        'Predicted_month',\n",
        "        'ML Cumulative Allocated Return',\n",
        "        'Equal Factor Weight Cumulative Return ExRF',\n",
        "        'Equal Factor Weight Cumulative Return InclRF',\n",
        "        'Benchmark Cumulative Return',\n",
        "        'Predicted Winner Weighted Cumulative Return'\n",
        "    ]\n",
        "else:\n",
        "    desired_cols = [\n",
        "        'Predicted_month',\n",
        "        'ML Cumulative Allocated Return',\n",
        "        'Equal Factor Weight Cumulative Return',\n",
        "        'Benchmark Cumulative Return',\n",
        "        'Predicted Winner Weighted Cumulative Return'\n",
        "    ]\n",
        "cumulative_table = cum[[c for c in desired_cols if c in cum.columns]].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "print(\"Returns Table:\")\n",
        "display(returns_table)\n",
        "print(\"Cumulative Returns Table:\")\n",
        "display(cumulative_table)\n",
        "\n",
        "# -----------------------------\n",
        "# 2) APPLY PERFORMANCE METRICS\n",
        "#    (One example approach)\n",
        "# -----------------------------\n",
        "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
        "\n",
        "# We'll do annualized return, volatility, Sharpe, max drawdown for each column we have.\n",
        "\n",
        "def annualized_metrics(monthly_returns):\n",
        "    \"\"\"Helper to return (ann_return, ann_vol, sharpe).\"\"\"\n",
        "    monthly_returns = monthly_returns.fillna(0)\n",
        "    mean_m = monthly_returns.mean()\n",
        "    std_m  = monthly_returns.std()\n",
        "    ann_ret = mean_m * 12\n",
        "    ann_vol = std_m * np.sqrt(12)\n",
        "    sharpe  = ann_ret / ann_vol if ann_vol != 0 else np.nan\n",
        "    return ann_ret, ann_vol, sharpe\n",
        "\n",
        "def max_drawdown(monthly_returns):\n",
        "    \"\"\"Compute wealth index and return the minimum drawdown.\"\"\"\n",
        "    wealth = (1 + monthly_returns.fillna(0)).cumprod()\n",
        "    dd_series = wealth / wealth.cummax() - 1\n",
        "    return dd_series.min()\n",
        "\n",
        "# We'll do it on 'returns_table' since that has monthly returns:\n",
        "for col in returns_table.columns:\n",
        "    # skip the date column\n",
        "    if col == 'Predicted_month':\n",
        "        continue\n",
        "    # each col is presumably a monthly return except 'Predicted_month'\n",
        "    monthly_ret_series = returns_table[col]\n",
        "    ann_ret, ann_vol, sharpe = annualized_metrics(monthly_ret_series)\n",
        "    mdd = max_drawdown(monthly_ret_series)\n",
        "\n",
        "    print(f\"\\nStrategy: {col}\")\n",
        "    print(f\"  Annualized Return:      {ann_ret:.2%}\")\n",
        "    print(f\"  Annualized Volatility:  {ann_vol:.2%}\")\n",
        "    print(f\"  Sharpe Ratio:           {sharpe:.2f}\")\n",
        "    print(f\"  Max Drawdown:           {mdd:.2%}\")"
      ],
      "metadata": {
        "id": "1o0tq56K64bL"
      },
      "id": "1o0tq56K64bL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drawdown chart"
      ],
      "metadata": {
        "id": "YXD3cJKFO_sE"
      },
      "id": "YXD3cJKFO_sE"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. TOGGLE OPTIONS\n",
        "# -------------------------------------------------------------------------\n",
        "show_benchmark_drawdown = True           # Show/hide the benchmark's drawdown\n",
        "show_equal_weight_drawdown = True        # Single Equal Weight version\n",
        "show_equal_weight_drawdown_exRF = True   # If you have \"ExRF\"\n",
        "show_equal_weight_drawdown_inclRF = True # If you have \"InclRF\"\n",
        "show_winner_weighted_drawdown = True     # If you have \"Predicted Winner Weighted\"\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. COPY YOUR cumulative_table\n",
        "#    (Assuming it has columns like \"ML Cumulative Allocated Return\",\n",
        "#     \"Benchmark Cumulative Return\", \"Equal Factor Weight Cumulative Return\", etc.)\n",
        "# -------------------------------------------------------------------------\n",
        "drawdown_df = cumulative_table.copy()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. CALCULATE DRAWDOWNS\n",
        "#    drawdown = current_wealth / peak_wealth - 1\n",
        "# -------------------------------------------------------------------------\n",
        "# a) ML Strategy\n",
        "if \"ML Cumulative Allocated Return\" in drawdown_df.columns:\n",
        "    drawdown_df[\"ML Drawdown\"] = (\n",
        "        drawdown_df[\"ML Cumulative Allocated Return\"] /\n",
        "        drawdown_df[\"ML Cumulative Allocated Return\"].cummax() - 1\n",
        "    )\n",
        "\n",
        "# b) Benchmark\n",
        "if \"Benchmark Cumulative Return\" in drawdown_df.columns:\n",
        "    drawdown_df[\"Benchmark Drawdown\"] = (\n",
        "        drawdown_df[\"Benchmark Cumulative Return\"] /\n",
        "        drawdown_df[\"Benchmark Cumulative Return\"].cummax() - 1\n",
        "    )\n",
        "\n",
        "# c) Equal Weight (single version)\n",
        "if \"Equal Factor Weight Cumulative Return\" in drawdown_df.columns:\n",
        "    drawdown_df[\"Equal Weight Drawdown\"] = (\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return\"] /\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return\"].cummax() - 1\n",
        "    )\n",
        "\n",
        "# d) Equal Weight (ExRF/InclRF) versions\n",
        "if \"Equal Factor Weight Cumulative Return ExRF\" in drawdown_df.columns:\n",
        "    drawdown_df[\"Equal Weight Drawdown ExRF\"] = (\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return ExRF\"] /\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return ExRF\"].cummax() - 1\n",
        "    )\n",
        "if \"Equal Factor Weight Cumulative Return InclRF\" in drawdown_df.columns:\n",
        "    drawdown_df[\"Equal Weight Drawdown InclRF\"] = (\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return InclRF\"] /\n",
        "        drawdown_df[\"Equal Factor Weight Cumulative Return InclRF\"].cummax() - 1\n",
        "    )\n",
        "\n",
        "# e) Winner Weighted\n",
        "if \"Predicted Winner Weighted Cumulative Return\" in drawdown_df.columns:\n",
        "    drawdown_df[\"Winner Weighted Drawdown\"] = (\n",
        "        drawdown_df[\"Predicted Winner Weighted Cumulative Return\"] /\n",
        "        drawdown_df[\"Predicted Winner Weighted Cumulative Return\"].cummax() - 1\n",
        "    )\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4. FILTER BY DATE RANGE\n",
        "# -------------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "drawdown_df[\"Predicted_month\"] = pd.to_datetime(drawdown_df[\"Predicted_month\"])\n",
        "plot_df = drawdown_df[\n",
        "    (drawdown_df[\"Predicted_month\"] >= start_date) &\n",
        "    (drawdown_df[\"Predicted_month\"] <= end_date)\n",
        "]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5. PLOT THE DRAWDOWNS\n",
        "# -------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# a) ML Drawdown\n",
        "if \"ML Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"ML Drawdown\"], label=\"ML Drawdown\")\n",
        "\n",
        "# b) Benchmark\n",
        "if show_benchmark_drawdown and \"Benchmark Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Benchmark Drawdown\"], label=\"Benchmark Drawdown\")\n",
        "\n",
        "# c) Equal Weight single\n",
        "if show_equal_weight_drawdown and \"Equal Weight Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Equal Weight Drawdown\"], label=\"Equal Weight Drawdown\")\n",
        "\n",
        "# d) Equal Weight ExRF\n",
        "if show_equal_weight_drawdown_exRF and \"Equal Weight Drawdown ExRF\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Equal Weight Drawdown ExRF\"], label=\"Equal Weight Drawdown (ExRF)\")\n",
        "\n",
        "# e) Equal Weight InclRF\n",
        "if show_equal_weight_drawdown_inclRF and \"Equal Weight Drawdown InclRF\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Equal Weight Drawdown InclRF\"], label=\"Equal Weight Drawdown (InclRF)\")\n",
        "\n",
        "# f) Winner Weighted\n",
        "if show_winner_weighted_drawdown and \"Winner Weighted Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Winner Weighted Drawdown\"], label=\"Winner Weighted Drawdown\")\n",
        "\n",
        "# Format as % on the y-axis\n",
        "plt.gca().yaxis.set_major_formatter(\n",
        "    mticker.FuncFormatter(lambda val, _: f\"{val*100:.0f}%\")\n",
        ")\n",
        "\n",
        "# Final styling\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Drawdown\")\n",
        "plt.title(\"Drawdowns of Strategies\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FK-XFzpv4SXo"
      },
      "id": "FK-XFzpv4SXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression"
      ],
      "metadata": {
        "id": "S244JpMyoSVe"
      },
      "id": "S244JpMyoSVe"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# 1. Create a working copy with just the necessary columns\n",
        "regression_df = results_df_local[['Predicted_month', 'Allocated_Return', 'Equal_Weight_Return']].copy()\n",
        "\n",
        "# 2. Load FF5 data from Excel (treat numeric columns as strings to clean commas)\n",
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "df_factors = xls_file.parse(\"FF5\", dtype=str)  # read all as string to handle comma decimal\n",
        "\n",
        "# 3. Convert date\n",
        "df_factors[\"Date\"] = pd.to_datetime(df_factors[\"Date\"])\n",
        "\n",
        "# 4. Convert numeric columns: replace commas, convert to float, and divide by 100\n",
        "factor_columns = ['RF', 'Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "for col in factor_columns:\n",
        "    df_factors[col] = df_factors[col].str.replace(\",\", \".\", regex=False).astype(float) / 100\n",
        "\n",
        "# 5. Convert date column in main data\n",
        "regression_df[\"Predicted_month\"] = pd.to_datetime(regression_df[\"Predicted_month\"])\n",
        "\n",
        "# 6. Merge\n",
        "df_merged = pd.merge(\n",
        "    regression_df,\n",
        "    df_factors,\n",
        "    how=\"inner\",\n",
        "    left_on=\"Predicted_month\",\n",
        "    right_on=\"Date\"\n",
        ")\n",
        "\n",
        "# 7. Sort by date\n",
        "df_merged = df_merged.sort_values(\"Predicted_month\")\n",
        "\n",
        "# 8. Adjust returns if needed\n",
        "subtract_rf = (FF5_long or MSCI)\n",
        "if subtract_rf:\n",
        "    print(\"Subtracting RF from strategy returns (long-only case).\")\n",
        "    df_merged['Adj_Allocated_Return'] = df_merged['Allocated_Return'] - df_merged['RF']\n",
        "    df_merged['Adj_Equal_Weight_Return'] = df_merged['Equal_Weight_Return'] - df_merged['RF']\n",
        "else:\n",
        "    print(\"Not subtracting RF from strategy returns (plain FF5).\")\n",
        "    df_merged['Adj_Allocated_Return'] = df_merged['Allocated_Return']\n",
        "    df_merged['Adj_Equal_Weight_Return'] = df_merged['Equal_Weight_Return']\n",
        "\n",
        "# 9. Prepare X (independent variables)\n",
        "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "X = df_merged[factors]\n",
        "X = sm.add_constant(X)\n",
        "\n",
        "# 10. Regressions\n",
        "model_ml = sm.OLS(df_merged['Adj_Allocated_Return'], X, missing='drop').fit()\n",
        "model_eq = sm.OLS(df_merged['Adj_Equal_Weight_Return'], X, missing='drop').fit()\n",
        "\n",
        "# 11. Output: ML strategy\n",
        "print(\"\\n=== ML Strategy vs. FF5 Factors ===\")\n",
        "print(model_ml.summary())\n",
        "print(f\"\\nAlpha (Monthly, ML Strategy): {model_ml.params['const']:.4f}\")\n",
        "print(f\"Market Beta (ML Strategy):    {model_ml.params['Mkt-RF']:.4f}\")\n",
        "\n",
        "# 12. Output: Equal-Weight strategy\n",
        "print(\"\\n=== Equal-Weight Strategy vs. FF5 Factors ===\")\n",
        "print(model_eq.summary())\n",
        "print(f\"\\nAlpha (Monthly, Equal-Weight): {model_eq.params['const']:.4f}\")\n",
        "print(f\"Market Beta (Equal-Weight):   {model_eq.params['Mkt-RF']:.4f}\")\n"
      ],
      "metadata": {
        "id": "MOb-rnOzH_4W"
      },
      "id": "MOb-rnOzH_4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 1) Define your date range\n",
        "# -----------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "# Example DataFrame: merged_df\n",
        "# merged_df has columns:\n",
        "#   ['Predicted_month', 'ML Allocated Strategy Return', 'Equal Factor Weight Strategy Return ExRF',\n",
        "#    'Equal Factor Weight Strategy Return InclRF', 'Benchmark Return',\n",
        "#    'Predicted Winner Weighted Strategy Return', 'Mkt-RF', 'SMB', 'HML', 'RMW',\n",
        "#    'CMA', 'RF', 'Excess Return', 'Year']\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 2) Filter the DataFrame for the desired date range\n",
        "# -----------------------------------------------------------------------\n",
        "mask = (merged_df['Predicted_month'] >= start_date) & (merged_df['Predicted_month'] <= end_date)\n",
        "df_filtered = merged_df.loc[mask].copy()\n",
        "\n",
        "# Ensure there's a 'Year' column (if not, create one)\n",
        "if 'Year' not in df_filtered.columns:\n",
        "    df_filtered['Year'] = df_filtered['Predicted_month'].dt.year\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 3) Define a function that calculates annual Sharpe from a Series\n",
        "# -----------------------------------------------------------------------\n",
        "def annual_sharpe(series):\n",
        "    \"\"\"\n",
        "    Calculate the annual Sharpe ratio from a monthly return Series.\n",
        "      - annual return = product(1 + monthly returns) - 1\n",
        "      - annual volatility = std(mo. returns) * sqrt(12)\n",
        "      - sharpe = annual return / annual volatility  (risk-free = 0)\n",
        "    \"\"\"\n",
        "    annual_return = (1 + series).prod() - 1\n",
        "    annual_vol = series.std() * np.sqrt(12)\n",
        "    if annual_vol == 0:\n",
        "        return np.nan\n",
        "    return annual_return / annual_vol\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 4) Choose the columns you want Sharpe ratios for\n",
        "# -----------------------------------------------------------------------\n",
        "ml_col    = \"ML Allocated Strategy Return\"\n",
        "bench_col = \"Benchmark Return\"\n",
        "eq_exrf   = \"Equal Factor Weight Strategy Return ExRF\"\n",
        "\n",
        "# Make sure these columns exist:\n",
        "for c in [ml_col, bench_col, eq_exrf]:\n",
        "    if c not in df_filtered.columns:\n",
        "        print(f\"WARNING: Column '{c}' not in DataFrame.\")\n",
        "        # you might want to continue or raise an error\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 5) Group by year & compute Sharpe for each strategy\n",
        "#    (avoids deprecation warnings by applying the Series directly)\n",
        "# -----------------------------------------------------------------------\n",
        "sharpe_ml = df_filtered.groupby(\"Year\", group_keys=False)[ml_col].apply(annual_sharpe)\n",
        "sharpe_bench = df_filtered.groupby(\"Year\", group_keys=False)[bench_col].apply(annual_sharpe)\n",
        "sharpe_eqex = df_filtered.groupby(\"Year\", group_keys=False)[eq_exrf].apply(annual_sharpe)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 6) Combine results into one DataFrame\n",
        "# -----------------------------------------------------------------------\n",
        "annual_sharpe_table = pd.DataFrame({\n",
        "    \"ML Sharpe Ratio\"                  : sharpe_ml,\n",
        "    \"Benchmark Sharpe Ratio\"           : sharpe_bench,\n",
        "    \"Equal Factor Weight (ExRF) Sharpe\": sharpe_eqex\n",
        "})\n",
        "\n",
        "# (Optional) round to 2 decimals or format as strings\n",
        "annual_sharpe_table = annual_sharpe_table.round(2)\n",
        "\n",
        "# -----------------------------------------------------------------------\n",
        "# 7) Display the table as HTML\n",
        "# -----------------------------------------------------------------------\n",
        "display(HTML(annual_sharpe_table.to_html()))"
      ],
      "metadata": {
        "id": "E0cAJzowyqZn"
      },
      "id": "E0cAJzowyqZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Prepare the Data: Create a Year Column from 'Predicted_month'\n",
        "merged_df['Year'] = merged_df['Predicted_month'].dt.year\n",
        "\n",
        "# --- Function to Calculate Annual Return ---\n",
        "def annual_return(group, col):\n",
        "    \"\"\"\n",
        "    Compute the annual return as the compounded return over the year.\n",
        "    \"\"\"\n",
        "    return (1 + group[col]).prod() - 1\n",
        "\n",
        "# --- Compute Annual Returns for the ML Allocated Strategy ---\n",
        "annual_return_ml = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, 'ML Allocated Strategy Return'))\n",
        "\n",
        "# --- Compute Annual Returns for Each Factor ---\n",
        "factor_names = FACTORS\n",
        "annual_returns_factors = {}\n",
        "excess_returns = {}\n",
        "\n",
        "for factor in factor_names:\n",
        "    annual_returns_factors[factor] = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, factor))\n",
        "    # Compute excess return: ML strategy annual return minus factor's annual return.\n",
        "    excess_returns[factor] = annual_return_ml - annual_returns_factors[factor]\n",
        "\n",
        "# Combine the computed metrics into a DataFrame for reference (optional)\n",
        "annual_metrics = pd.DataFrame({'ML Annual Return': annual_return_ml})\n",
        "for factor in factor_names:\n",
        "    annual_returns_factors[factor] = merged_df.groupby('Year').apply(lambda grp: annual_return(grp, factor))\n",
        "    excess_returns[factor] = annual_return_ml - annual_returns_factors[factor]\n",
        "\n",
        "print(\"Annual Metrics:\")\n",
        "print(annual_metrics.round(4))\n",
        "\n",
        "# --- Plot Excess Returns for Each Factor ---\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14,10), sharex=True)\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, factor in enumerate(factor_names):\n",
        "    ax = axes[idx]\n",
        "    # Plot excess return: ML annual return minus factor annual return.\n",
        "    ax.plot(excess_returns[factor].index, excess_returns[factor],\n",
        "            label=f'Excess Return (ML - {factor})', marker='o')\n",
        "    ax.set_title(f'Excess Return (ML - {factor})')\n",
        "    ax.set_xlabel('Year')\n",
        "    ax.set_ylabel('Excess Return')\n",
        "    ax.grid(True, linestyle='--', alpha=0.7)\n",
        "    ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Lo1wj3nU3ceD"
      },
      "id": "Lo1wj3nU3ceD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Return comparison by period"
      ],
      "metadata": {
        "id": "tC8mxG1Ynm-j"
      },
      "id": "tC8mxG1Ynm-j"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# ======================\n",
        "# --- Part 1: Process results_df (ML Strategy) ---\n",
        "# ======================\n",
        "\n",
        "# Ensure 'Predicted_month' is in datetime format\n",
        "results_df['Predicted_month'] = pd.to_datetime(results_df['Predicted_month'])\n",
        "\n",
        "# Define your dynamic date range (adjust as needed)\n",
        "start_date = pd.to_datetime('1973-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Filter results_df for the date range and select the desired columns\n",
        "filtered_df = results_df.loc[\n",
        "    (results_df['Predicted_month'] >= start_date) & (results_df['Predicted_month'] <= end_date),\n",
        "    ['Predicted_month', 'Allocated_Return']\n",
        "].sort_values('Predicted_month')\n",
        "\n",
        "# Rename 'Allocated_Return' to 'ML Allocated Strategy Return'\n",
        "filtered_df = filtered_df.rename(columns={'Allocated_Return': 'ML Allocated Strategy Return'})\n",
        "\n",
        "# Calculate cumulative returns for the ML allocated strategy using compound returns (wealth index starting at 1)\n",
        "filtered_df['ML Cumulative Allocated Return'] = (1 + filtered_df['ML Allocated Strategy Return']).cumprod()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# --- Part 2: Process the second DataFrame (df) (Benchmark & Equal Factor Weight) ---\n",
        "# ======================\n",
        "\n",
        "# Create a filtered copy of df for the same date range using the 'Date' column.\n",
        "df_filtered = df.loc[\n",
        "    (pd.to_datetime(df['Date']) >= start_date) & (pd.to_datetime(df['Date']) <= end_date)\n",
        "].copy()\n",
        "\n",
        "# Ensure the 'Date' column is datetime for correct sorting and merging\n",
        "df_filtered['Date'] = pd.to_datetime(df_filtered['Date'])\n",
        "\n",
        "# Sort by 'Date'\n",
        "df_filtered = df_filtered.sort_values('Date')\n",
        "\n",
        "# Calculate cumulative returns for the RMW benchmark using 'RMW'\n",
        "df_filtered['RMW Cumulative'] = (1 + df_filtered['RMW']).cumprod()\n",
        "\n",
        "# Calculate the Equal Factor Weight Strategy returns by taking an equal-weight average of 4 factors (excluding MOM)\n",
        "df_filtered['Equal Factor Weight Strategy'] = df_filtered[FACTORS].mean(axis=1)\n",
        "\n",
        "\n",
        "# Calculate cumulative returns for the Equal Factor Weight Strategy using compound returns (wealth index starting at 1)\n",
        "df_filtered['Equal Factor Weight Cumulative Return'] = (1 + df_filtered['Equal Factor Weight Strategy']).cumprod()\n",
        "\n",
        "\n",
        "# ======================\n",
        "# --- Part 3: Merge the two DataFrames ---\n",
        "# ======================\n",
        "\n",
        "# Merge on the date columns:\n",
        "#   - In filtered_df: 'Predicted_month'\n",
        "#   - In df_filtered: 'Date'\n",
        "merged_df = pd.merge(\n",
        "    filtered_df,\n",
        "    df_filtered[['Date', 'RMW Cumulative', 'Equal Factor Weight Strategy', 'Equal Factor Weight Cumulative Return']],\n",
        "    left_on='Predicted_month',\n",
        "    right_on='Date',\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Drop the duplicate 'Date' column from the merge\n",
        "merged_df.drop(columns=['Date'], inplace=True)\n",
        "\n",
        "# ======================\n",
        "# --- Part 4: Define 5-Year Periods and Compute Cumulative Returns for Each Block ---\n",
        "# ======================\n",
        "\n",
        "def define_5_year_period(date, start_year=1973):\n",
        "    \"\"\"\n",
        "    Assign each date to a discrete 5-year block starting at 'start_year'.\n",
        "    For example, a date in 1973-1977 will be labeled \"1973-1977\",\n",
        "    in 1978-1982 as \"1978-1982\", etc.\n",
        "    \"\"\"\n",
        "    if pd.isnull(date):\n",
        "        return None\n",
        "    year = date.year\n",
        "    # Determine the block index from start_year\n",
        "    block_index = (year - start_year) // 5\n",
        "    block_start = start_year + 5 * block_index\n",
        "    block_end = block_start + 4\n",
        "    return f\"{block_start}-{block_end}\"\n",
        "\n",
        "# Apply the function to create a new column for 5-year period labels\n",
        "merged_df['5_Year_Period'] = merged_df['Predicted_month'].apply(lambda x: define_5_year_period(x, start_year=1973))\n",
        "\n",
        "def five_year_return(group, col):\n",
        "    \"\"\"\n",
        "    Compute the total return over the 5-year block for the given cumulative return column.\n",
        "    This is done by taking the ratio of the last value to the first value of the block, minus 1.\n",
        "    \"\"\"\n",
        "    # Ensure the group is sorted by date\n",
        "    group = group.sort_values(by='Predicted_month')\n",
        "    start_val = group[col].iloc[0]\n",
        "    end_val   = group[col].iloc[-1]\n",
        "    return (end_val / start_val) - 1\n",
        "\n",
        "# Group by the 5_Year_Period and compute returns for each strategy\n",
        "five_year_results = merged_df.groupby('5_Year_Period').apply(\n",
        "    lambda g: pd.Series({\n",
        "        'ML_5yr_return'    : five_year_return(g, 'ML Cumulative Allocated Return'),\n",
        "        'Equal_5yr_return' : five_year_return(g, 'Equal Factor Weight Cumulative Return'),\n",
        "        'RMW_5yr_return'   : five_year_return(g, 'RMW Cumulative')\n",
        "    })\n",
        ").reset_index()\n",
        "\n",
        "# Calculate the excess return of ML strategy over the Equal Factor Weight strategy\n",
        "five_year_results['Excess_return'] = five_year_results['ML_5yr_return'] - five_year_results['Equal_5yr_return']\n",
        "\n",
        "# ======================\n",
        "# (Optional) Remove partial 5-year blocks if desired.\n",
        "# For example, you might require at least 5 full years of data.\n",
        "# Uncomment the following block if needed.\n",
        "# ======================\n",
        "# def is_full_5_years(group):\n",
        "#     date_range = group['Predicted_month'].max() - group['Predicted_month'].min()\n",
        "#     return date_range.days >= 1825  # Approximately 5 years in days\n",
        "#\n",
        "# full_periods = []\n",
        "# for period, grp in merged_df.groupby('5_Year_Period'):\n",
        "#     if is_full_5_years(grp):\n",
        "#         full_periods.append(period)\n",
        "#\n",
        "# five_year_results = five_year_results[five_year_results['5_Year_Period'].isin(full_periods)]\n",
        "\n",
        "# ======================\n",
        "# --- Part 5: Display the 5-Year Cumulative Returns ---\n",
        "# ======================\n",
        "\n",
        "print(\"5-Year Cumulative Returns Comparison:\")\n",
        "print(five_year_results)"
      ],
      "metadata": {
        "id": "_KfEgK83EKLT"
      },
      "id": "_KfEgK83EKLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corr Heat map & regiimi sharpet\n"
      ],
      "metadata": {
        "id": "WnQxAZGgWuKt"
      },
      "id": "WnQxAZGgWuKt"
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Regime-wise Correlation Heatmaps\n",
        "#\n",
        "# For the selected return columns, compute and plot the correlation matrix\n",
        "# for each market regime as a heatmap.\n",
        "\n",
        "# %%\n",
        "# Use the global FACTORS instead of redefining returns_columns\n",
        "unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "for regime in unique_regimes:\n",
        "    regime_data = df[df[REGIMES_COLUMN] == regime][FACTORS]\n",
        "    corr = regime_data.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "    plt.title(f\"Return Correlation Heatmap - {regime}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4GbzDKk2FZYH"
      },
      "id": "4GbzDKk2FZYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Sharpe Ratios by Market Regime\n",
        "#\n",
        "# Compute and visualize Sharpe ratios for selected factors across each regime,\n",
        "# as well as the unconditional (all-data) values, using a bar chart.\n",
        "# The numeric regime codes are converted back to their original names using the regime_mapping,\n",
        "# and then further shortened using regime_short_mapping.\n",
        "\n",
        "# %%\n",
        "# Define factors and regime columns (using global variables if already defined)\n",
        "factors_columns = FACTORS\n",
        "regimes_column = REGIMES_COLUMN   # Assumes REGIMES_COLUMN was defined earlier\n",
        "\n",
        "# Use the previously created regime_short_mapping to convert numeric codes back to short names.\n",
        "# (If a code is not in regime_short_mapping, it will default to \"Regime <code>\")\n",
        "regime_short_names = {reg: regime_short_mapping.get(reg, f\"Regime {reg}\")\n",
        "                      for reg in df[regimes_column].unique()}\n",
        "\n",
        "sharpe_ratios = {\n",
        "    regime_short_names[regime]: (\n",
        "        df[df[regimes_column] == regime][factors_columns].mean() /\n",
        "        df[df[regimes_column] == regime][factors_columns].std()\n",
        "    )\n",
        "    for regime in df[regimes_column].unique()\n",
        "}\n",
        "\n",
        "# Calculate the \"Unconditional\" Sharpe ratios (using all data)\n",
        "sharpe_ratios[\"Unconditional\"] = df[factors_columns].mean() / df[factors_columns].std()\n",
        "\n",
        "# Convert the dictionary to a DataFrame and set column names\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios).T\n",
        "sharpe_ratios_df.columns = factors_columns\n",
        "\n",
        "# Plot the Sharpe ratios using the same styling as before.\n",
        "plt.figure(figsize=(14, 8))\n",
        "sharpe_ratios_df.plot(\n",
        "    kind=\"bar\",\n",
        "    grid=True,\n",
        "    colormap=\"viridis\",\n",
        "    title=\"Sharpe Ratios by Regime and Unconditional\",\n",
        "    figsize=(14, 8)\n",
        ")\n",
        "plt.ylabel(\"Sharpe Ratio\", fontsize=12)\n",
        "plt.xlabel(\"Market Regimes\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(title=\"Factors\", fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCkBikW6A2o0"
      },
      "id": "GCkBikW6A2o0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance by period"
      ],
      "metadata": {
        "id": "SwsjcsWyoH2Z"
      },
      "id": "SwsjcsWyoH2Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======= USER-DEFINED DATE RANGE =======\n",
        "# Adjust these dates to view feature importances for a specific period\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date   = pd.to_datetime('2022-12-31')\n",
        "\n",
        "# ======= Filter the Data =======\n",
        "# Filter the results_df for the specified date range based on the 'Predicted_month' column\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "]\n",
        "\n",
        "# ======= Get Unique Regimes and Feature Count =======\n",
        "existing_regimes = filtered_results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "n_features = len(filtered_results_df['Feature_Importances'].iloc[0])  # Assumes each entry is a vector\n",
        "\n",
        "# ======= Robust Feature Naming =======\n",
        "try:\n",
        "    # Validate if the predefined FEATURES list matches the actual feature count\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"⚠️ Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features}).\")\n",
        "        print(\"Using auto-generated feature names instead.\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate default feature names if there's a mismatch or if FEATURES is undefined\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features.\")\n",
        "\n",
        "# ======= Compute Overall Average Feature Importances =======\n",
        "overall_avg_fi = np.vstack(filtered_results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# ======= Compute Regime-Specific Average Feature Importances =======\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = filtered_results_df[filtered_results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# ======= Sort Features by Overall Importance (Descending) =======\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # Ensure index bounds\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# ======= Plotting =======\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One overall plot plus one for each regime\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Feature Importances ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    # If zero or one regime, show only the overall chart\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Extra space for feature labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XgeSIiAu-M2b"
      },
      "id": "XgeSIiAu-M2b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}