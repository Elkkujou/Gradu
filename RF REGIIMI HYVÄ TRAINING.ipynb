{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "IDEAT: Vix takas? Joku momentum indikaattori? Sentimentti?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mqpPtrCOkXAO"
      },
      "id": "mqpPtrCOkXAO"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Model settings"
      ],
      "metadata": {
        "id": "k1mEzhhWXup6"
      },
      "id": "k1mEzhhWXup6"
    },
    {
      "cell_type": "code",
      "source": [
        "use_regime_split = False\n",
        "\n",
        "#Default models\n",
        "RF = True # perus random forest\n",
        "RF2 = False\n",
        "GB = False # perus gradient boost\n",
        "Hybrid = False\n",
        "\n",
        "#Looping models\n",
        "RF_feature_seek = False # random forest all combinations\n",
        "seek_all = False\n",
        "gb_loop = False\n",
        "\n",
        "#DATA\n",
        "FF5 = True\n",
        "FF5_long = False\n",
        "MSCI = False\n",
        "\n",
        "RSI = True\n",
        "\n",
        "local = False #ajetaanko colab vai oma kone\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-AF3FdwvaLPp"
      },
      "id": "-AF3FdwvaLPp",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify active model flags\n",
        "active_modes = [name for name, flag in zip(\n",
        "    ['RF', 'GB', 'RF_feature_seek', 'Hybrid', 'seek_all', 'gb_loop'],\n",
        "    [RF, GB, RF_feature_seek, Hybrid, seek_all, gb_loop]\n",
        ") if flag]\n",
        "\n",
        "if active_modes:\n",
        "    print(\"âœ… Active model modes:\", \", \".join(active_modes))\n",
        "else:\n",
        "    print(\"âš ï¸ No active model mode selected.\")\n",
        "\n",
        "# Check dataset toggles: exactly one must be True\n",
        "datasets = {\n",
        "    'FF5': FF5,\n",
        "    'FF5_long': FF5_long,\n",
        "    'MSCI': MSCI\n",
        "}\n",
        "active_datasets = [name for name, flag in datasets.items() if flag]\n",
        "\n",
        "if len(active_datasets) != 1:\n",
        "    raise ValueError(\"Error: Exactly one of [FF5, FF5_long, MSCI] must be True.\")\n",
        "else:\n",
        "    print(f\"ğŸ“Š Using dataset: {active_datasets[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ktRzXgrQHYX",
        "outputId": "9bcb19f3-dd86-4ba1-ab1c-5d5eb0971e75"
      },
      "id": "7ktRzXgrQHYX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Active model modes: RF\n",
            "ğŸ“Š Using dataset: FF5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4085d55c-e568-465c-9bcc-6013281c105d",
      "metadata": {
        "tags": [],
        "id": "4085d55c-e568-465c-9bcc-6013281c105d"
      },
      "outputs": [],
      "source": [
        "# # Import Required Libraries\n",
        "#\n",
        "# Import all necessary libraries for data manipulation, visualization,\n",
        "# machine learning, and regression analysis.\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import subprocess\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import statsmodels.api as sm\n",
        "from tabulate import tabulate\n",
        "\n",
        "from IPython.display import display, HTML\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not local:\n",
        "\n",
        "  %cd /content\n",
        "  !rm -rf Gradu\n",
        "  !git clone https://github.com/Elkkujou/Gradu.git\n",
        "  %cd /content/Gradu\n",
        "  !ls\n",
        "  xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "\n",
        "else:\n",
        "\n",
        "\n",
        "\n",
        "    repo_url = \"https://github.com/Elkkujou/Gradu.git\"\n",
        "    repo_name = \"Gradu\"  # Name of the cloned folder\n",
        "\n",
        "    # Check if the directory already exists\n",
        "    if os.path.exists(repo_name):\n",
        "        print(f\"Folder '{repo_name}' already exists. Pulling latest changes...\")\n",
        "        # Change to the existing repo folder and pull the latest updates\n",
        "        subprocess.run([\"git\", \"-C\", repo_name, \"pull\"], check=True)\n",
        "    else:\n",
        "        print(f\"Cloning repository into '{repo_name}'...\")\n",
        "        subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "    # List contents of the cloned repository\n",
        "    subprocess.run([\"ls\", repo_name], check=True)\n",
        "    xls_file = pd.ExcelFile(\"Gradu/THE_2ND_latest.xlsx\")\n",
        "\n"
      ],
      "metadata": {
        "id": "j2fmaZCMluYf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778bf44b-0f74-4fbb-dafb-b7953d7718dc"
      },
      "id": "j2fmaZCMluYf",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'Gradu'...\n",
            "remote: Enumerating objects: 926, done.\u001b[K\n",
            "remote: Counting objects: 100% (227/227), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 926 (delta 191), reused 128 (delta 128), pack-reused 699 (from 1)\u001b[K\n",
            "Receiving objects: 100% (926/926), 210.07 MiB | 29.45 MiB/s, done.\n",
            "Resolving deltas: 100% (458/458), done.\n",
            "/content/Gradu\n",
            " chatti_RF.ipynb\t\t      regime_prediction_msci.ipynb\n",
            " data+regimes.xlsx\t\t      regime_pred.txt\n",
            " Fama_french_XGBOOST.ipynb\t      RF_Gradu.ipynb\n",
            "'Financial turbulence.ipynb'\t     'RF REGIIMI HYVÃ„ TRAINING.ipynb'\n",
            " FT_source.xlsx\t\t\t     'RF_REGIIMI_HYVÃ„_TRAINING (MSCI).ipynb'\n",
            " Gradient_boost_malli.ipynb\t     'RF_regime (3).ipynb'\n",
            " MSCI_XGBOOST.ipynb\t\t      THE_2ND_latest.xlsx\n",
            " Regiimi_prediction.ipynb\t      THE_2ND.xlsx\n",
            " regime_prediction_famafrench.ipynb   THE_ONE.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5:\n",
        "  SHEET_NAME = \"ajodata_FF5\"\n",
        "  FEATURES = ['CPI%', 'LEI', 'EWMA_0.94','T10YFF']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = False"
      ],
      "metadata": {
        "id": "R_dU9PNF-Puv"
      },
      "id": "R_dU9PNF-Puv",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if FF5_long:\n",
        "  SHEET_NAME = \"ajodata_FF5_long\"\n",
        "  FEATURES = ['CPI%', 'LEI', 'EWMA_0.94','T10YFF']\n",
        "  FACTORS = [\n",
        "    'SMB',\n",
        "    'HML',\n",
        "    'CMA',\n",
        "    'RMW',\n",
        "    #'RF'\n",
        "]\n",
        "  BENCHMARK = ['Mkt']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "wJo52ErY-v0Q"
      },
      "id": "wJo52ErY-v0Q",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if MSCI:\n",
        "  SHEET_NAME = \"ajodata_MSCI\"\n",
        "  FEATURES = ['CPI%', 'T10YFF', 'CFNAI','Cape']\n",
        "  FACTORS = [\n",
        "    'Size',\n",
        "    'value',\n",
        "    'Quality',\n",
        "    'min_vola']\n",
        "  BENCHMARK = ['Us_standard']\n",
        "  show_benchmark = True"
      ],
      "metadata": {
        "id": "AT6Cf_ge_ApN"
      },
      "id": "AT6Cf_ge_ApN",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Prepare data"
      ],
      "metadata": {
        "id": "ZwuAM8venlx5"
      },
      "id": "ZwuAM8venlx5"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
      "metadata": {
        "tags": [],
        "id": "d31b30d7-aff9-4b1e-9eb2-3557c3993edc",
        "outputId": "7282af63-d0f6-4391-e5c6-e7d316298c5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Headers in the 'ajodata_FF5' sheet:\n",
            "Index(['Date', 'SMB', 'HML', 'RMW', 'CMA', 'Mkt', 'RF', 'Mkt-RF', 'GARCH_1M',\n",
            "       'CPI%', 'T10YFF', 'Amihud', 'LEI%', 'Cape', 'Cape %', 'GDP', 'TED',\n",
            "       'T10Y3', 'LEI', 'CFNAI', 'HV', 'VIX', 'EWMA', 'AvgShock', 'AR_Shock',\n",
            "       'RealizedVol', 'RelVol_12m', 'VolBucket', 'GARCH_1M_REL', 'EWMA_1M_REL',\n",
            "       'EWMA_0.94'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table class=\"dataframe table table-striped\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Description</th>\n",
              "      <th>Value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>First observation date</td>\n",
              "      <td>1963-07-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Last observation date</td>\n",
              "      <td>2024-11-30 00:00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Total number of observations</td>\n",
              "      <td>737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "df = xls_file.parse(SHEET_NAME)\n",
        "df.columns = df.columns.get_level_values(0)\n",
        "\n",
        "# Print headers dynamically\n",
        "print(f\"Headers in the '{SHEET_NAME}' sheet:\")\n",
        "print(df.columns)\n",
        "\n",
        "REGIMES_COLUMN = 'Predicted_reg'\n",
        "\n",
        "# Convert the leftmost column (assumed to be the date column) to datetime\n",
        "date_column = df.columns[0]\n",
        "df[date_column] = pd.to_datetime(df[date_column])\n",
        "\n",
        "# Retrieve first and last observation dates and count observations\n",
        "first_date = df[date_column].iloc[0]\n",
        "last_date = df[date_column].iloc[-1]\n",
        "n_observations = len(df)\n",
        "\n",
        "# Create a DataFrame with the information\n",
        "info_df = pd.DataFrame({\n",
        "    \"Description\": [\"First observation date\", \"Last observation date\", \"Total number of observations\"],\n",
        "    \"Value\": [first_date, last_date, n_observations]\n",
        "})\n",
        "\n",
        "# Display the results as a neat HTML table\n",
        "display(HTML(info_df.to_html(index=False, classes=\"table table-striped\", border=0)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if RSI:\n",
        "\n",
        "  # --- Add lagged 12â€‘month moving average for each factor return ---\n",
        "  for f in FACTORS:\n",
        "      # shift by 1 so that MA at time t uses returns t-12â€¦t-1\n",
        "      df[f + '_MA12'] = (\n",
        "          df[f]\n",
        "          .shift(1)                          # drop â€œtodayâ€\n",
        "          .rolling(window=12, min_periods=12)\n",
        "          .mean()\n",
        "      )\n",
        "\n",
        "  # update your FEATURES list\n",
        "  FEATURES += [f + '_MA12' for f in FACTORS]\n",
        "\n",
        "  print(\"âœ¨ Added lagged 12â€‘month MA columns:\", [f + '_MA12' for f in FACTORS])\n",
        "  print(\"ğŸ§© New FEATURES list:\", FEATURES)"
      ],
      "metadata": {
        "id": "3bGFK-QXeeDR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ec55110-4870-4da8-d407-50a236487c7c"
      },
      "id": "3bGFK-QXeeDR",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ¨ Added lagged 12â€‘month MA columns: ['SMB_MA12', 'HML_MA12', 'CMA_MA12', 'RMW_MA12']\n",
            "ğŸ§© New FEATURES list: ['CPI%', 'LEI', 'EWMA_0.94', 'T10YFF', 'SMB_MA12', 'HML_MA12', 'CMA_MA12', 'RMW_MA12']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- Define Helper Functions ---\n",
        "def annualized_return(returns):\n",
        "    \"\"\"Compute the compounded annualized return (assuming monthly returns).\"\"\"\n",
        "    return np.prod(1 + returns)**(12 / len(returns)) - 1\n",
        "\n",
        "def compute_metrics(returns):\n",
        "    \"\"\"\n",
        "    Compute key metrics for a returns series:\n",
        "      - Annualized Return\n",
        "      - Annualized Volatility (assuming monthly returns)\n",
        "      - Total Cumulative Return\n",
        "    \"\"\"\n",
        "    cumulative_returns = (1 + returns).cumprod()\n",
        "    total_cum_return = cumulative_returns.iloc[-1] - 1\n",
        "    ann_ret = annualized_return(returns)\n",
        "    ann_vol = np.std(returns) * np.sqrt(12)\n",
        "    return ann_ret, ann_vol, total_cum_return\n",
        "\n",
        "# --- Compute Metrics for Benchmark and Each Factor ---\n",
        "metrics = []\n",
        "\n",
        "# Compute metrics for the benchmark.\n",
        "benchmark_returns = df[BENCHMARK[0]]\n",
        "bench_ann_ret, bench_ann_vol, bench_cum_return = compute_metrics(benchmark_returns)\n",
        "metrics.append({\n",
        "    \"Strategy\": \"Benchmark\",\n",
        "    \"Annualized Return\": f\"{bench_ann_ret*100:.2f}%\",\n",
        "    \"Annualized Volatility\": f\"{bench_ann_vol*100:.2f}%\",\n",
        "    \"Total Cumulative Return\": f\"{bench_cum_return*100:.2f}%\"\n",
        "})\n",
        "\n",
        "# Compute metrics for each factor in FACTORS.\n",
        "for factor in FACTORS:\n",
        "    factor_returns = df[factor]\n",
        "    factor_ann_ret, factor_ann_vol, factor_cum_return = compute_metrics(factor_returns)\n",
        "    metrics.append({\n",
        "        \"Strategy\": factor,\n",
        "        \"Annualized Return\": f\"{factor_ann_ret*100:.2f}%\",\n",
        "        \"Annualized Volatility\": f\"{factor_ann_vol*100:.2f}%\",\n",
        "        \"Total Cumulative Return\": f\"{factor_cum_return*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame from the metrics.\n",
        "metrics_df = pd.DataFrame(metrics)\n",
        "\n",
        "# --- Display the Results as an HTML Table ---\n",
        "display(HTML(metrics_df.to_html(index=False)))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "phBSNpYXviHe",
        "outputId": "fd7768e3-6c38-44fb-ab68-3dd20a045024",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "id": "phBSNpYXviHe",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>Strategy</th>\n",
              "      <th>Annualized Return</th>\n",
              "      <th>Annualized Volatility</th>\n",
              "      <th>Total Cumulative Return</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>Benchmark</td>\n",
              "      <td>10.71%</td>\n",
              "      <td>15.46%</td>\n",
              "      <td>51691.71%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>SMB</td>\n",
              "      <td>1.89%</td>\n",
              "      <td>10.55%</td>\n",
              "      <td>216.75%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>HML</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>10.37%</td>\n",
              "      <td>473.19%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>CMA</td>\n",
              "      <td>2.88%</td>\n",
              "      <td>7.18%</td>\n",
              "      <td>472.94%</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>RMW</td>\n",
              "      <td>3.14%</td>\n",
              "      <td>7.67%</td>\n",
              "      <td>568.93%</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
      "metadata": {
        "tags": [],
        "id": "15e9d46d-2b85-4c9b-84e6-32e612d2c4a6",
        "outputId": "5566965f-f906-456e-b09e-35b672a16538",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of rows before cleaning: 737\n",
            "\n",
            "Missing values in FEATURES before cleaning:\n",
            "CPI%          0\n",
            "LEI           0\n",
            "EWMA_0.94     2\n",
            "T10YFF        0\n",
            "SMB_MA12     12\n",
            "HML_MA12     12\n",
            "CMA_MA12     12\n",
            "RMW_MA12     12\n",
            "dtype: int64\n",
            "\n",
            "Dropped 12 rows due to missing FEATURES.\n",
            "\n",
            "Missing values in FEATURES after cleaning:\n",
            "CPI%         0\n",
            "LEI          0\n",
            "EWMA_0.94    0\n",
            "T10YFF       0\n",
            "SMB_MA12     0\n",
            "HML_MA12     0\n",
            "CMA_MA12     0\n",
            "RMW_MA12     0\n",
            "dtype: int64\n",
            "\n",
            "Final dataset now has 725 rows.\n",
            "Target value counts:\n",
            "Winning Factor\n",
            "SMB    227\n",
            "RMW    214\n",
            "HML    168\n",
            "CMA    116\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Toggle for dropping rows with missing values in the FEATURES columns.\n",
        "drop_empty = True\n",
        "\n",
        "# 1) Print the initial number of rows.\n",
        "initial_rows = len(df)\n",
        "print(f\"Total number of rows before cleaning: {initial_rows}\")\n",
        "\n",
        "# 2) Show missingâ€value counts in FEATURES.\n",
        "missing_counts = df[FEATURES].isna().sum()\n",
        "print(\"\\nMissing values in FEATURES before cleaning:\")\n",
        "print(missing_counts)\n",
        "\n",
        "# 3) Drop any rows with NA in FEATURES, if requested.\n",
        "if drop_empty:\n",
        "    before = len(df)\n",
        "    df.dropna(subset=FEATURES, inplace=True)\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "    dropped = before - len(df)\n",
        "    print(f\"\\nDropped {dropped} rows due to missing FEATURES.\")\n",
        "else:\n",
        "    print(\"\\nKeeping all rows, including those with missing FEATURES.\")\n",
        "\n",
        "# 4) Reâ€check that FEATURES are now complete:\n",
        "print(\"\\nMissing values in FEATURES after cleaning:\")\n",
        "print(df[FEATURES].isna().sum())\n",
        "\n",
        "# 5) Compute your target column inâ€place.\n",
        "df['Winning Factor'] = df[FACTORS].idxmax(axis=1).astype('category')\n",
        "\n",
        "# 6) (Optionally) create a numeric code column\n",
        "df['Winning Factor Code'] = df['Winning Factor'].cat.codes\n",
        "\n",
        "# 7) Quick summary:\n",
        "print(f\"\\nFinal dataset now has {len(df)} rows.\")\n",
        "print(\"Target value counts:\")\n",
        "print(df['Winning Factor'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if use_regime_split:\n",
        "\n",
        "    # --- Regime Mapping & Conversion to Numeric Codes (Dynamic) ---\n",
        "\n",
        "    # Dynamically extract the unique values in the REGIMES_COLUMN.\n",
        "    unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "\n",
        "    # Convert the Regimes column to a categorical type with the unique values, ordered alphabetically.\n",
        "    df[REGIMES_COLUMN] = pd.Categorical(df[REGIMES_COLUMN], categories=sorted(unique_regimes), ordered=True)\n",
        "\n",
        "    # Create a dictionary mapping numeric codes to the regime names based on the unique values.\n",
        "    regime_mapping = {i: cat for i, cat in enumerate(df[REGIMES_COLUMN].cat.categories)}\n",
        "\n",
        "    # Now encode the Regimes column as numeric codes.\n",
        "    df[REGIMES_COLUMN] = df[REGIMES_COLUMN].cat.codes\n",
        "\n",
        "    # Create a mapping from numeric codes to original regime names.\n",
        "    regime_short_mapping = {code: name for code, name in regime_mapping.items()}\n",
        "\n",
        "    # Calculate the number of observations for each regime using value_counts (without reindexing).\n",
        "    obs_counts = df[REGIMES_COLUMN].value_counts(sort=False)\n",
        "\n",
        "    # Create a DataFrame preview of the regime mapping, including observation counts.\n",
        "    mapping_table_data = []\n",
        "    for code in regime_mapping.keys():\n",
        "        mapping_table_data.append({\n",
        "            \"Numeric Code\": code,\n",
        "            \"Original Name\": regime_mapping.get(code, \"N/A\"),\n",
        "            \"Observations\": obs_counts.get(code, 0)\n",
        "        })\n",
        "\n",
        "    # Append a row with the total observations.\n",
        "    total_obs = obs_counts.sum()\n",
        "    mapping_table_data.append({\n",
        "        \"Numeric Code\": \"\",\n",
        "        \"Original Name\": \"Total\",\n",
        "        \"Observations\": total_obs\n",
        "    })\n",
        "\n",
        "    # Create the DataFrame for regime mapping preview and print.\n",
        "    regime_mapping_df = pd.DataFrame(mapping_table_data)\n",
        "\n",
        "    from tabulate import tabulate\n",
        "    print(\"Preview of Dynamic Regime Mapping:\")\n",
        "    print(tabulate(regime_mapping_df, headers=\"keys\", tablefmt=\"psql\", showindex=False))\n"
      ],
      "metadata": {
        "id": "wYgvlvGRUUG4"
      },
      "id": "wYgvlvGRUUG4",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Models"
      ],
      "metadata": {
        "id": "0wYzowb5Xdau"
      },
      "id": "0wYzowb5Xdau"
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature seek"
      ],
      "metadata": {
        "id": "KkFWbyO6XlRZ"
      },
      "id": "KkFWbyO6XlRZ"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF_feature_seek:\n",
        "    import itertools\n",
        "    import os\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "    # --------------------------\n",
        "    # Parameters for Feature & Training Window Search\n",
        "    # --------------------------\n",
        "    min_features = 2                  # minimum number of features in a subset\n",
        "    max_features = len(FEATURES)      # maximum number of features (or set to a smaller number if desired)\n",
        "\n",
        "    # Define fixed rolling window sizes (in years) to test (assuming monthly data)\n",
        "    training_window_years = [5, 10, 15, 20]\n",
        "\n",
        "    # Also run an expanding window experiment\n",
        "    run_expanding_window = True\n",
        "\n",
        "    # Independent variable: minimum number of observations required for making a prediction.\n",
        "    # This is now decoupled from the training window calculation.\n",
        "    min_obs_for_prediction = 60  # adjust this value as desired\n",
        "\n",
        "    output_filename = \"feature_subset_results.csv\"\n",
        "    if os.path.exists(output_filename):\n",
        "        os.remove(output_filename)\n",
        "\n",
        "    # Ensure the data is sorted by date.\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # --------------------------\n",
        "    # Outer Loop: Fixed Rolling Window Modes\n",
        "    # --------------------------\n",
        "    for years in training_window_years:\n",
        "        # Convert years to number of observations (assume 12 obs per year)\n",
        "        rolling_window_size = years * 12\n",
        "        # Ensure predictions start only after both the rolling window and the independent minimum are met.\n",
        "        start_index = max(min_obs_for_prediction, rolling_window_size)\n",
        "        print(f\"\\n--- Testing fixed rolling window of {years} years \"\n",
        "              f\"({rolling_window_size} observations, starting predictions at index {start_index}) ---\")\n",
        "        outer_start_time = time.time()\n",
        "\n",
        "        # Inner loop over feature subset sizes\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            # Loop over all combinations of size r\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination: {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # Loop over test rows, starting when we have enough training data\n",
        "                for i in range(start_index, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "\n",
        "                    # Build fixed rolling training window (most recent rolling_window_size observations)\n",
        "                    train_window = df_sorted.iloc[i - rolling_window_size : i].copy()\n",
        "\n",
        "                    # Ensure the last training observation is strictly before test row date\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    # (Optional) Regime check if use_regime_split is True:\n",
        "                    if use_regime_split:\n",
        "                        regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                        insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                        if insufficient_regimes:\n",
        "                            continue\n",
        "                        current_regime = test_row[REGIMES_COLUMN]\n",
        "                        train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                        if len(train_window) < min_obs_regime:\n",
        "                            continue\n",
        "\n",
        "                    # Prepare training data for the current feature subset.\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    # Train the RandomForest model.\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    # Use the last row of the training window as test data.\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    # Map the predicted probabilities to the full set of FACTORS.\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    # months_ahead: how many months ahead the prediction is (optional usage)\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    # Collect feature levels for logging\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    # Create a result row\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": years,   # <--- Record the training window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                # End of inner test row loop for this feature combination.\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Results for combination {current_features} appended to CSV. \"\n",
        "                          f\"Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed fixed rolling window of {years} years in {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "    # --------------------------\n",
        "    # Expanding Window Mode\n",
        "    # --------------------------\n",
        "    if run_expanding_window:\n",
        "        print(\"\\n--- Testing Expanding Window Mode ---\")\n",
        "        outer_start_time = time.time()\n",
        "        for r in range(min_features, max_features + 1):\n",
        "            for comb in itertools.combinations(FEATURES, r):\n",
        "                current_features = list(comb)\n",
        "                inner_start_time = time.time()\n",
        "                print(f\"\\nTesting feature combination (expanding): {current_features}\")\n",
        "                results = []\n",
        "\n",
        "                # In expanding mode, the training window goes from the start until the test row.\n",
        "                # Start predictions only after the minimum observation threshold is met.\n",
        "                for i in range(min_obs_for_prediction, len(df_sorted)):\n",
        "                    test_row = df_sorted.iloc[i]\n",
        "                    Predicted_month = test_row['Date']\n",
        "                    train_window = df_sorted.iloc[:i].copy()\n",
        "                    if train_window.empty:\n",
        "                        continue\n",
        "\n",
        "                    last_train_date = train_window['Date'].iloc[-1]\n",
        "                    if (last_train_date.year == Predicted_month.year) and \\\n",
        "                       (last_train_date.month >= Predicted_month.month):\n",
        "                        continue\n",
        "\n",
        "                    X_train = train_window[list(current_features)].dropna()\n",
        "                    y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "                    if len(X_train) < 1:\n",
        "                        continue\n",
        "\n",
        "                    rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "                    rf_model.fit(X_train, y_train)\n",
        "\n",
        "                    X_test = train_window[list(current_features)].iloc[[-1]].dropna()\n",
        "                    if X_test.empty:\n",
        "                        continue\n",
        "\n",
        "                    predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "                    predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "                    full_probs = np.zeros(len(FACTORS))\n",
        "                    for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "                        try:\n",
        "                            idx = FACTORS.index(cls)\n",
        "                            full_probs[idx] = prob\n",
        "                        except ValueError:\n",
        "                            continue\n",
        "\n",
        "                    allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "\n",
        "                    months_ahead = (\n",
        "                        (Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month)\n",
        "                    )\n",
        "\n",
        "                    feature_levels = {\n",
        "                        f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in current_features\n",
        "                    }\n",
        "\n",
        "                    result = {\n",
        "                        \"TrainingWindowYears\": \"expanding\",  # <--- Indicate expanding window\n",
        "                        \"Features_used\": str(current_features),\n",
        "                        \"Predicted_month\": Predicted_month,\n",
        "                        \"Allocated_Return\": allocated_return,\n",
        "                        \"Predicted_Winner\": predicted_winner,\n",
        "                        \"Actual_Winner\": test_row['Winning Factor'],\n",
        "                        \"Prediction_Horizon_Months\": months_ahead,\n",
        "                        **feature_levels\n",
        "                    }\n",
        "                    results.append(result)\n",
        "\n",
        "                if results:\n",
        "                    df_results_comb = pd.DataFrame(results)\n",
        "                    if not os.path.exists(output_filename):\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=True, index=False)\n",
        "                    else:\n",
        "                        df_results_comb.to_csv(output_filename, mode='a', header=False, index=False)\n",
        "                    elapsed_inner = time.time() - inner_start_time\n",
        "                    minutes = int(elapsed_inner // 60)\n",
        "                    seconds = int(elapsed_inner % 60)\n",
        "                    print(f\"Expanding window: Results for combination {current_features} \"\n",
        "                          f\"appended to CSV. Time taken: {minutes:02d}:{seconds:02d}\")\n",
        "\n",
        "        elapsed_outer = time.time() - outer_start_time\n",
        "        minutes = int(elapsed_outer // 60)\n",
        "        seconds = int(elapsed_outer % 60)\n",
        "        print(f\"Completed Expanding Window Mode in {minutes:02d}:{seconds:02d}\")\n"
      ],
      "metadata": {
        "id": "kraj1YkNhEq4"
      },
      "id": "kraj1YkNhEq4",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Seek all"
      ],
      "metadata": {
        "id": "QkeGs9s9dExF"
      },
      "id": "QkeGs9s9dExF"
    },
    {
      "cell_type": "code",
      "source": [
        "if seek_all:\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import csv\n",
        "  import itertools\n",
        "  import time\n",
        "  import math\n",
        "  from sklearn.ensemble import RandomForestClassifier\n",
        "  from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # Assumes `df` (with 'Date' & 'Winning Factor'),\n",
        "  # `FACTORS` (list of all factor names) and\n",
        "  # `FEATURES` (base feature list) are defined above\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # Toggle featureâ€‘looping on/off\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  loop_features = False   # False â‡’ single run on FEATURES; True â‡’ sweep always+optional\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 1) Define alwaysâ€‘on & optional features\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  always_features = [\n",
        "      \"CMA_MA12\", \"SMB_MA12\", \"RMW_MA12\", \"HML_MA12\",\n",
        "      \"CPI%\"\n",
        "  ]\n",
        "  optional_features = [\n",
        "      \"LEI%\", \"Cape\", \"Cape %\", \"TED\",\n",
        "      \"T10Y3\", \"LEI\", \"AR_Shock\", \"HV\",\n",
        "      \"EWMA_0.94\", \"T10YFF\", \"CFNAI\", \"GARCH_1M\", \"VIX\",\"BAA10Y\"\n",
        "  ]\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 1a) Control how many optional features per combo\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  min_optional = 1   # minimum number of optional features in each combo\n",
        "  max_optional = 5   # maximum number of optional features in each combo\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 2) Build feature_combinations (with new constraints)\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  volatility_features = {\"GARCH_1M\", \"VIX\", \"AR_Shock\", \"HV\", \"EWMA_0.94\"}\n",
        "  lei_group          = {\"LEI\", \"LEI%\", \"CFNAI\"}\n",
        "  cape_group         = {\"Cape\", \"Cape %\"}\n",
        "\n",
        "  if loop_features:\n",
        "      def valid_combo(combo):\n",
        "          combo = set(combo)\n",
        "          # 1) at most 2 volatility measures\n",
        "          if len(combo & volatility_features) > 2:\n",
        "              return False\n",
        "          # 2) T10Y3 and T10YFF cannot coâ€‘exist\n",
        "          if {\"T10Y3\", \"T10YFF\"} <= combo:\n",
        "              return False\n",
        "          # 3) only one of LEI, LEI%, CFNAI\n",
        "          if len(combo & lei_group) > 1:\n",
        "              return False\n",
        "          # 4) only one of Cape, Cape %\n",
        "          if len(combo & cape_group) > 1:\n",
        "              return False\n",
        "          return True\n",
        "\n",
        "      feature_combinations = [\n",
        "          always_features + list(combo)\n",
        "          for r in range(min_optional, max_optional + 1)\n",
        "          for combo in itertools.combinations(optional_features, r)\n",
        "          if valid_combo(combo)\n",
        "      ]\n",
        "  else:\n",
        "      feature_combinations = [FEATURES]\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 3) RF hyperparameter template (excluding max_features)\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  param_grid_template = {\n",
        "      'n_estimators':      [100, 300, 500],\n",
        "      'max_depth':         [None, 7, 10, 15],\n",
        "      'min_samples_split': [2, 4, 6],\n",
        "      'min_samples_leaf':  [1, 3, 5, 7],\n",
        "      'bootstrap':         [False, True],\n",
        "      'n_jobs':            [-1]\n",
        "  }\n",
        "  base_param_list = list(ParameterGrid(param_grid_template))\n",
        "\n",
        "  # compute total iterations for progress display\n",
        "  total_iterations = sum(\n",
        "      len(base_param_list) * len(range(2, len(feat_set) + 1, 2))\n",
        "      for feat_set in feature_combinations\n",
        "  )\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 4) CSV logging setup\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  csv_file = 'rf_feature_search.csv'\n",
        "  fieldnames = [\n",
        "      'Iteration','Training_Window','Features','Hyperparameters',\n",
        "      'Num_Preds','First_Pred','Last_Pred',\n",
        "      'CumAlloc_Post2000','CumEqual_Post2000',\n",
        "      'CumAlloc_Total','CumEqual_Total',\n",
        "      'Sharpe_Post2000','Win_Count_Post2000'\n",
        "  ]\n",
        "  with open(csv_file, 'w', newline='') as f:\n",
        "      writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
        "      writer.writeheader()\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 5) Rollingâ€‘window & data settings\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  rolling_window_size = 60   # months in each fixed window\n",
        "  min_months_train    = 60   # minimum months of history required\n",
        "  min_obs_train       = 0    # minimum non-missing rows in X_train\n",
        "  use_fixed_window    = True # True: fixed-length rolling window; False: expanding window\n",
        "\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  # 6) Loop over feature sets & hyperparameters\n",
        "  # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
        "  iteration = 0\n",
        "  summary_records = []\n",
        "\n",
        "  for feat_set in feature_combinations:\n",
        "      n_feats = len(feat_set)\n",
        "      # for n_feats = 8, this gives [2, 4, 6, 8]\n",
        "      max_features_opts = list(range(2, n_feats + 1, 2))\n",
        "\n",
        "      # inject max_features into each base parameter combination\n",
        "      param_list = [\n",
        "          {**p, 'max_features': mf}\n",
        "          for p in base_param_list\n",
        "          for mf in max_features_opts\n",
        "      ]\n",
        "\n",
        "      for rf_params in param_list:\n",
        "          iteration += 1\n",
        "          start_time = time.perf_counter()\n",
        "\n",
        "          # --- print at start ---\n",
        "          print(f\"\\n=== Iteration {iteration}/{total_iterations} ===\")\n",
        "          if loop_features:\n",
        "              print(f\"Alwaysâ€‘on features ({len(always_features)}): {always_features}\")\n",
        "              print(f\"Optional count range: {min_optional}â€“{max_optional}\")\n",
        "          print(f\"Using features ({n_feats}): {feat_set}\")\n",
        "          print(f\"RF hyperparameters: {rf_params}\")\n",
        "\n",
        "          # drop rows with missing values in this feature set only\n",
        "          df_iter = df.dropna(subset=feat_set).copy()\n",
        "          df_sorted = df_iter.sort_values('Date').reset_index(drop=True)\n",
        "          results = []\n",
        "\n",
        "          # perâ€‘row prediction loop\n",
        "          for i in range(1, len(df_sorted)):\n",
        "              test_row = df_sorted.iloc[i]\n",
        "\n",
        "              # build training window from prior rows\n",
        "              if use_fixed_window:\n",
        "                  start_idx    = max(0, i - rolling_window_size)\n",
        "                  train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "              else:\n",
        "                  train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "              if len(train_window) < min_months_train:\n",
        "                  continue\n",
        "\n",
        "              # prepare training data\n",
        "              X_train = train_window[feat_set].dropna()\n",
        "              y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "              if len(X_train) < min_obs_train:\n",
        "                  continue\n",
        "\n",
        "              # fit RandomForest\n",
        "              rf = RandomForestClassifier(**rf_params, random_state=42)\n",
        "              rf.fit(X_train, y_train)\n",
        "\n",
        "              # predict on the last available row in train_window\n",
        "              X_test = train_window[feat_set].iloc[[-1]].dropna()\n",
        "              if X_test.empty:\n",
        "                  continue\n",
        "\n",
        "              probs      = rf.predict_proba(X_test)[0]\n",
        "              full_probs = np.zeros(len(FACTORS))\n",
        "              for cls, p in zip(rf.classes_, probs):\n",
        "                  full_probs[FACTORS.index(cls)] = p\n",
        "\n",
        "              alloc_ret = (full_probs * test_row[FACTORS].values).sum()\n",
        "              eq_ret    = test_row[FACTORS].mean()\n",
        "\n",
        "              results.append({\n",
        "                  'Date': test_row['Date'],\n",
        "                  'Allocated_Return': alloc_ret,\n",
        "                  'Equal_Weight_Return': eq_ret\n",
        "              })\n",
        "\n",
        "          # compute summary metrics\n",
        "          res_df = pd.DataFrame(results).sort_values('Date')\n",
        "          if res_df.empty:\n",
        "              summary = dict.fromkeys(fieldnames, np.nan)\n",
        "              summary.update({\n",
        "                  'Iteration': iteration,\n",
        "                  'Training_Window': rolling_window_size,\n",
        "                  'Features': \",\".join(feat_set),\n",
        "                  'Hyperparameters': \";\".join(f\"{k}={v}\" for k,v in rf_params.items()),\n",
        "                  'Num_Preds': 0\n",
        "              })\n",
        "          else:\n",
        "              post2k        = res_df[res_df['Date'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "              cum_alloc_2k  = (1 + post2k['Allocated_Return']).prod() - 1\n",
        "              cum_eq_2k     = (1 + post2k['Equal_Weight_Return']).prod()  - 1\n",
        "              cum_alloc_tot = (1 + res_df['Allocated_Return']).prod()     - 1\n",
        "              cum_eq_tot    = (1 + res_df['Equal_Weight_Return']).prod()  - 1\n",
        "\n",
        "              sharpe_2k = (post2k['Allocated_Return'].mean() /\n",
        "                          post2k['Allocated_Return'].std()) * np.sqrt(12)\n",
        "              win2k = (post2k['Allocated_Return'] > post2k['Equal_Weight_Return']).sum()\n",
        "\n",
        "              summary = {\n",
        "                  'Iteration': iteration,\n",
        "                  'Training_Window': rolling_window_size,\n",
        "                  'Features': \",\".join(feat_set),\n",
        "                  'Hyperparameters': \";\".join(f\"{k}={v}\" for k,v in rf_params.items()),\n",
        "                  'Num_Preds': len(res_df),\n",
        "                  'First_Pred': res_df['Date'].iloc[0].strftime(\"%Y-%m-%d\"),\n",
        "                  'Last_Pred':  res_df['Date'].iloc[-1].strftime(\"%Y-%m-%d\"),\n",
        "                  'CumAlloc_Post2000': cum_alloc_2k,\n",
        "                  'CumEqual_Post2000':  cum_eq_2k,\n",
        "                  'CumAlloc_Total':      cum_alloc_tot,\n",
        "                  'CumEqual_Total':       cum_eq_tot,\n",
        "                  'Sharpe_Post2000':     sharpe_2k,\n",
        "                  'Win_Count_Post2000':  win2k\n",
        "              }\n",
        "\n",
        "          # write one line to CSV\n",
        "          with open(csv_file, 'a', newline='') as f:\n",
        "              writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=';')\n",
        "              writer.writerow(summary)\n",
        "              f.flush()\n",
        "\n",
        "          # update running ranks & print end summary\n",
        "          summary_records.append(summary)\n",
        "          df_sum = pd.DataFrame(summary_records)\n",
        "          df_sum['Rank_CumAlloc_Post2000']   = df_sum['CumAlloc_Post2000'].rank(ascending=False, method='min')\n",
        "          df_sum['Rank_Sharpe_Post2000']     = df_sum['Sharpe_Post2000'].rank(ascending=False, method='min')\n",
        "          df_sum['Rank_CumAlloc_Total']      = df_sum['CumAlloc_Total'].rank(ascending=False, method='min')\n",
        "          df_sum['Rank_Win_Count_Post2000']  = df_sum['Win_Count_Post2000'].rank(ascending=False, method='min')\n",
        "\n",
        "          cur = df_sum.iloc[-1]\n",
        "          duration = time.perf_counter() - start_time\n",
        "\n",
        "          # print endâ€‘ofâ€‘iteration stats\n",
        "          print(f\"Completed iteration {iteration}/{total_iterations} in {duration:.1f}s\")\n",
        "          print(f\"  CumAlloc_Post2000: {cur['CumAlloc_Post2000']:.4f} (rank {int(cur['Rank_CumAlloc_Post2000'])}/{iteration})\")\n",
        "          print(f\"  Sharpe_Post2000:   {cur['Sharpe_Post2000']:.4f} (rank {int(cur['Rank_Sharpe_Post2000'])}/{iteration})\")\n",
        "          print(f\"  CumAlloc_Total:    {cur['CumAlloc_Total']:.4f} (rank {int(cur['Rank_CumAlloc_Total'])}/{iteration})\")\n",
        "          print(f\"  Win_Count_Post2000:{int(cur['Win_Count_Post2000'])} (rank {int(cur['Rank_Win_Count_Post2000'])}/{iteration})\")\n",
        "          print(f\"  EqualWeight_Post2000: {cur['CumEqual_Post2000']:.4f}\")\n",
        "          print(f\"  EqualWeight_Total:     {cur['CumEqual_Total']:.4f}\")\n"
      ],
      "metadata": {
        "id": "b_ac9qZ7ir9I"
      },
      "id": "b_ac9qZ7ir9I",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random forest"
      ],
      "metadata": {
        "id": "MrSJ4xhmDuzE"
      },
      "id": "MrSJ4xhmDuzE"
    },
    {
      "cell_type": "code",
      "source": [
        "if RF:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "\n",
        "    RF1_FEATURES = FEATURES\n",
        "\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Min obs per regime if splitting\n",
        "    min_obs_train = 0         # Min total obs after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # New toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    # -------------------\n",
        "    # New: Tunable hyperparameter for parallel jobs\n",
        "    # -------------------\n",
        "    n_jobs = -1  # Set to -1 to use all available cores; adjust as needed\n",
        "\n",
        "    # -------------------\n",
        "    # Hyperparameter Settings for Random Forest\n",
        "    # -------------------\n",
        "    if default_hyperparameters:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': 'sqrt',\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 1,\n",
        "            'bootstrap': True,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "    else:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': None,\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 5,\n",
        "            'bootstrap': False,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check that we have enough training rows\n",
        "        if len(train_window) < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training rows ({len(train_window)} rows). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  ğŸ”´ Regime split active. Insufficient data in: {', '.join(regime_str_list)}. Skipping.\\n\")\n",
        "                continue\n",
        "\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  ğŸ”´ Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\\n\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure last training date < test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train using RF1_FEATURES\n",
        "        X_train = train_window[RF1_FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\\n\")\n",
        "            continue\n",
        "\n",
        "        # Fit RandomForest\n",
        "        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict using the last row in training window\n",
        "        X_test = train_window[RF1_FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\\n\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = rf_model.predict_proba(X_test)[0]\n",
        "        predicted_winner = rf_model.classes_[predicted_probabilities.argmax()]\n",
        "\n",
        "        # Map probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, prob in zip(rf_model.classes_, predicted_probabilities):\n",
        "            try:\n",
        "                idx = FACTORS.index(cls)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        # Calculate returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Gather additional info\n",
        "        tree_depths = [estimator.tree_.max_depth for estimator in rf_model.estimators_]\n",
        "        avg_depth = np.mean(tree_depths)\n",
        "        max_depth_val = np.max(tree_depths)\n",
        "        months_ahead = ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                        (Predicted_month.month - last_train_date.month))\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in RF1_FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth_val,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build results DataFrame\n",
        "    # -------------------\n",
        "    results_df_rf = pd.DataFrame(results)\n",
        "    print(\"Final results_df_rf columns:\", results_df_rf.columns.tolist())\n",
        "    display(results_df_rf.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # 4) Cumulative Returns from 2000-01-01\n",
        "    # -------------------\n",
        "    filtered_results = results_df_rf[results_df_rf['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered_results.empty:\n",
        "        cum_return_allocated = (1 + filtered_results['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal = (1 + filtered_results['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_pred_month = filtered_results.iloc[0]['Predicted_month']\n",
        "        last_pred_month = filtered_results.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_pred_month.date(), last_pred_month.date(),\n",
        "            cum_return_allocated, cum_return_equal))\n",
        "    else:\n",
        "        print(\"No predictions from 1 Jan 2000 onwards.\")\n",
        "\n",
        "    # -------------------\n",
        "    # 5) Total Time Cumulative Returns\n",
        "    # -------------------\n",
        "    if not results_df_rf.empty:\n",
        "        cum_return_allocated_total = (1 + results_df_rf['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal_total = (1 + results_df_rf['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_total_month = results_df_rf.iloc[0]['Predicted_month']\n",
        "        last_total_month = results_df_rf.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_total_month.date(), last_total_month.date(),\n",
        "            cum_return_allocated_total, cum_return_equal_total))\n",
        "    else:\n",
        "        print(\"No predictions available for total time.\")"
      ],
      "metadata": {
        "id": "YjHj_tiKSCYZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6dddedeb-3ca9-478b-d9b4-308b0fc689af"
      },
      "id": "YjHj_tiKSCYZ",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test row date: 1964-08-30 - Insufficient training rows (1 rows). Skipping.\n",
            "Test row date: 1964-09-30 - Insufficient training rows (2 rows). Skipping.\n",
            "Test row date: 1964-10-30 - Insufficient training rows (3 rows). Skipping.\n",
            "Test row date: 1964-11-30 - Insufficient training rows (4 rows). Skipping.\n",
            "Test row date: 1964-12-30 - Insufficient training rows (5 rows). Skipping.\n",
            "Test row date: 1965-01-30 - Insufficient training rows (6 rows). Skipping.\n",
            "Test row date: 1965-02-28 - Insufficient training rows (7 rows). Skipping.\n",
            "Test row date: 1965-03-30 - Insufficient training rows (8 rows). Skipping.\n",
            "Test row date: 1965-04-30 - Insufficient training rows (9 rows). Skipping.\n",
            "Test row date: 1965-05-30 - Insufficient training rows (10 rows). Skipping.\n",
            "Test row date: 1965-06-30 - Insufficient training rows (11 rows). Skipping.\n",
            "Test row date: 1965-07-30 - Insufficient training rows (12 rows). Skipping.\n",
            "Test row date: 1965-08-30 - Insufficient training rows (13 rows). Skipping.\n",
            "Test row date: 1965-09-30 - Insufficient training rows (14 rows). Skipping.\n",
            "Test row date: 1965-10-30 - Insufficient training rows (15 rows). Skipping.\n",
            "Test row date: 1965-11-30 - Insufficient training rows (16 rows). Skipping.\n",
            "Test row date: 1965-12-30 - Insufficient training rows (17 rows). Skipping.\n",
            "Test row date: 1966-01-30 - Insufficient training rows (18 rows). Skipping.\n",
            "Test row date: 1966-02-28 - Insufficient training rows (19 rows). Skipping.\n",
            "Test row date: 1966-03-30 - Insufficient training rows (20 rows). Skipping.\n",
            "Test row date: 1966-04-30 - Insufficient training rows (21 rows). Skipping.\n",
            "Test row date: 1966-05-30 - Insufficient training rows (22 rows). Skipping.\n",
            "Test row date: 1966-06-30 - Insufficient training rows (23 rows). Skipping.\n",
            "Test row date: 1966-07-30 - Insufficient training rows (24 rows). Skipping.\n",
            "Test row date: 1966-08-30 - Insufficient training rows (25 rows). Skipping.\n",
            "Test row date: 1966-09-30 - Insufficient training rows (26 rows). Skipping.\n",
            "Test row date: 1966-10-30 - Insufficient training rows (27 rows). Skipping.\n",
            "Test row date: 1966-11-30 - Insufficient training rows (28 rows). Skipping.\n",
            "Test row date: 1966-12-30 - Insufficient training rows (29 rows). Skipping.\n",
            "Test row date: 1967-01-30 - Insufficient training rows (30 rows). Skipping.\n",
            "Test row date: 1967-02-28 - Insufficient training rows (31 rows). Skipping.\n",
            "Test row date: 1967-03-30 - Insufficient training rows (32 rows). Skipping.\n",
            "Test row date: 1967-04-30 - Insufficient training rows (33 rows). Skipping.\n",
            "Test row date: 1967-05-30 - Insufficient training rows (34 rows). Skipping.\n",
            "Test row date: 1967-06-30 - Insufficient training rows (35 rows). Skipping.\n",
            "Test row date: 1967-07-30 - Insufficient training rows (36 rows). Skipping.\n",
            "Test row date: 1967-08-30 - Insufficient training rows (37 rows). Skipping.\n",
            "Test row date: 1967-09-30 - Insufficient training rows (38 rows). Skipping.\n",
            "Test row date: 1967-10-30 - Insufficient training rows (39 rows). Skipping.\n",
            "Test row date: 1967-11-30 - Insufficient training rows (40 rows). Skipping.\n",
            "Test row date: 1967-12-30 - Insufficient training rows (41 rows). Skipping.\n",
            "Test row date: 1968-01-30 - Insufficient training rows (42 rows). Skipping.\n",
            "Test row date: 1968-02-29 - Insufficient training rows (43 rows). Skipping.\n",
            "Test row date: 1968-03-30 - Insufficient training rows (44 rows). Skipping.\n",
            "Test row date: 1968-04-30 - Insufficient training rows (45 rows). Skipping.\n",
            "Test row date: 1968-05-30 - Insufficient training rows (46 rows). Skipping.\n",
            "Test row date: 1968-06-30 - Insufficient training rows (47 rows). Skipping.\n",
            "Test row date: 1968-07-30 - Insufficient training rows (48 rows). Skipping.\n",
            "Test row date: 1968-08-30 - Insufficient training rows (49 rows). Skipping.\n",
            "Test row date: 1968-09-30 - Insufficient training rows (50 rows). Skipping.\n",
            "Test row date: 1968-10-30 - Insufficient training rows (51 rows). Skipping.\n",
            "Test row date: 1968-11-30 - Insufficient training rows (52 rows). Skipping.\n",
            "Test row date: 1968-12-30 - Insufficient training rows (53 rows). Skipping.\n",
            "Test row date: 1969-01-30 - Insufficient training rows (54 rows). Skipping.\n",
            "Test row date: 1969-02-28 - Insufficient training rows (55 rows). Skipping.\n",
            "Test row date: 1969-03-30 - Insufficient training rows (56 rows). Skipping.\n",
            "Test row date: 1969-04-30 - Insufficient training rows (57 rows). Skipping.\n",
            "Test row date: 1969-05-30 - Insufficient training rows (58 rows). Skipping.\n",
            "Test row date: 1969-06-30 - Insufficient training rows (59 rows). Skipping.\n",
            "Test row date: 1969-07-30 -> Model trained, prediction made (using: 1964-07-30 - 1969-06-30)\n",
            "Test row date: 1969-08-30 -> Model trained, prediction made (using: 1964-08-30 - 1969-07-30)\n",
            "Test row date: 1969-09-30 -> Model trained, prediction made (using: 1964-09-30 - 1969-08-30)\n",
            "Test row date: 1969-10-30 -> Model trained, prediction made (using: 1964-10-30 - 1969-09-30)\n",
            "Test row date: 1969-11-30 -> Model trained, prediction made (using: 1964-11-30 - 1969-10-30)\n",
            "Test row date: 1969-12-30 -> Model trained, prediction made (using: 1964-12-30 - 1969-11-30)\n",
            "Test row date: 1970-01-30 -> Model trained, prediction made (using: 1965-01-30 - 1969-12-30)\n",
            "Test row date: 1970-02-28 -> Model trained, prediction made (using: 1965-02-28 - 1970-01-30)\n",
            "Test row date: 1970-03-30 -> Model trained, prediction made (using: 1965-03-30 - 1970-02-28)\n",
            "Test row date: 1970-04-30 -> Model trained, prediction made (using: 1965-04-30 - 1970-03-30)\n",
            "Test row date: 1970-05-30 -> Model trained, prediction made (using: 1965-05-30 - 1970-04-30)\n",
            "Test row date: 1970-06-30 -> Model trained, prediction made (using: 1965-06-30 - 1970-05-30)\n",
            "Test row date: 1970-07-30 -> Model trained, prediction made (using: 1965-07-30 - 1970-06-30)\n",
            "Test row date: 1970-08-30 -> Model trained, prediction made (using: 1965-08-30 - 1970-07-30)\n",
            "Test row date: 1970-09-30 -> Model trained, prediction made (using: 1965-09-30 - 1970-08-30)\n",
            "Test row date: 1970-10-30 -> Model trained, prediction made (using: 1965-10-30 - 1970-09-30)\n",
            "Test row date: 1970-11-30 -> Model trained, prediction made (using: 1965-11-30 - 1970-10-30)\n",
            "Test row date: 1970-12-30 -> Model trained, prediction made (using: 1965-12-30 - 1970-11-30)\n",
            "Test row date: 1971-01-30 -> Model trained, prediction made (using: 1966-01-30 - 1970-12-30)\n",
            "Test row date: 1971-02-28 -> Model trained, prediction made (using: 1966-02-28 - 1971-01-30)\n",
            "Test row date: 1971-03-30 -> Model trained, prediction made (using: 1966-03-30 - 1971-02-28)\n",
            "Test row date: 1971-04-30 -> Model trained, prediction made (using: 1966-04-30 - 1971-03-30)\n",
            "Test row date: 1971-05-30 -> Model trained, prediction made (using: 1966-05-30 - 1971-04-30)\n",
            "Test row date: 1971-06-30 -> Model trained, prediction made (using: 1966-06-30 - 1971-05-30)\n",
            "Test row date: 1971-07-30 -> Model trained, prediction made (using: 1966-07-30 - 1971-06-30)\n",
            "Test row date: 1971-08-30 -> Model trained, prediction made (using: 1966-08-30 - 1971-07-30)\n",
            "Test row date: 1971-09-30 -> Model trained, prediction made (using: 1966-09-30 - 1971-08-30)\n",
            "Test row date: 1971-10-30 -> Model trained, prediction made (using: 1966-10-30 - 1971-09-30)\n",
            "Test row date: 1971-11-30 -> Model trained, prediction made (using: 1966-11-30 - 1971-10-30)\n",
            "Test row date: 1971-12-30 -> Model trained, prediction made (using: 1966-12-30 - 1971-11-30)\n",
            "Test row date: 1972-01-30 -> Model trained, prediction made (using: 1967-01-30 - 1971-12-30)\n",
            "Test row date: 1972-02-29 -> Model trained, prediction made (using: 1967-02-28 - 1972-01-30)\n",
            "Test row date: 1972-03-30 -> Model trained, prediction made (using: 1967-03-30 - 1972-02-29)\n",
            "Test row date: 1972-04-30 -> Model trained, prediction made (using: 1967-04-30 - 1972-03-30)\n",
            "Test row date: 1972-05-30 -> Model trained, prediction made (using: 1967-05-30 - 1972-04-30)\n",
            "Test row date: 1972-06-30 -> Model trained, prediction made (using: 1967-06-30 - 1972-05-30)\n",
            "Test row date: 1972-07-30 -> Model trained, prediction made (using: 1967-07-30 - 1972-06-30)\n",
            "Test row date: 1972-08-30 -> Model trained, prediction made (using: 1967-08-30 - 1972-07-30)\n",
            "Test row date: 1972-09-30 -> Model trained, prediction made (using: 1967-09-30 - 1972-08-30)\n",
            "Test row date: 1972-10-30 -> Model trained, prediction made (using: 1967-10-30 - 1972-09-30)\n",
            "Test row date: 1972-11-30 -> Model trained, prediction made (using: 1967-11-30 - 1972-10-30)\n",
            "Test row date: 1972-12-30 -> Model trained, prediction made (using: 1967-12-30 - 1972-11-30)\n",
            "Test row date: 1973-01-30 -> Model trained, prediction made (using: 1968-01-30 - 1972-12-30)\n",
            "Test row date: 1973-02-28 -> Model trained, prediction made (using: 1968-02-29 - 1973-01-30)\n",
            "Test row date: 1973-03-30 -> Model trained, prediction made (using: 1968-03-30 - 1973-02-28)\n",
            "Test row date: 1973-04-30 -> Model trained, prediction made (using: 1968-04-30 - 1973-03-30)\n",
            "Test row date: 1973-05-30 -> Model trained, prediction made (using: 1968-05-30 - 1973-04-30)\n",
            "Test row date: 1973-06-30 -> Model trained, prediction made (using: 1968-06-30 - 1973-05-30)\n",
            "Test row date: 1973-07-30 -> Model trained, prediction made (using: 1968-07-30 - 1973-06-30)\n",
            "Test row date: 1973-08-30 -> Model trained, prediction made (using: 1968-08-30 - 1973-07-30)\n",
            "Test row date: 1973-09-30 -> Model trained, prediction made (using: 1968-09-30 - 1973-08-30)\n",
            "Test row date: 1973-10-30 -> Model trained, prediction made (using: 1968-10-30 - 1973-09-30)\n",
            "Test row date: 1973-11-30 -> Model trained, prediction made (using: 1968-11-30 - 1973-10-30)\n",
            "Test row date: 1973-12-30 -> Model trained, prediction made (using: 1968-12-30 - 1973-11-30)\n",
            "Test row date: 1974-01-30 -> Model trained, prediction made (using: 1969-01-30 - 1973-12-30)\n",
            "Test row date: 1974-02-28 -> Model trained, prediction made (using: 1969-02-28 - 1974-01-30)\n",
            "Test row date: 1974-03-30 -> Model trained, prediction made (using: 1969-03-30 - 1974-02-28)\n",
            "Test row date: 1974-04-30 -> Model trained, prediction made (using: 1969-04-30 - 1974-03-30)\n",
            "Test row date: 1974-05-30 -> Model trained, prediction made (using: 1969-05-30 - 1974-04-30)\n",
            "Test row date: 1974-06-30 -> Model trained, prediction made (using: 1969-06-30 - 1974-05-30)\n",
            "Test row date: 1974-07-30 -> Model trained, prediction made (using: 1969-07-30 - 1974-06-30)\n",
            "Test row date: 1974-08-30 -> Model trained, prediction made (using: 1969-08-30 - 1974-07-30)\n",
            "Test row date: 1974-09-30 -> Model trained, prediction made (using: 1969-09-30 - 1974-08-30)\n",
            "Test row date: 1974-10-30 -> Model trained, prediction made (using: 1969-10-30 - 1974-09-30)\n",
            "Test row date: 1974-11-30 -> Model trained, prediction made (using: 1969-11-30 - 1974-10-30)\n",
            "Test row date: 1974-12-30 -> Model trained, prediction made (using: 1969-12-30 - 1974-11-30)\n",
            "Test row date: 1975-01-30 -> Model trained, prediction made (using: 1970-01-30 - 1974-12-30)\n",
            "Test row date: 1975-02-28 -> Model trained, prediction made (using: 1970-02-28 - 1975-01-30)\n",
            "Test row date: 1975-03-30 -> Model trained, prediction made (using: 1970-03-30 - 1975-02-28)\n",
            "Test row date: 1975-04-30 -> Model trained, prediction made (using: 1970-04-30 - 1975-03-30)\n",
            "Test row date: 1975-05-30 -> Model trained, prediction made (using: 1970-05-30 - 1975-04-30)\n",
            "Test row date: 1975-06-30 -> Model trained, prediction made (using: 1970-06-30 - 1975-05-30)\n",
            "Test row date: 1975-07-30 -> Model trained, prediction made (using: 1970-07-30 - 1975-06-30)\n",
            "Test row date: 1975-08-30 -> Model trained, prediction made (using: 1970-08-30 - 1975-07-30)\n",
            "Test row date: 1975-09-30 -> Model trained, prediction made (using: 1970-09-30 - 1975-08-30)\n",
            "Test row date: 1975-10-30 -> Model trained, prediction made (using: 1970-10-30 - 1975-09-30)\n",
            "Test row date: 1975-11-30 -> Model trained, prediction made (using: 1970-11-30 - 1975-10-30)\n",
            "Test row date: 1975-12-30 -> Model trained, prediction made (using: 1970-12-30 - 1975-11-30)\n",
            "Test row date: 1976-01-30 -> Model trained, prediction made (using: 1971-01-30 - 1975-12-30)\n",
            "Test row date: 1976-02-29 -> Model trained, prediction made (using: 1971-02-28 - 1976-01-30)\n",
            "Test row date: 1976-03-30 -> Model trained, prediction made (using: 1971-03-30 - 1976-02-29)\n",
            "Test row date: 1976-04-30 -> Model trained, prediction made (using: 1971-04-30 - 1976-03-30)\n",
            "Test row date: 1976-05-30 -> Model trained, prediction made (using: 1971-05-30 - 1976-04-30)\n",
            "Test row date: 1976-06-30 -> Model trained, prediction made (using: 1971-06-30 - 1976-05-30)\n",
            "Test row date: 1976-07-30 -> Model trained, prediction made (using: 1971-07-30 - 1976-06-30)\n",
            "Test row date: 1976-08-30 -> Model trained, prediction made (using: 1971-08-30 - 1976-07-30)\n",
            "Test row date: 1976-09-30 -> Model trained, prediction made (using: 1971-09-30 - 1976-08-30)\n",
            "Test row date: 1976-10-30 -> Model trained, prediction made (using: 1971-10-30 - 1976-09-30)\n",
            "Test row date: 1976-11-30 -> Model trained, prediction made (using: 1971-11-30 - 1976-10-30)\n",
            "Test row date: 1976-12-30 -> Model trained, prediction made (using: 1971-12-30 - 1976-11-30)\n",
            "Test row date: 1977-01-30 -> Model trained, prediction made (using: 1972-01-30 - 1976-12-30)\n",
            "Test row date: 1977-02-28 -> Model trained, prediction made (using: 1972-02-29 - 1977-01-30)\n",
            "Test row date: 1977-03-30 -> Model trained, prediction made (using: 1972-03-30 - 1977-02-28)\n",
            "Test row date: 1977-04-30 -> Model trained, prediction made (using: 1972-04-30 - 1977-03-30)\n",
            "Test row date: 1977-05-30 -> Model trained, prediction made (using: 1972-05-30 - 1977-04-30)\n",
            "Test row date: 1977-06-30 -> Model trained, prediction made (using: 1972-06-30 - 1977-05-30)\n",
            "Test row date: 1977-07-30 -> Model trained, prediction made (using: 1972-07-30 - 1977-06-30)\n",
            "Test row date: 1977-08-30 -> Model trained, prediction made (using: 1972-08-30 - 1977-07-30)\n",
            "Test row date: 1977-09-30 -> Model trained, prediction made (using: 1972-09-30 - 1977-08-30)\n",
            "Test row date: 1977-10-30 -> Model trained, prediction made (using: 1972-10-30 - 1977-09-30)\n",
            "Test row date: 1977-11-30 -> Model trained, prediction made (using: 1972-11-30 - 1977-10-30)\n",
            "Test row date: 1977-12-30 -> Model trained, prediction made (using: 1972-12-30 - 1977-11-30)\n",
            "Test row date: 1978-01-30 -> Model trained, prediction made (using: 1973-01-30 - 1977-12-30)\n",
            "Test row date: 1978-02-28 -> Model trained, prediction made (using: 1973-02-28 - 1978-01-30)\n",
            "Test row date: 1978-03-30 -> Model trained, prediction made (using: 1973-03-30 - 1978-02-28)\n",
            "Test row date: 1978-04-30 -> Model trained, prediction made (using: 1973-04-30 - 1978-03-30)\n",
            "Test row date: 1978-05-30 -> Model trained, prediction made (using: 1973-05-30 - 1978-04-30)\n",
            "Test row date: 1978-06-30 -> Model trained, prediction made (using: 1973-06-30 - 1978-05-30)\n",
            "Test row date: 1978-07-30 -> Model trained, prediction made (using: 1973-07-30 - 1978-06-30)\n",
            "Test row date: 1978-08-30 -> Model trained, prediction made (using: 1973-08-30 - 1978-07-30)\n",
            "Test row date: 1978-09-30 -> Model trained, prediction made (using: 1973-09-30 - 1978-08-30)\n",
            "Test row date: 1978-10-30 -> Model trained, prediction made (using: 1973-10-30 - 1978-09-30)\n",
            "Test row date: 1978-11-30 -> Model trained, prediction made (using: 1973-11-30 - 1978-10-30)\n",
            "Test row date: 1978-12-30 -> Model trained, prediction made (using: 1973-12-30 - 1978-11-30)\n",
            "Test row date: 1979-01-30 -> Model trained, prediction made (using: 1974-01-30 - 1978-12-30)\n",
            "Test row date: 1979-02-28 -> Model trained, prediction made (using: 1974-02-28 - 1979-01-30)\n",
            "Test row date: 1979-03-30 -> Model trained, prediction made (using: 1974-03-30 - 1979-02-28)\n",
            "Test row date: 1979-04-30 -> Model trained, prediction made (using: 1974-04-30 - 1979-03-30)\n",
            "Test row date: 1979-05-30 -> Model trained, prediction made (using: 1974-05-30 - 1979-04-30)\n",
            "Test row date: 1979-06-30 -> Model trained, prediction made (using: 1974-06-30 - 1979-05-30)\n",
            "Test row date: 1979-07-30 -> Model trained, prediction made (using: 1974-07-30 - 1979-06-30)\n",
            "Test row date: 1979-08-30 -> Model trained, prediction made (using: 1974-08-30 - 1979-07-30)\n",
            "Test row date: 1979-09-30 -> Model trained, prediction made (using: 1974-09-30 - 1979-08-30)\n",
            "Test row date: 1979-10-30 -> Model trained, prediction made (using: 1974-10-30 - 1979-09-30)\n",
            "Test row date: 1979-11-30 -> Model trained, prediction made (using: 1974-11-30 - 1979-10-30)\n",
            "Test row date: 1979-12-30 -> Model trained, prediction made (using: 1974-12-30 - 1979-11-30)\n",
            "Test row date: 1980-01-30 -> Model trained, prediction made (using: 1975-01-30 - 1979-12-30)\n",
            "Test row date: 1980-02-29 -> Model trained, prediction made (using: 1975-02-28 - 1980-01-30)\n",
            "Test row date: 1980-03-30 -> Model trained, prediction made (using: 1975-03-30 - 1980-02-29)\n",
            "Test row date: 1980-04-30 -> Model trained, prediction made (using: 1975-04-30 - 1980-03-30)\n",
            "Test row date: 1980-05-30 -> Model trained, prediction made (using: 1975-05-30 - 1980-04-30)\n",
            "Test row date: 1980-06-30 -> Model trained, prediction made (using: 1975-06-30 - 1980-05-30)\n",
            "Test row date: 1980-07-30 -> Model trained, prediction made (using: 1975-07-30 - 1980-06-30)\n",
            "Test row date: 1980-08-30 -> Model trained, prediction made (using: 1975-08-30 - 1980-07-30)\n",
            "Test row date: 1980-09-30 -> Model trained, prediction made (using: 1975-09-30 - 1980-08-30)\n",
            "Test row date: 1980-10-30 -> Model trained, prediction made (using: 1975-10-30 - 1980-09-30)\n",
            "Test row date: 1980-11-30 -> Model trained, prediction made (using: 1975-11-30 - 1980-10-30)\n",
            "Test row date: 1980-12-30 -> Model trained, prediction made (using: 1975-12-30 - 1980-11-30)\n",
            "Test row date: 1981-01-30 -> Model trained, prediction made (using: 1976-01-30 - 1980-12-30)\n",
            "Test row date: 1981-02-28 -> Model trained, prediction made (using: 1976-02-29 - 1981-01-30)\n",
            "Test row date: 1981-03-30 -> Model trained, prediction made (using: 1976-03-30 - 1981-02-28)\n",
            "Test row date: 1981-04-30 -> Model trained, prediction made (using: 1976-04-30 - 1981-03-30)\n",
            "Test row date: 1981-05-30 -> Model trained, prediction made (using: 1976-05-30 - 1981-04-30)\n",
            "Test row date: 1981-06-30 -> Model trained, prediction made (using: 1976-06-30 - 1981-05-30)\n",
            "Test row date: 1981-07-30 -> Model trained, prediction made (using: 1976-07-30 - 1981-06-30)\n",
            "Test row date: 1981-08-30 -> Model trained, prediction made (using: 1976-08-30 - 1981-07-30)\n",
            "Test row date: 1981-09-30 -> Model trained, prediction made (using: 1976-09-30 - 1981-08-30)\n",
            "Test row date: 1981-10-30 -> Model trained, prediction made (using: 1976-10-30 - 1981-09-30)\n",
            "Test row date: 1981-11-30 -> Model trained, prediction made (using: 1976-11-30 - 1981-10-30)\n",
            "Test row date: 1981-12-30 -> Model trained, prediction made (using: 1976-12-30 - 1981-11-30)\n",
            "Test row date: 1982-01-30 -> Model trained, prediction made (using: 1977-01-30 - 1981-12-30)\n",
            "Test row date: 1982-02-28 -> Model trained, prediction made (using: 1977-02-28 - 1982-01-30)\n",
            "Test row date: 1982-03-30 -> Model trained, prediction made (using: 1977-03-30 - 1982-02-28)\n",
            "Test row date: 1982-04-30 -> Model trained, prediction made (using: 1977-04-30 - 1982-03-30)\n",
            "Test row date: 1982-05-30 -> Model trained, prediction made (using: 1977-05-30 - 1982-04-30)\n",
            "Test row date: 1982-06-30 -> Model trained, prediction made (using: 1977-06-30 - 1982-05-30)\n",
            "Test row date: 1982-07-30 -> Model trained, prediction made (using: 1977-07-30 - 1982-06-30)\n",
            "Test row date: 1982-08-30 -> Model trained, prediction made (using: 1977-08-30 - 1982-07-30)\n",
            "Test row date: 1982-09-30 -> Model trained, prediction made (using: 1977-09-30 - 1982-08-30)\n",
            "Test row date: 1982-10-30 -> Model trained, prediction made (using: 1977-10-30 - 1982-09-30)\n",
            "Test row date: 1982-11-30 -> Model trained, prediction made (using: 1977-11-30 - 1982-10-30)\n",
            "Test row date: 1982-12-30 -> Model trained, prediction made (using: 1977-12-30 - 1982-11-30)\n",
            "Test row date: 1983-01-30 -> Model trained, prediction made (using: 1978-01-30 - 1982-12-30)\n",
            "Test row date: 1983-02-28 -> Model trained, prediction made (using: 1978-02-28 - 1983-01-30)\n",
            "Test row date: 1983-03-30 -> Model trained, prediction made (using: 1978-03-30 - 1983-02-28)\n",
            "Test row date: 1983-04-30 -> Model trained, prediction made (using: 1978-04-30 - 1983-03-30)\n",
            "Test row date: 1983-05-30 -> Model trained, prediction made (using: 1978-05-30 - 1983-04-30)\n",
            "Test row date: 1983-06-30 -> Model trained, prediction made (using: 1978-06-30 - 1983-05-30)\n",
            "Test row date: 1983-07-30 -> Model trained, prediction made (using: 1978-07-30 - 1983-06-30)\n",
            "Test row date: 1983-08-30 -> Model trained, prediction made (using: 1978-08-30 - 1983-07-30)\n",
            "Test row date: 1983-09-30 -> Model trained, prediction made (using: 1978-09-30 - 1983-08-30)\n",
            "Test row date: 1983-10-30 -> Model trained, prediction made (using: 1978-10-30 - 1983-09-30)\n",
            "Test row date: 1983-11-30 -> Model trained, prediction made (using: 1978-11-30 - 1983-10-30)\n",
            "Test row date: 1983-12-30 -> Model trained, prediction made (using: 1978-12-30 - 1983-11-30)\n",
            "Test row date: 1984-01-30 -> Model trained, prediction made (using: 1979-01-30 - 1983-12-30)\n",
            "Test row date: 1984-02-29 -> Model trained, prediction made (using: 1979-02-28 - 1984-01-30)\n",
            "Test row date: 1984-03-30 -> Model trained, prediction made (using: 1979-03-30 - 1984-02-29)\n",
            "Test row date: 1984-04-30 -> Model trained, prediction made (using: 1979-04-30 - 1984-03-30)\n",
            "Test row date: 1984-05-30 -> Model trained, prediction made (using: 1979-05-30 - 1984-04-30)\n",
            "Test row date: 1984-06-30 -> Model trained, prediction made (using: 1979-06-30 - 1984-05-30)\n",
            "Test row date: 1984-07-30 -> Model trained, prediction made (using: 1979-07-30 - 1984-06-30)\n",
            "Test row date: 1984-08-30 -> Model trained, prediction made (using: 1979-08-30 - 1984-07-30)\n",
            "Test row date: 1984-09-30 -> Model trained, prediction made (using: 1979-09-30 - 1984-08-30)\n",
            "Test row date: 1984-10-30 -> Model trained, prediction made (using: 1979-10-30 - 1984-09-30)\n",
            "Test row date: 1984-11-30 -> Model trained, prediction made (using: 1979-11-30 - 1984-10-30)\n",
            "Test row date: 1984-12-30 -> Model trained, prediction made (using: 1979-12-30 - 1984-11-30)\n",
            "Test row date: 1985-01-30 -> Model trained, prediction made (using: 1980-01-30 - 1984-12-30)\n",
            "Test row date: 1985-02-28 -> Model trained, prediction made (using: 1980-02-29 - 1985-01-30)\n",
            "Test row date: 1985-03-30 -> Model trained, prediction made (using: 1980-03-30 - 1985-02-28)\n",
            "Test row date: 1985-04-30 -> Model trained, prediction made (using: 1980-04-30 - 1985-03-30)\n",
            "Test row date: 1985-05-30 -> Model trained, prediction made (using: 1980-05-30 - 1985-04-30)\n",
            "Test row date: 1985-06-30 -> Model trained, prediction made (using: 1980-06-30 - 1985-05-30)\n",
            "Test row date: 1985-07-30 -> Model trained, prediction made (using: 1980-07-30 - 1985-06-30)\n",
            "Test row date: 1985-08-30 -> Model trained, prediction made (using: 1980-08-30 - 1985-07-30)\n",
            "Test row date: 1985-09-30 -> Model trained, prediction made (using: 1980-09-30 - 1985-08-30)\n",
            "Test row date: 1985-10-30 -> Model trained, prediction made (using: 1980-10-30 - 1985-09-30)\n",
            "Test row date: 1985-11-30 -> Model trained, prediction made (using: 1980-11-30 - 1985-10-30)\n",
            "Test row date: 1985-12-30 -> Model trained, prediction made (using: 1980-12-30 - 1985-11-30)\n",
            "Test row date: 1986-01-30 -> Model trained, prediction made (using: 1981-01-30 - 1985-12-30)\n",
            "Test row date: 1986-02-28 -> Model trained, prediction made (using: 1981-02-28 - 1986-01-30)\n",
            "Test row date: 1986-03-30 -> Model trained, prediction made (using: 1981-03-30 - 1986-02-28)\n",
            "Test row date: 1986-04-30 -> Model trained, prediction made (using: 1981-04-30 - 1986-03-30)\n",
            "Test row date: 1986-05-30 -> Model trained, prediction made (using: 1981-05-30 - 1986-04-30)\n",
            "Test row date: 1986-06-30 -> Model trained, prediction made (using: 1981-06-30 - 1986-05-30)\n",
            "Test row date: 1986-07-30 -> Model trained, prediction made (using: 1981-07-30 - 1986-06-30)\n",
            "Test row date: 1986-08-30 -> Model trained, prediction made (using: 1981-08-30 - 1986-07-30)\n",
            "Test row date: 1986-09-30 -> Model trained, prediction made (using: 1981-09-30 - 1986-08-30)\n",
            "Test row date: 1986-10-30 -> Model trained, prediction made (using: 1981-10-30 - 1986-09-30)\n",
            "Test row date: 1986-11-30 -> Model trained, prediction made (using: 1981-11-30 - 1986-10-30)\n",
            "Test row date: 1986-12-30 -> Model trained, prediction made (using: 1981-12-30 - 1986-11-30)\n",
            "Test row date: 1987-01-30 -> Model trained, prediction made (using: 1982-01-30 - 1986-12-30)\n",
            "Test row date: 1987-02-28 -> Model trained, prediction made (using: 1982-02-28 - 1987-01-30)\n",
            "Test row date: 1987-03-30 -> Model trained, prediction made (using: 1982-03-30 - 1987-02-28)\n",
            "Test row date: 1987-04-30 -> Model trained, prediction made (using: 1982-04-30 - 1987-03-30)\n",
            "Test row date: 1987-05-30 -> Model trained, prediction made (using: 1982-05-30 - 1987-04-30)\n",
            "Test row date: 1987-06-30 -> Model trained, prediction made (using: 1982-06-30 - 1987-05-30)\n",
            "Test row date: 1987-07-30 -> Model trained, prediction made (using: 1982-07-30 - 1987-06-30)\n",
            "Test row date: 1987-08-30 -> Model trained, prediction made (using: 1982-08-30 - 1987-07-30)\n",
            "Test row date: 1987-09-30 -> Model trained, prediction made (using: 1982-09-30 - 1987-08-30)\n",
            "Test row date: 1987-10-30 -> Model trained, prediction made (using: 1982-10-30 - 1987-09-30)\n",
            "Test row date: 1987-11-30 -> Model trained, prediction made (using: 1982-11-30 - 1987-10-30)\n",
            "Test row date: 1987-12-30 -> Model trained, prediction made (using: 1982-12-30 - 1987-11-30)\n",
            "Test row date: 1988-01-30 -> Model trained, prediction made (using: 1983-01-30 - 1987-12-30)\n",
            "Test row date: 1988-02-29 -> Model trained, prediction made (using: 1983-02-28 - 1988-01-30)\n",
            "Test row date: 1988-03-30 -> Model trained, prediction made (using: 1983-03-30 - 1988-02-29)\n",
            "Test row date: 1988-04-30 -> Model trained, prediction made (using: 1983-04-30 - 1988-03-30)\n",
            "Test row date: 1988-05-30 -> Model trained, prediction made (using: 1983-05-30 - 1988-04-30)\n",
            "Test row date: 1988-06-30 -> Model trained, prediction made (using: 1983-06-30 - 1988-05-30)\n",
            "Test row date: 1988-07-30 -> Model trained, prediction made (using: 1983-07-30 - 1988-06-30)\n",
            "Test row date: 1988-08-30 -> Model trained, prediction made (using: 1983-08-30 - 1988-07-30)\n",
            "Test row date: 1988-09-30 -> Model trained, prediction made (using: 1983-09-30 - 1988-08-30)\n",
            "Test row date: 1988-10-30 -> Model trained, prediction made (using: 1983-10-30 - 1988-09-30)\n",
            "Test row date: 1988-11-30 -> Model trained, prediction made (using: 1983-11-30 - 1988-10-30)\n",
            "Test row date: 1988-12-30 -> Model trained, prediction made (using: 1983-12-30 - 1988-11-30)\n",
            "Test row date: 1989-01-30 -> Model trained, prediction made (using: 1984-01-30 - 1988-12-30)\n",
            "Test row date: 1989-02-28 -> Model trained, prediction made (using: 1984-02-29 - 1989-01-30)\n",
            "Test row date: 1989-03-30 -> Model trained, prediction made (using: 1984-03-30 - 1989-02-28)\n",
            "Test row date: 1989-04-30 -> Model trained, prediction made (using: 1984-04-30 - 1989-03-30)\n",
            "Test row date: 1989-05-30 -> Model trained, prediction made (using: 1984-05-30 - 1989-04-30)\n",
            "Test row date: 1989-06-30 -> Model trained, prediction made (using: 1984-06-30 - 1989-05-30)\n",
            "Test row date: 1989-07-30 -> Model trained, prediction made (using: 1984-07-30 - 1989-06-30)\n",
            "Test row date: 1989-08-30 -> Model trained, prediction made (using: 1984-08-30 - 1989-07-30)\n",
            "Test row date: 1989-09-30 -> Model trained, prediction made (using: 1984-09-30 - 1989-08-30)\n",
            "Test row date: 1989-10-30 -> Model trained, prediction made (using: 1984-10-30 - 1989-09-30)\n",
            "Test row date: 1989-11-30 -> Model trained, prediction made (using: 1984-11-30 - 1989-10-30)\n",
            "Test row date: 1989-12-30 -> Model trained, prediction made (using: 1984-12-30 - 1989-11-30)\n",
            "Test row date: 1990-01-30 -> Model trained, prediction made (using: 1985-01-30 - 1989-12-30)\n",
            "Test row date: 1990-02-28 -> Model trained, prediction made (using: 1985-02-28 - 1990-01-30)\n",
            "Test row date: 1990-03-30 -> Model trained, prediction made (using: 1985-03-30 - 1990-02-28)\n",
            "Test row date: 1990-04-30 -> Model trained, prediction made (using: 1985-04-30 - 1990-03-30)\n",
            "Test row date: 1990-05-30 -> Model trained, prediction made (using: 1985-05-30 - 1990-04-30)\n",
            "Test row date: 1990-06-30 -> Model trained, prediction made (using: 1985-06-30 - 1990-05-30)\n",
            "Test row date: 1990-07-30 -> Model trained, prediction made (using: 1985-07-30 - 1990-06-30)\n",
            "Test row date: 1990-08-30 -> Model trained, prediction made (using: 1985-08-30 - 1990-07-30)\n",
            "Test row date: 1990-09-30 -> Model trained, prediction made (using: 1985-09-30 - 1990-08-30)\n",
            "Test row date: 1990-10-30 -> Model trained, prediction made (using: 1985-10-30 - 1990-09-30)\n",
            "Test row date: 1990-11-30 -> Model trained, prediction made (using: 1985-11-30 - 1990-10-30)\n",
            "Test row date: 1990-12-30 -> Model trained, prediction made (using: 1985-12-30 - 1990-11-30)\n",
            "Test row date: 1991-01-30 -> Model trained, prediction made (using: 1986-01-30 - 1990-12-30)\n",
            "Test row date: 1991-02-28 -> Model trained, prediction made (using: 1986-02-28 - 1991-01-30)\n",
            "Test row date: 1991-03-30 -> Model trained, prediction made (using: 1986-03-30 - 1991-02-28)\n",
            "Test row date: 1991-04-30 -> Model trained, prediction made (using: 1986-04-30 - 1991-03-30)\n",
            "Test row date: 1991-05-30 -> Model trained, prediction made (using: 1986-05-30 - 1991-04-30)\n",
            "Test row date: 1991-06-30 -> Model trained, prediction made (using: 1986-06-30 - 1991-05-30)\n",
            "Test row date: 1991-07-30 -> Model trained, prediction made (using: 1986-07-30 - 1991-06-30)\n",
            "Test row date: 1991-08-30 -> Model trained, prediction made (using: 1986-08-30 - 1991-07-30)\n",
            "Test row date: 1991-09-30 -> Model trained, prediction made (using: 1986-09-30 - 1991-08-30)\n",
            "Test row date: 1991-10-30 -> Model trained, prediction made (using: 1986-10-30 - 1991-09-30)\n",
            "Test row date: 1991-11-30 -> Model trained, prediction made (using: 1986-11-30 - 1991-10-30)\n",
            "Test row date: 1991-12-30 -> Model trained, prediction made (using: 1986-12-30 - 1991-11-30)\n",
            "Test row date: 1992-01-30 -> Model trained, prediction made (using: 1987-01-30 - 1991-12-30)\n",
            "Test row date: 1992-02-29 -> Model trained, prediction made (using: 1987-02-28 - 1992-01-30)\n",
            "Test row date: 1992-03-30 -> Model trained, prediction made (using: 1987-03-30 - 1992-02-29)\n",
            "Test row date: 1992-04-30 -> Model trained, prediction made (using: 1987-04-30 - 1992-03-30)\n",
            "Test row date: 1992-05-30 -> Model trained, prediction made (using: 1987-05-30 - 1992-04-30)\n",
            "Test row date: 1992-06-30 -> Model trained, prediction made (using: 1987-06-30 - 1992-05-30)\n",
            "Test row date: 1992-07-30 -> Model trained, prediction made (using: 1987-07-30 - 1992-06-30)\n",
            "Test row date: 1992-08-30 -> Model trained, prediction made (using: 1987-08-30 - 1992-07-30)\n",
            "Test row date: 1992-09-30 -> Model trained, prediction made (using: 1987-09-30 - 1992-08-30)\n",
            "Test row date: 1992-10-30 -> Model trained, prediction made (using: 1987-10-30 - 1992-09-30)\n",
            "Test row date: 1992-11-30 -> Model trained, prediction made (using: 1987-11-30 - 1992-10-30)\n",
            "Test row date: 1992-12-30 -> Model trained, prediction made (using: 1987-12-30 - 1992-11-30)\n",
            "Test row date: 1993-01-30 -> Model trained, prediction made (using: 1988-01-30 - 1992-12-30)\n",
            "Test row date: 1993-02-28 -> Model trained, prediction made (using: 1988-02-29 - 1993-01-30)\n",
            "Test row date: 1993-03-30 -> Model trained, prediction made (using: 1988-03-30 - 1993-02-28)\n",
            "Test row date: 1993-04-30 -> Model trained, prediction made (using: 1988-04-30 - 1993-03-30)\n",
            "Test row date: 1993-05-30 -> Model trained, prediction made (using: 1988-05-30 - 1993-04-30)\n",
            "Test row date: 1993-06-30 -> Model trained, prediction made (using: 1988-06-30 - 1993-05-30)\n",
            "Test row date: 1993-07-30 -> Model trained, prediction made (using: 1988-07-30 - 1993-06-30)\n",
            "Test row date: 1993-08-30 -> Model trained, prediction made (using: 1988-08-30 - 1993-07-30)\n",
            "Test row date: 1993-09-30 -> Model trained, prediction made (using: 1988-09-30 - 1993-08-30)\n",
            "Test row date: 1993-10-30 -> Model trained, prediction made (using: 1988-10-30 - 1993-09-30)\n",
            "Test row date: 1993-11-30 -> Model trained, prediction made (using: 1988-11-30 - 1993-10-30)\n",
            "Test row date: 1993-12-30 -> Model trained, prediction made (using: 1988-12-30 - 1993-11-30)\n",
            "Test row date: 1994-01-30 -> Model trained, prediction made (using: 1989-01-30 - 1993-12-30)\n",
            "Test row date: 1994-02-28 -> Model trained, prediction made (using: 1989-02-28 - 1994-01-30)\n",
            "Test row date: 1994-03-30 -> Model trained, prediction made (using: 1989-03-30 - 1994-02-28)\n",
            "Test row date: 1994-04-30 -> Model trained, prediction made (using: 1989-04-30 - 1994-03-30)\n",
            "Test row date: 1994-05-30 -> Model trained, prediction made (using: 1989-05-30 - 1994-04-30)\n",
            "Test row date: 1994-06-30 -> Model trained, prediction made (using: 1989-06-30 - 1994-05-30)\n",
            "Test row date: 1994-07-30 -> Model trained, prediction made (using: 1989-07-30 - 1994-06-30)\n",
            "Test row date: 1994-08-30 -> Model trained, prediction made (using: 1989-08-30 - 1994-07-30)\n",
            "Test row date: 1994-09-30 -> Model trained, prediction made (using: 1989-09-30 - 1994-08-30)\n",
            "Test row date: 1994-10-30 -> Model trained, prediction made (using: 1989-10-30 - 1994-09-30)\n",
            "Test row date: 1994-11-30 -> Model trained, prediction made (using: 1989-11-30 - 1994-10-30)\n",
            "Test row date: 1994-12-30 -> Model trained, prediction made (using: 1989-12-30 - 1994-11-30)\n",
            "Test row date: 1995-01-30 -> Model trained, prediction made (using: 1990-01-30 - 1994-12-30)\n",
            "Test row date: 1995-02-28 -> Model trained, prediction made (using: 1990-02-28 - 1995-01-30)\n",
            "Test row date: 1995-03-30 -> Model trained, prediction made (using: 1990-03-30 - 1995-02-28)\n",
            "Test row date: 1995-04-30 -> Model trained, prediction made (using: 1990-04-30 - 1995-03-30)\n",
            "Test row date: 1995-05-30 -> Model trained, prediction made (using: 1990-05-30 - 1995-04-30)\n",
            "Test row date: 1995-06-30 -> Model trained, prediction made (using: 1990-06-30 - 1995-05-30)\n",
            "Test row date: 1995-07-30 -> Model trained, prediction made (using: 1990-07-30 - 1995-06-30)\n",
            "Test row date: 1995-08-30 -> Model trained, prediction made (using: 1990-08-30 - 1995-07-30)\n",
            "Test row date: 1995-09-30 -> Model trained, prediction made (using: 1990-09-30 - 1995-08-30)\n",
            "Test row date: 1995-10-30 -> Model trained, prediction made (using: 1990-10-30 - 1995-09-30)\n",
            "Test row date: 1995-11-30 -> Model trained, prediction made (using: 1990-11-30 - 1995-10-30)\n",
            "Test row date: 1995-12-30 -> Model trained, prediction made (using: 1990-12-30 - 1995-11-30)\n",
            "Test row date: 1996-01-30 -> Model trained, prediction made (using: 1991-01-30 - 1995-12-30)\n",
            "Test row date: 1996-02-29 -> Model trained, prediction made (using: 1991-02-28 - 1996-01-30)\n",
            "Test row date: 1996-03-30 -> Model trained, prediction made (using: 1991-03-30 - 1996-02-29)\n",
            "Test row date: 1996-04-30 -> Model trained, prediction made (using: 1991-04-30 - 1996-03-30)\n",
            "Test row date: 1996-05-30 -> Model trained, prediction made (using: 1991-05-30 - 1996-04-30)\n",
            "Test row date: 1996-06-30 -> Model trained, prediction made (using: 1991-06-30 - 1996-05-30)\n",
            "Test row date: 1996-07-30 -> Model trained, prediction made (using: 1991-07-30 - 1996-06-30)\n",
            "Test row date: 1996-08-30 -> Model trained, prediction made (using: 1991-08-30 - 1996-07-30)\n",
            "Test row date: 1996-09-30 -> Model trained, prediction made (using: 1991-09-30 - 1996-08-30)\n",
            "Test row date: 1996-10-30 -> Model trained, prediction made (using: 1991-10-30 - 1996-09-30)\n",
            "Test row date: 1996-11-30 -> Model trained, prediction made (using: 1991-11-30 - 1996-10-30)\n",
            "Test row date: 1996-12-30 -> Model trained, prediction made (using: 1991-12-30 - 1996-11-30)\n",
            "Test row date: 1997-01-30 -> Model trained, prediction made (using: 1992-01-30 - 1996-12-30)\n",
            "Test row date: 1997-02-28 -> Model trained, prediction made (using: 1992-02-29 - 1997-01-30)\n",
            "Test row date: 1997-03-30 -> Model trained, prediction made (using: 1992-03-30 - 1997-02-28)\n",
            "Test row date: 1997-04-30 -> Model trained, prediction made (using: 1992-04-30 - 1997-03-30)\n",
            "Test row date: 1997-05-30 -> Model trained, prediction made (using: 1992-05-30 - 1997-04-30)\n",
            "Test row date: 1997-06-30 -> Model trained, prediction made (using: 1992-06-30 - 1997-05-30)\n",
            "Test row date: 1997-07-30 -> Model trained, prediction made (using: 1992-07-30 - 1997-06-30)\n",
            "Test row date: 1997-08-30 -> Model trained, prediction made (using: 1992-08-30 - 1997-07-30)\n",
            "Test row date: 1997-09-30 -> Model trained, prediction made (using: 1992-09-30 - 1997-08-30)\n",
            "Test row date: 1997-10-30 -> Model trained, prediction made (using: 1992-10-30 - 1997-09-30)\n",
            "Test row date: 1997-11-30 -> Model trained, prediction made (using: 1992-11-30 - 1997-10-30)\n",
            "Test row date: 1997-12-30 -> Model trained, prediction made (using: 1992-12-30 - 1997-11-30)\n",
            "Test row date: 1998-01-30 -> Model trained, prediction made (using: 1993-01-30 - 1997-12-30)\n",
            "Test row date: 1998-02-28 -> Model trained, prediction made (using: 1993-02-28 - 1998-01-30)\n",
            "Test row date: 1998-03-30 -> Model trained, prediction made (using: 1993-03-30 - 1998-02-28)\n",
            "Test row date: 1998-04-30 -> Model trained, prediction made (using: 1993-04-30 - 1998-03-30)\n",
            "Test row date: 1998-05-30 -> Model trained, prediction made (using: 1993-05-30 - 1998-04-30)\n",
            "Test row date: 1998-06-30 -> Model trained, prediction made (using: 1993-06-30 - 1998-05-30)\n",
            "Test row date: 1998-07-30 -> Model trained, prediction made (using: 1993-07-30 - 1998-06-30)\n",
            "Test row date: 1998-08-30 -> Model trained, prediction made (using: 1993-08-30 - 1998-07-30)\n",
            "Test row date: 1998-09-30 -> Model trained, prediction made (using: 1993-09-30 - 1998-08-30)\n",
            "Test row date: 1998-10-30 -> Model trained, prediction made (using: 1993-10-30 - 1998-09-30)\n",
            "Test row date: 1998-11-30 -> Model trained, prediction made (using: 1993-11-30 - 1998-10-30)\n",
            "Test row date: 1998-12-30 -> Model trained, prediction made (using: 1993-12-30 - 1998-11-30)\n",
            "Test row date: 1999-01-30 -> Model trained, prediction made (using: 1994-01-30 - 1998-12-30)\n",
            "Test row date: 1999-02-28 -> Model trained, prediction made (using: 1994-02-28 - 1999-01-30)\n",
            "Test row date: 1999-03-30 -> Model trained, prediction made (using: 1994-03-30 - 1999-02-28)\n",
            "Test row date: 1999-04-30 -> Model trained, prediction made (using: 1994-04-30 - 1999-03-30)\n",
            "Test row date: 1999-05-30 -> Model trained, prediction made (using: 1994-05-30 - 1999-04-30)\n",
            "Test row date: 1999-06-30 -> Model trained, prediction made (using: 1994-06-30 - 1999-05-30)\n",
            "Test row date: 1999-07-30 -> Model trained, prediction made (using: 1994-07-30 - 1999-06-30)\n",
            "Test row date: 1999-08-30 -> Model trained, prediction made (using: 1994-08-30 - 1999-07-30)\n",
            "Test row date: 1999-09-30 -> Model trained, prediction made (using: 1994-09-30 - 1999-08-30)\n",
            "Test row date: 1999-10-30 -> Model trained, prediction made (using: 1994-10-30 - 1999-09-30)\n",
            "Test row date: 1999-11-30 -> Model trained, prediction made (using: 1994-11-30 - 1999-10-30)\n",
            "Test row date: 1999-12-30 -> Model trained, prediction made (using: 1994-12-30 - 1999-11-30)\n",
            "Test row date: 2000-01-30 -> Model trained, prediction made (using: 1995-01-30 - 1999-12-30)\n",
            "Test row date: 2000-02-29 -> Model trained, prediction made (using: 1995-02-28 - 2000-01-30)\n",
            "Test row date: 2000-03-30 -> Model trained, prediction made (using: 1995-03-30 - 2000-02-29)\n",
            "Test row date: 2000-04-30 -> Model trained, prediction made (using: 1995-04-30 - 2000-03-30)\n",
            "Test row date: 2000-05-30 -> Model trained, prediction made (using: 1995-05-30 - 2000-04-30)\n",
            "Test row date: 2000-06-30 -> Model trained, prediction made (using: 1995-06-30 - 2000-05-30)\n",
            "Test row date: 2000-07-30 -> Model trained, prediction made (using: 1995-07-30 - 2000-06-30)\n",
            "Test row date: 2000-08-30 -> Model trained, prediction made (using: 1995-08-30 - 2000-07-30)\n",
            "Test row date: 2000-09-30 -> Model trained, prediction made (using: 1995-09-30 - 2000-08-30)\n",
            "Test row date: 2000-10-30 -> Model trained, prediction made (using: 1995-10-30 - 2000-09-30)\n",
            "Test row date: 2000-11-30 -> Model trained, prediction made (using: 1995-11-30 - 2000-10-30)\n",
            "Test row date: 2000-12-30 -> Model trained, prediction made (using: 1995-12-30 - 2000-11-30)\n",
            "Test row date: 2001-01-30 -> Model trained, prediction made (using: 1996-01-30 - 2000-12-30)\n",
            "Test row date: 2001-02-28 -> Model trained, prediction made (using: 1996-02-29 - 2001-01-30)\n",
            "Test row date: 2001-03-30 -> Model trained, prediction made (using: 1996-03-30 - 2001-02-28)\n",
            "Test row date: 2001-04-30 -> Model trained, prediction made (using: 1996-04-30 - 2001-03-30)\n",
            "Test row date: 2001-05-30 -> Model trained, prediction made (using: 1996-05-30 - 2001-04-30)\n",
            "Test row date: 2001-06-30 -> Model trained, prediction made (using: 1996-06-30 - 2001-05-30)\n",
            "Test row date: 2001-07-30 -> Model trained, prediction made (using: 1996-07-30 - 2001-06-30)\n",
            "Test row date: 2001-08-30 -> Model trained, prediction made (using: 1996-08-30 - 2001-07-30)\n",
            "Test row date: 2001-09-30 -> Model trained, prediction made (using: 1996-09-30 - 2001-08-30)\n",
            "Test row date: 2001-10-30 -> Model trained, prediction made (using: 1996-10-30 - 2001-09-30)\n",
            "Test row date: 2001-11-30 -> Model trained, prediction made (using: 1996-11-30 - 2001-10-30)\n",
            "Test row date: 2001-12-30 -> Model trained, prediction made (using: 1996-12-30 - 2001-11-30)\n",
            "Test row date: 2002-01-30 -> Model trained, prediction made (using: 1997-01-30 - 2001-12-30)\n",
            "Test row date: 2002-02-28 -> Model trained, prediction made (using: 1997-02-28 - 2002-01-30)\n",
            "Test row date: 2002-03-30 -> Model trained, prediction made (using: 1997-03-30 - 2002-02-28)\n",
            "Test row date: 2002-04-30 -> Model trained, prediction made (using: 1997-04-30 - 2002-03-30)\n",
            "Test row date: 2002-05-30 -> Model trained, prediction made (using: 1997-05-30 - 2002-04-30)\n",
            "Test row date: 2002-06-30 -> Model trained, prediction made (using: 1997-06-30 - 2002-05-30)\n",
            "Test row date: 2002-07-30 -> Model trained, prediction made (using: 1997-07-30 - 2002-06-30)\n",
            "Test row date: 2002-08-30 -> Model trained, prediction made (using: 1997-08-30 - 2002-07-30)\n",
            "Test row date: 2002-09-30 -> Model trained, prediction made (using: 1997-09-30 - 2002-08-30)\n",
            "Test row date: 2002-10-30 -> Model trained, prediction made (using: 1997-10-30 - 2002-09-30)\n",
            "Test row date: 2002-11-30 -> Model trained, prediction made (using: 1997-11-30 - 2002-10-30)\n",
            "Test row date: 2002-12-30 -> Model trained, prediction made (using: 1997-12-30 - 2002-11-30)\n",
            "Test row date: 2003-01-30 -> Model trained, prediction made (using: 1998-01-30 - 2002-12-30)\n",
            "Test row date: 2003-02-28 -> Model trained, prediction made (using: 1998-02-28 - 2003-01-30)\n",
            "Test row date: 2003-03-30 -> Model trained, prediction made (using: 1998-03-30 - 2003-02-28)\n",
            "Test row date: 2003-04-30 -> Model trained, prediction made (using: 1998-04-30 - 2003-03-30)\n",
            "Test row date: 2003-05-30 -> Model trained, prediction made (using: 1998-05-30 - 2003-04-30)\n",
            "Test row date: 2003-06-30 -> Model trained, prediction made (using: 1998-06-30 - 2003-05-30)\n",
            "Test row date: 2003-07-30 -> Model trained, prediction made (using: 1998-07-30 - 2003-06-30)\n",
            "Test row date: 2003-08-30 -> Model trained, prediction made (using: 1998-08-30 - 2003-07-30)\n",
            "Test row date: 2003-09-30 -> Model trained, prediction made (using: 1998-09-30 - 2003-08-30)\n",
            "Test row date: 2003-10-30 -> Model trained, prediction made (using: 1998-10-30 - 2003-09-30)\n",
            "Test row date: 2003-11-30 -> Model trained, prediction made (using: 1998-11-30 - 2003-10-30)\n",
            "Test row date: 2003-12-30 -> Model trained, prediction made (using: 1998-12-30 - 2003-11-30)\n",
            "Test row date: 2004-01-30 -> Model trained, prediction made (using: 1999-01-30 - 2003-12-30)\n",
            "Test row date: 2004-02-29 -> Model trained, prediction made (using: 1999-02-28 - 2004-01-30)\n",
            "Test row date: 2004-03-30 -> Model trained, prediction made (using: 1999-03-30 - 2004-02-29)\n",
            "Test row date: 2004-04-30 -> Model trained, prediction made (using: 1999-04-30 - 2004-03-30)\n",
            "Test row date: 2004-05-30 -> Model trained, prediction made (using: 1999-05-30 - 2004-04-30)\n",
            "Test row date: 2004-06-30 -> Model trained, prediction made (using: 1999-06-30 - 2004-05-30)\n",
            "Test row date: 2004-07-30 -> Model trained, prediction made (using: 1999-07-30 - 2004-06-30)\n",
            "Test row date: 2004-08-30 -> Model trained, prediction made (using: 1999-08-30 - 2004-07-30)\n",
            "Test row date: 2004-09-30 -> Model trained, prediction made (using: 1999-09-30 - 2004-08-30)\n",
            "Test row date: 2004-10-30 -> Model trained, prediction made (using: 1999-10-30 - 2004-09-30)\n",
            "Test row date: 2004-11-30 -> Model trained, prediction made (using: 1999-11-30 - 2004-10-30)\n",
            "Test row date: 2004-12-30 -> Model trained, prediction made (using: 1999-12-30 - 2004-11-30)\n",
            "Test row date: 2005-01-30 -> Model trained, prediction made (using: 2000-01-30 - 2004-12-30)\n",
            "Test row date: 2005-02-28 -> Model trained, prediction made (using: 2000-02-29 - 2005-01-30)\n",
            "Test row date: 2005-03-30 -> Model trained, prediction made (using: 2000-03-30 - 2005-02-28)\n",
            "Test row date: 2005-04-30 -> Model trained, prediction made (using: 2000-04-30 - 2005-03-30)\n",
            "Test row date: 2005-05-30 -> Model trained, prediction made (using: 2000-05-30 - 2005-04-30)\n",
            "Test row date: 2005-06-30 -> Model trained, prediction made (using: 2000-06-30 - 2005-05-30)\n",
            "Test row date: 2005-07-30 -> Model trained, prediction made (using: 2000-07-30 - 2005-06-30)\n",
            "Test row date: 2005-08-30 -> Model trained, prediction made (using: 2000-08-30 - 2005-07-30)\n",
            "Test row date: 2005-09-30 -> Model trained, prediction made (using: 2000-09-30 - 2005-08-30)\n",
            "Test row date: 2005-10-30 -> Model trained, prediction made (using: 2000-10-30 - 2005-09-30)\n",
            "Test row date: 2005-11-30 -> Model trained, prediction made (using: 2000-11-30 - 2005-10-30)\n",
            "Test row date: 2005-12-30 -> Model trained, prediction made (using: 2000-12-30 - 2005-11-30)\n",
            "Test row date: 2006-01-30 -> Model trained, prediction made (using: 2001-01-30 - 2005-12-30)\n",
            "Test row date: 2006-02-28 -> Model trained, prediction made (using: 2001-02-28 - 2006-01-30)\n",
            "Test row date: 2006-03-30 -> Model trained, prediction made (using: 2001-03-30 - 2006-02-28)\n",
            "Test row date: 2006-04-30 -> Model trained, prediction made (using: 2001-04-30 - 2006-03-30)\n",
            "Test row date: 2006-05-30 -> Model trained, prediction made (using: 2001-05-30 - 2006-04-30)\n",
            "Test row date: 2006-06-30 -> Model trained, prediction made (using: 2001-06-30 - 2006-05-30)\n",
            "Test row date: 2006-07-30 -> Model trained, prediction made (using: 2001-07-30 - 2006-06-30)\n",
            "Test row date: 2006-08-30 -> Model trained, prediction made (using: 2001-08-30 - 2006-07-30)\n",
            "Test row date: 2006-09-30 -> Model trained, prediction made (using: 2001-09-30 - 2006-08-30)\n",
            "Test row date: 2006-10-30 -> Model trained, prediction made (using: 2001-10-30 - 2006-09-30)\n",
            "Test row date: 2006-11-30 -> Model trained, prediction made (using: 2001-11-30 - 2006-10-30)\n",
            "Test row date: 2006-12-30 -> Model trained, prediction made (using: 2001-12-30 - 2006-11-30)\n",
            "Test row date: 2007-01-30 -> Model trained, prediction made (using: 2002-01-30 - 2006-12-30)\n",
            "Test row date: 2007-02-28 -> Model trained, prediction made (using: 2002-02-28 - 2007-01-30)\n",
            "Test row date: 2007-03-30 -> Model trained, prediction made (using: 2002-03-30 - 2007-02-28)\n",
            "Test row date: 2007-04-30 -> Model trained, prediction made (using: 2002-04-30 - 2007-03-30)\n",
            "Test row date: 2007-05-30 -> Model trained, prediction made (using: 2002-05-30 - 2007-04-30)\n",
            "Test row date: 2007-06-30 -> Model trained, prediction made (using: 2002-06-30 - 2007-05-30)\n",
            "Test row date: 2007-07-30 -> Model trained, prediction made (using: 2002-07-30 - 2007-06-30)\n",
            "Test row date: 2007-08-30 -> Model trained, prediction made (using: 2002-08-30 - 2007-07-30)\n",
            "Test row date: 2007-09-30 -> Model trained, prediction made (using: 2002-09-30 - 2007-08-30)\n",
            "Test row date: 2007-10-30 -> Model trained, prediction made (using: 2002-10-30 - 2007-09-30)\n",
            "Test row date: 2007-11-30 -> Model trained, prediction made (using: 2002-11-30 - 2007-10-30)\n",
            "Test row date: 2007-12-30 -> Model trained, prediction made (using: 2002-12-30 - 2007-11-30)\n",
            "Test row date: 2008-01-30 -> Model trained, prediction made (using: 2003-01-30 - 2007-12-30)\n",
            "Test row date: 2008-02-29 -> Model trained, prediction made (using: 2003-02-28 - 2008-01-30)\n",
            "Test row date: 2008-03-30 -> Model trained, prediction made (using: 2003-03-30 - 2008-02-29)\n",
            "Test row date: 2008-04-30 -> Model trained, prediction made (using: 2003-04-30 - 2008-03-30)\n",
            "Test row date: 2008-05-30 -> Model trained, prediction made (using: 2003-05-30 - 2008-04-30)\n",
            "Test row date: 2008-06-30 -> Model trained, prediction made (using: 2003-06-30 - 2008-05-30)\n",
            "Test row date: 2008-07-30 -> Model trained, prediction made (using: 2003-07-30 - 2008-06-30)\n",
            "Test row date: 2008-08-30 -> Model trained, prediction made (using: 2003-08-30 - 2008-07-30)\n",
            "Test row date: 2008-09-30 -> Model trained, prediction made (using: 2003-09-30 - 2008-08-30)\n",
            "Test row date: 2008-10-30 -> Model trained, prediction made (using: 2003-10-30 - 2008-09-30)\n",
            "Test row date: 2008-11-30 -> Model trained, prediction made (using: 2003-11-30 - 2008-10-30)\n",
            "Test row date: 2008-12-30 -> Model trained, prediction made (using: 2003-12-30 - 2008-11-30)\n",
            "Test row date: 2009-01-30 -> Model trained, prediction made (using: 2004-01-30 - 2008-12-30)\n",
            "Test row date: 2009-02-28 -> Model trained, prediction made (using: 2004-02-29 - 2009-01-30)\n",
            "Test row date: 2009-03-30 -> Model trained, prediction made (using: 2004-03-30 - 2009-02-28)\n",
            "Test row date: 2009-04-30 -> Model trained, prediction made (using: 2004-04-30 - 2009-03-30)\n",
            "Test row date: 2009-05-30 -> Model trained, prediction made (using: 2004-05-30 - 2009-04-30)\n",
            "Test row date: 2009-06-30 -> Model trained, prediction made (using: 2004-06-30 - 2009-05-30)\n",
            "Test row date: 2009-07-30 -> Model trained, prediction made (using: 2004-07-30 - 2009-06-30)\n",
            "Test row date: 2009-08-30 -> Model trained, prediction made (using: 2004-08-30 - 2009-07-30)\n",
            "Test row date: 2009-09-30 -> Model trained, prediction made (using: 2004-09-30 - 2009-08-30)\n",
            "Test row date: 2009-10-30 -> Model trained, prediction made (using: 2004-10-30 - 2009-09-30)\n",
            "Test row date: 2009-11-30 -> Model trained, prediction made (using: 2004-11-30 - 2009-10-30)\n",
            "Test row date: 2009-12-30 -> Model trained, prediction made (using: 2004-12-30 - 2009-11-30)\n",
            "Test row date: 2010-01-30 -> Model trained, prediction made (using: 2005-01-30 - 2009-12-30)\n",
            "Test row date: 2010-02-28 -> Model trained, prediction made (using: 2005-02-28 - 2010-01-30)\n",
            "Test row date: 2010-03-30 -> Model trained, prediction made (using: 2005-03-30 - 2010-02-28)\n",
            "Test row date: 2010-04-30 -> Model trained, prediction made (using: 2005-04-30 - 2010-03-30)\n",
            "Test row date: 2010-05-30 -> Model trained, prediction made (using: 2005-05-30 - 2010-04-30)\n",
            "Test row date: 2010-06-30 -> Model trained, prediction made (using: 2005-06-30 - 2010-05-30)\n",
            "Test row date: 2010-07-30 -> Model trained, prediction made (using: 2005-07-30 - 2010-06-30)\n",
            "Test row date: 2010-08-30 -> Model trained, prediction made (using: 2005-08-30 - 2010-07-30)\n",
            "Test row date: 2010-09-30 -> Model trained, prediction made (using: 2005-09-30 - 2010-08-30)\n",
            "Test row date: 2010-10-30 -> Model trained, prediction made (using: 2005-10-30 - 2010-09-30)\n",
            "Test row date: 2010-11-30 -> Model trained, prediction made (using: 2005-11-30 - 2010-10-30)\n",
            "Test row date: 2010-12-30 -> Model trained, prediction made (using: 2005-12-30 - 2010-11-30)\n",
            "Test row date: 2011-01-30 -> Model trained, prediction made (using: 2006-01-30 - 2010-12-30)\n",
            "Test row date: 2011-02-28 -> Model trained, prediction made (using: 2006-02-28 - 2011-01-30)\n",
            "Test row date: 2011-03-30 -> Model trained, prediction made (using: 2006-03-30 - 2011-02-28)\n",
            "Test row date: 2011-04-30 -> Model trained, prediction made (using: 2006-04-30 - 2011-03-30)\n",
            "Test row date: 2011-05-30 -> Model trained, prediction made (using: 2006-05-30 - 2011-04-30)\n",
            "Test row date: 2011-06-30 -> Model trained, prediction made (using: 2006-06-30 - 2011-05-30)\n",
            "Test row date: 2011-07-30 -> Model trained, prediction made (using: 2006-07-30 - 2011-06-30)\n",
            "Test row date: 2011-08-30 -> Model trained, prediction made (using: 2006-08-30 - 2011-07-30)\n",
            "Test row date: 2011-09-30 -> Model trained, prediction made (using: 2006-09-30 - 2011-08-30)\n",
            "Test row date: 2011-10-30 -> Model trained, prediction made (using: 2006-10-30 - 2011-09-30)\n",
            "Test row date: 2011-11-30 -> Model trained, prediction made (using: 2006-11-30 - 2011-10-30)\n",
            "Test row date: 2011-12-30 -> Model trained, prediction made (using: 2006-12-30 - 2011-11-30)\n",
            "Test row date: 2012-01-30 -> Model trained, prediction made (using: 2007-01-30 - 2011-12-30)\n",
            "Test row date: 2012-02-29 -> Model trained, prediction made (using: 2007-02-28 - 2012-01-30)\n",
            "Test row date: 2012-03-30 -> Model trained, prediction made (using: 2007-03-30 - 2012-02-29)\n",
            "Test row date: 2012-04-30 -> Model trained, prediction made (using: 2007-04-30 - 2012-03-30)\n",
            "Test row date: 2012-05-30 -> Model trained, prediction made (using: 2007-05-30 - 2012-04-30)\n",
            "Test row date: 2012-06-30 -> Model trained, prediction made (using: 2007-06-30 - 2012-05-30)\n",
            "Test row date: 2012-07-30 -> Model trained, prediction made (using: 2007-07-30 - 2012-06-30)\n",
            "Test row date: 2012-08-30 -> Model trained, prediction made (using: 2007-08-30 - 2012-07-30)\n",
            "Test row date: 2012-09-30 -> Model trained, prediction made (using: 2007-09-30 - 2012-08-30)\n",
            "Test row date: 2012-10-30 -> Model trained, prediction made (using: 2007-10-30 - 2012-09-30)\n",
            "Test row date: 2012-11-30 -> Model trained, prediction made (using: 2007-11-30 - 2012-10-30)\n",
            "Test row date: 2012-12-30 -> Model trained, prediction made (using: 2007-12-30 - 2012-11-30)\n",
            "Test row date: 2013-01-30 -> Model trained, prediction made (using: 2008-01-30 - 2012-12-30)\n",
            "Test row date: 2013-02-28 -> Model trained, prediction made (using: 2008-02-29 - 2013-01-30)\n",
            "Test row date: 2013-03-30 -> Model trained, prediction made (using: 2008-03-30 - 2013-02-28)\n",
            "Test row date: 2013-04-30 -> Model trained, prediction made (using: 2008-04-30 - 2013-03-30)\n",
            "Test row date: 2013-05-30 -> Model trained, prediction made (using: 2008-05-30 - 2013-04-30)\n",
            "Test row date: 2013-06-30 -> Model trained, prediction made (using: 2008-06-30 - 2013-05-30)\n",
            "Test row date: 2013-07-30 -> Model trained, prediction made (using: 2008-07-30 - 2013-06-30)\n",
            "Test row date: 2013-08-30 -> Model trained, prediction made (using: 2008-08-30 - 2013-07-30)\n",
            "Test row date: 2013-09-30 -> Model trained, prediction made (using: 2008-09-30 - 2013-08-30)\n",
            "Test row date: 2013-10-30 -> Model trained, prediction made (using: 2008-10-30 - 2013-09-30)\n",
            "Test row date: 2013-11-30 -> Model trained, prediction made (using: 2008-11-30 - 2013-10-30)\n",
            "Test row date: 2013-12-30 -> Model trained, prediction made (using: 2008-12-30 - 2013-11-30)\n",
            "Test row date: 2014-01-30 -> Model trained, prediction made (using: 2009-01-30 - 2013-12-30)\n",
            "Test row date: 2014-02-28 -> Model trained, prediction made (using: 2009-02-28 - 2014-01-30)\n",
            "Test row date: 2014-03-30 -> Model trained, prediction made (using: 2009-03-30 - 2014-02-28)\n",
            "Test row date: 2014-04-30 -> Model trained, prediction made (using: 2009-04-30 - 2014-03-30)\n",
            "Test row date: 2014-05-30 -> Model trained, prediction made (using: 2009-05-30 - 2014-04-30)\n",
            "Test row date: 2014-06-30 -> Model trained, prediction made (using: 2009-06-30 - 2014-05-30)\n",
            "Test row date: 2014-07-30 -> Model trained, prediction made (using: 2009-07-30 - 2014-06-30)\n",
            "Test row date: 2014-08-30 -> Model trained, prediction made (using: 2009-08-30 - 2014-07-30)\n",
            "Test row date: 2014-09-30 -> Model trained, prediction made (using: 2009-09-30 - 2014-08-30)\n",
            "Test row date: 2014-10-30 -> Model trained, prediction made (using: 2009-10-30 - 2014-09-30)\n",
            "Test row date: 2014-11-30 -> Model trained, prediction made (using: 2009-11-30 - 2014-10-30)\n",
            "Test row date: 2014-12-30 -> Model trained, prediction made (using: 2009-12-30 - 2014-11-30)\n",
            "Test row date: 2015-01-30 -> Model trained, prediction made (using: 2010-01-30 - 2014-12-30)\n",
            "Test row date: 2015-02-28 -> Model trained, prediction made (using: 2010-02-28 - 2015-01-30)\n",
            "Test row date: 2015-03-30 -> Model trained, prediction made (using: 2010-03-30 - 2015-02-28)\n",
            "Test row date: 2015-04-30 -> Model trained, prediction made (using: 2010-04-30 - 2015-03-30)\n",
            "Test row date: 2015-05-30 -> Model trained, prediction made (using: 2010-05-30 - 2015-04-30)\n",
            "Test row date: 2015-06-30 -> Model trained, prediction made (using: 2010-06-30 - 2015-05-30)\n",
            "Test row date: 2015-07-30 -> Model trained, prediction made (using: 2010-07-30 - 2015-06-30)\n",
            "Test row date: 2015-08-30 -> Model trained, prediction made (using: 2010-08-30 - 2015-07-30)\n",
            "Test row date: 2015-09-30 -> Model trained, prediction made (using: 2010-09-30 - 2015-08-30)\n",
            "Test row date: 2015-10-30 -> Model trained, prediction made (using: 2010-10-30 - 2015-09-30)\n",
            "Test row date: 2015-11-30 -> Model trained, prediction made (using: 2010-11-30 - 2015-10-30)\n",
            "Test row date: 2015-12-30 -> Model trained, prediction made (using: 2010-12-30 - 2015-11-30)\n",
            "Test row date: 2016-01-30 -> Model trained, prediction made (using: 2011-01-30 - 2015-12-30)\n",
            "Test row date: 2016-02-29 -> Model trained, prediction made (using: 2011-02-28 - 2016-01-30)\n",
            "Test row date: 2016-03-30 -> Model trained, prediction made (using: 2011-03-30 - 2016-02-29)\n",
            "Test row date: 2016-04-30 -> Model trained, prediction made (using: 2011-04-30 - 2016-03-30)\n",
            "Test row date: 2016-05-30 -> Model trained, prediction made (using: 2011-05-30 - 2016-04-30)\n",
            "Test row date: 2016-06-30 -> Model trained, prediction made (using: 2011-06-30 - 2016-05-30)\n",
            "Test row date: 2016-07-30 -> Model trained, prediction made (using: 2011-07-30 - 2016-06-30)\n",
            "Test row date: 2016-08-30 -> Model trained, prediction made (using: 2011-08-30 - 2016-07-30)\n",
            "Test row date: 2016-09-30 -> Model trained, prediction made (using: 2011-09-30 - 2016-08-30)\n",
            "Test row date: 2016-10-30 -> Model trained, prediction made (using: 2011-10-30 - 2016-09-30)\n",
            "Test row date: 2016-11-30 -> Model trained, prediction made (using: 2011-11-30 - 2016-10-30)\n",
            "Test row date: 2016-12-30 -> Model trained, prediction made (using: 2011-12-30 - 2016-11-30)\n",
            "Test row date: 2017-01-30 -> Model trained, prediction made (using: 2012-01-30 - 2016-12-30)\n",
            "Test row date: 2017-02-28 -> Model trained, prediction made (using: 2012-02-29 - 2017-01-30)\n",
            "Test row date: 2017-03-30 -> Model trained, prediction made (using: 2012-03-30 - 2017-02-28)\n",
            "Test row date: 2017-04-30 -> Model trained, prediction made (using: 2012-04-30 - 2017-03-30)\n",
            "Test row date: 2017-05-30 -> Model trained, prediction made (using: 2012-05-30 - 2017-04-30)\n",
            "Test row date: 2017-06-30 -> Model trained, prediction made (using: 2012-06-30 - 2017-05-30)\n",
            "Test row date: 2017-07-30 -> Model trained, prediction made (using: 2012-07-30 - 2017-06-30)\n",
            "Test row date: 2017-08-30 -> Model trained, prediction made (using: 2012-08-30 - 2017-07-30)\n",
            "Test row date: 2017-09-30 -> Model trained, prediction made (using: 2012-09-30 - 2017-08-30)\n",
            "Test row date: 2017-10-30 -> Model trained, prediction made (using: 2012-10-30 - 2017-09-30)\n",
            "Test row date: 2017-11-30 -> Model trained, prediction made (using: 2012-11-30 - 2017-10-30)\n",
            "Test row date: 2017-12-30 -> Model trained, prediction made (using: 2012-12-30 - 2017-11-30)\n",
            "Test row date: 2018-01-30 -> Model trained, prediction made (using: 2013-01-30 - 2017-12-30)\n",
            "Test row date: 2018-02-28 -> Model trained, prediction made (using: 2013-02-28 - 2018-01-30)\n",
            "Test row date: 2018-03-30 -> Model trained, prediction made (using: 2013-03-30 - 2018-02-28)\n",
            "Test row date: 2018-04-30 -> Model trained, prediction made (using: 2013-04-30 - 2018-03-30)\n",
            "Test row date: 2018-05-30 -> Model trained, prediction made (using: 2013-05-30 - 2018-04-30)\n",
            "Test row date: 2018-06-30 -> Model trained, prediction made (using: 2013-06-30 - 2018-05-30)\n",
            "Test row date: 2018-07-30 -> Model trained, prediction made (using: 2013-07-30 - 2018-06-30)\n",
            "Test row date: 2018-08-30 -> Model trained, prediction made (using: 2013-08-30 - 2018-07-30)\n",
            "Test row date: 2018-09-30 -> Model trained, prediction made (using: 2013-09-30 - 2018-08-30)\n",
            "Test row date: 2018-10-30 -> Model trained, prediction made (using: 2013-10-30 - 2018-09-30)\n",
            "Test row date: 2018-11-30 -> Model trained, prediction made (using: 2013-11-30 - 2018-10-30)\n",
            "Test row date: 2018-12-30 -> Model trained, prediction made (using: 2013-12-30 - 2018-11-30)\n",
            "Test row date: 2019-01-30 -> Model trained, prediction made (using: 2014-01-30 - 2018-12-30)\n",
            "Test row date: 2019-02-28 -> Model trained, prediction made (using: 2014-02-28 - 2019-01-30)\n",
            "Test row date: 2019-03-30 -> Model trained, prediction made (using: 2014-03-30 - 2019-02-28)\n",
            "Test row date: 2019-04-30 -> Model trained, prediction made (using: 2014-04-30 - 2019-03-30)\n",
            "Test row date: 2019-05-30 -> Model trained, prediction made (using: 2014-05-30 - 2019-04-30)\n",
            "Test row date: 2019-06-30 -> Model trained, prediction made (using: 2014-06-30 - 2019-05-30)\n",
            "Test row date: 2019-07-30 -> Model trained, prediction made (using: 2014-07-30 - 2019-06-30)\n",
            "Test row date: 2019-08-30 -> Model trained, prediction made (using: 2014-08-30 - 2019-07-30)\n",
            "Test row date: 2019-09-30 -> Model trained, prediction made (using: 2014-09-30 - 2019-08-30)\n",
            "Test row date: 2019-10-30 -> Model trained, prediction made (using: 2014-10-30 - 2019-09-30)\n",
            "Test row date: 2019-11-30 -> Model trained, prediction made (using: 2014-11-30 - 2019-10-30)\n",
            "Test row date: 2019-12-30 -> Model trained, prediction made (using: 2014-12-30 - 2019-11-30)\n",
            "Test row date: 2020-01-30 -> Model trained, prediction made (using: 2015-01-30 - 2019-12-30)\n",
            "Test row date: 2020-02-29 -> Model trained, prediction made (using: 2015-02-28 - 2020-01-30)\n",
            "Test row date: 2020-03-30 -> Model trained, prediction made (using: 2015-03-30 - 2020-02-29)\n",
            "Test row date: 2020-04-30 -> Model trained, prediction made (using: 2015-04-30 - 2020-03-30)\n",
            "Test row date: 2020-05-30 -> Model trained, prediction made (using: 2015-05-30 - 2020-04-30)\n",
            "Test row date: 2020-06-30 -> Model trained, prediction made (using: 2015-06-30 - 2020-05-30)\n",
            "Test row date: 2020-07-30 -> Model trained, prediction made (using: 2015-07-30 - 2020-06-30)\n",
            "Test row date: 2020-08-30 -> Model trained, prediction made (using: 2015-08-30 - 2020-07-30)\n",
            "Test row date: 2020-09-30 -> Model trained, prediction made (using: 2015-09-30 - 2020-08-30)\n",
            "Test row date: 2020-10-30 -> Model trained, prediction made (using: 2015-10-30 - 2020-09-30)\n",
            "Test row date: 2020-11-30 -> Model trained, prediction made (using: 2015-11-30 - 2020-10-30)\n",
            "Test row date: 2020-12-30 -> Model trained, prediction made (using: 2015-12-30 - 2020-11-30)\n",
            "Test row date: 2021-01-30 -> Model trained, prediction made (using: 2016-01-30 - 2020-12-30)\n",
            "Test row date: 2021-02-28 -> Model trained, prediction made (using: 2016-02-29 - 2021-01-30)\n",
            "Test row date: 2021-03-30 -> Model trained, prediction made (using: 2016-03-30 - 2021-02-28)\n",
            "Test row date: 2021-04-30 -> Model trained, prediction made (using: 2016-04-30 - 2021-03-30)\n",
            "Test row date: 2021-05-30 -> Model trained, prediction made (using: 2016-05-30 - 2021-04-30)\n",
            "Test row date: 2021-06-30 -> Model trained, prediction made (using: 2016-06-30 - 2021-05-30)\n",
            "Test row date: 2021-07-30 -> Model trained, prediction made (using: 2016-07-30 - 2021-06-30)\n",
            "Test row date: 2021-08-30 -> Model trained, prediction made (using: 2016-08-30 - 2021-07-30)\n",
            "Test row date: 2021-09-30 -> Model trained, prediction made (using: 2016-09-30 - 2021-08-30)\n",
            "Test row date: 2021-10-30 -> Model trained, prediction made (using: 2016-10-30 - 2021-09-30)\n",
            "Test row date: 2021-11-30 -> Model trained, prediction made (using: 2016-11-30 - 2021-10-30)\n",
            "Test row date: 2021-12-30 -> Model trained, prediction made (using: 2016-12-30 - 2021-11-30)\n",
            "Test row date: 2022-01-30 -> Model trained, prediction made (using: 2017-01-30 - 2021-12-30)\n",
            "Test row date: 2022-02-28 -> Model trained, prediction made (using: 2017-02-28 - 2022-01-30)\n",
            "Test row date: 2022-03-30 -> Model trained, prediction made (using: 2017-03-30 - 2022-02-28)\n",
            "Test row date: 2022-04-30 -> Model trained, prediction made (using: 2017-04-30 - 2022-03-30)\n",
            "Test row date: 2022-05-30 -> Model trained, prediction made (using: 2017-05-30 - 2022-04-30)\n",
            "Test row date: 2022-06-30 -> Model trained, prediction made (using: 2017-06-30 - 2022-05-30)\n",
            "Test row date: 2022-07-30 -> Model trained, prediction made (using: 2017-07-30 - 2022-06-30)\n",
            "Test row date: 2022-08-30 -> Model trained, prediction made (using: 2017-08-30 - 2022-07-30)\n",
            "Test row date: 2022-09-30 -> Model trained, prediction made (using: 2017-09-30 - 2022-08-30)\n",
            "Test row date: 2022-10-30 -> Model trained, prediction made (using: 2017-10-30 - 2022-09-30)\n",
            "Test row date: 2022-11-30 -> Model trained, prediction made (using: 2017-11-30 - 2022-10-30)\n",
            "Test row date: 2022-12-30 -> Model trained, prediction made (using: 2017-12-30 - 2022-11-30)\n",
            "Test row date: 2023-01-30 -> Model trained, prediction made (using: 2018-01-30 - 2022-12-30)\n",
            "Test row date: 2023-02-28 -> Model trained, prediction made (using: 2018-02-28 - 2023-01-30)\n",
            "Test row date: 2023-03-30 -> Model trained, prediction made (using: 2018-03-30 - 2023-02-28)\n",
            "Test row date: 2023-04-30 -> Model trained, prediction made (using: 2018-04-30 - 2023-03-30)\n",
            "Test row date: 2023-05-30 -> Model trained, prediction made (using: 2018-05-30 - 2023-04-30)\n",
            "Test row date: 2023-06-30 -> Model trained, prediction made (using: 2018-06-30 - 2023-05-30)\n",
            "Test row date: 2023-07-30 -> Model trained, prediction made (using: 2018-07-30 - 2023-06-30)\n",
            "Test row date: 2023-08-30 -> Model trained, prediction made (using: 2018-08-30 - 2023-07-30)\n",
            "Test row date: 2023-09-30 -> Model trained, prediction made (using: 2018-09-30 - 2023-08-30)\n",
            "Test row date: 2023-10-30 -> Model trained, prediction made (using: 2018-10-30 - 2023-09-30)\n",
            "Test row date: 2023-11-30 -> Model trained, prediction made (using: 2018-11-30 - 2023-10-30)\n",
            "Test row date: 2023-12-30 -> Model trained, prediction made (using: 2018-12-30 - 2023-11-30)\n",
            "Test row date: 2024-01-30 -> Model trained, prediction made (using: 2019-01-30 - 2023-12-30)\n",
            "Test row date: 2024-02-29 -> Model trained, prediction made (using: 2019-02-28 - 2024-01-30)\n",
            "Test row date: 2024-03-30 -> Model trained, prediction made (using: 2019-03-30 - 2024-02-29)\n",
            "Test row date: 2024-04-30 -> Model trained, prediction made (using: 2019-04-30 - 2024-03-30)\n",
            "Test row date: 2024-05-30 -> Model trained, prediction made (using: 2019-05-30 - 2024-04-30)\n",
            "Test row date: 2024-06-30 -> Model trained, prediction made (using: 2019-06-30 - 2024-05-30)\n",
            "Test row date: 2024-07-30 -> Model trained, prediction made (using: 2019-07-30 - 2024-06-30)\n",
            "Test row date: 2024-08-30 -> Model trained, prediction made (using: 2019-08-30 - 2024-07-30)\n",
            "Test row date: 2024-09-30 -> Model trained, prediction made (using: 2019-09-30 - 2024-08-30)\n",
            "Test row date: 2024-10-30 -> Model trained, prediction made (using: 2019-10-30 - 2024-09-30)\n",
            "Test row date: 2024-11-30 -> Model trained, prediction made (using: 2019-11-30 - 2024-10-30)\n",
            "Final results_df_rf columns: ['Regime', 'Predicted_month', 'Train_Start_Date', 'Train_End_Date', 'Train_Count', 'Feature_Importances', 'Predicted_Probabilities', 'Predicted_Winner', 'Allocated_Return', 'Equal_Weight_Return', 'Actual_Winner', 'Num_Trees', 'Average_Tree_Depth', 'Max_Tree_Depth', 'Prediction_Horizon_Months', 'Feature_Level_CPI%', 'Feature_Level_LEI', 'Feature_Level_EWMA_0.94', 'Feature_Level_T10YFF', 'Feature_Level_SMB_MA12', 'Feature_Level_HML_MA12', 'Feature_Level_CMA_MA12', 'Feature_Level_RMW_MA12']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Regime Predicted_month Train_Start_Date Train_End_Date  Train_Count  \\\n",
              "655  NoRegime      2024-02-29       2019-02-28     2024-01-30           60   \n",
              "656  NoRegime      2024-03-30       2019-03-30     2024-02-29           60   \n",
              "657  NoRegime      2024-04-30       2019-04-30     2024-03-30           60   \n",
              "658  NoRegime      2024-05-30       2019-05-30     2024-04-30           60   \n",
              "659  NoRegime      2024-06-30       2019-06-30     2024-05-30           60   \n",
              "660  NoRegime      2024-07-30       2019-07-30     2024-06-30           60   \n",
              "661  NoRegime      2024-08-30       2019-08-30     2024-07-30           60   \n",
              "662  NoRegime      2024-09-30       2019-09-30     2024-08-30           60   \n",
              "663  NoRegime      2024-10-30       2019-10-30     2024-09-30           60   \n",
              "664  NoRegime      2024-11-30       2019-11-30     2024-10-30           60   \n",
              "\n",
              "                                   Feature_Importances  \\\n",
              "655  [0.0011075219197046644, 0.007125057683433329, ...   \n",
              "656  [0.001060695344725992, 0.05569936251138364, 0....   \n",
              "657  [0.061560765599685034, 0.02688150024249609, 0....   \n",
              "658  [0.0010915706488781143, 0.03134627046694975, 0...   \n",
              "659  [0.10971915890461427, 0.0020353477765108403, 0...   \n",
              "660  [0.11669339034384454, 0.0, 0.2375168035242865,...   \n",
              "661  [0.1627590379670246, 0.0, 0.10815307820299522,...   \n",
              "662  [0.18611193366893655, 0.0, 0.08024874148652647...   \n",
              "663  [0.12433706540954628, 0.005727754861520321, 0....   \n",
              "664  [0.12783230340482263, 0.0, 0.0, 0.364692102455...   \n",
              "\n",
              "                               Predicted_Probabilities Predicted_Winner  \\\n",
              "655  [0.0, 0.10999999999999993, 0.0, 0.889999999999...              RMW   \n",
              "656  [0.600000000000001, 0.0, 0.0, 0.39999999999999...              SMB   \n",
              "657  [0.22222222222222177, 0.7777777777777771, 0.0,...              HML   \n",
              "658  [0.3233333333333335, 0.0, 0.17666666666666672,...              RMW   \n",
              "659  [0.0, 0.15399999999999978, 0.0, 0.845999999999...              RMW   \n",
              "660  [0.39999999999999925, 0.19999999999999962, 0.0...              RMW   \n",
              "661  [0.7999999999999985, 0.0, 0.0, 0.1999999999999...              SMB   \n",
              "662  [0.16666666666666646, 0.0, 0.5, 0.333333333333...              CMA   \n",
              "663                               [0.0, 0.0, 0.0, 1.0]              RMW   \n",
              "664  [0.1428571428571427, 0.0, 0.2857142857142854, ...              RMW   \n",
              "\n",
              "     Allocated_Return  Equal_Weight_Return  ... Max_Tree_Depth  \\\n",
              "655         -0.021494            -0.021050  ...              6   \n",
              "656         -0.001200             0.014250  ...              7   \n",
              "657         -0.009711            -0.004725  ...              7   \n",
              "658          0.011916            -0.002500  ...              7   \n",
              "659         -0.000783            -0.022375  ...              7   \n",
              "660          0.045480             0.036675  ...              7   \n",
              "661         -0.027500            -0.007675  ...              7   \n",
              "662         -0.002867            -0.009575  ...              7   \n",
              "663         -0.013800            -0.000850  ...              6   \n",
              "664         -0.014343            -0.000150  ...              6   \n",
              "\n",
              "     Prediction_Horizon_Months  Feature_Level_CPI%  Feature_Level_LEI  \\\n",
              "655                          1             0.21033              104.7   \n",
              "656                          1             0.34301              104.2   \n",
              "657                          1             0.39639              104.3   \n",
              "658                          1             0.34885              104.1   \n",
              "659                          1             0.29125              103.5   \n",
              "660                          1             0.03961              103.1   \n",
              "661                          1            -0.00287              102.9   \n",
              "662                          1             0.13892              102.4   \n",
              "663                          1             0.18019              102.1   \n",
              "664                          1             0.22920              101.7   \n",
              "\n",
              "     Feature_Level_EWMA_0.94  Feature_Level_T10YFF  Feature_Level_SMB_MA12  \\\n",
              "655                 0.757643                 -1.34               -0.002417   \n",
              "656                 0.800839                 -1.08               -0.010833   \n",
              "657                 0.609385                 -1.13               -0.012017   \n",
              "658                 0.898366                 -0.64               -0.007225   \n",
              "659                 0.597877                 -0.82               -0.007208   \n",
              "660                 0.411006                 -0.97               -0.006250   \n",
              "661                 0.964565                 -1.24               -0.011025   \n",
              "662                 1.144704                 -1.42               -0.006492   \n",
              "663                 0.814018                 -1.02               -0.006467   \n",
              "664                 0.713907                 -0.55               -0.005825   \n",
              "\n",
              "     Feature_Level_HML_MA12  Feature_Level_CMA_MA12  Feature_Level_RMW_MA12  \n",
              "655               -0.008700               -0.014225                0.003442  \n",
              "656               -0.007425               -0.011375                0.006008  \n",
              "657               -0.009667               -0.012075                0.003500  \n",
              "658                0.001242               -0.009092                0.002783  \n",
              "659                0.000850               -0.011717                0.002000  \n",
              "660                0.005908               -0.008275                0.005992  \n",
              "661                0.003317               -0.008408                0.004525  \n",
              "662                0.004675               -0.008567                0.005183  \n",
              "663                0.004633               -0.005875                0.003042  \n",
              "664                0.001267               -0.005392                0.001533  \n",
              "\n",
              "[10 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44811ecc-1032-4b9a-a504-bef050bd1e52\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Regime</th>\n",
              "      <th>Predicted_month</th>\n",
              "      <th>Train_Start_Date</th>\n",
              "      <th>Train_End_Date</th>\n",
              "      <th>Train_Count</th>\n",
              "      <th>Feature_Importances</th>\n",
              "      <th>Predicted_Probabilities</th>\n",
              "      <th>Predicted_Winner</th>\n",
              "      <th>Allocated_Return</th>\n",
              "      <th>Equal_Weight_Return</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Tree_Depth</th>\n",
              "      <th>Prediction_Horizon_Months</th>\n",
              "      <th>Feature_Level_CPI%</th>\n",
              "      <th>Feature_Level_LEI</th>\n",
              "      <th>Feature_Level_EWMA_0.94</th>\n",
              "      <th>Feature_Level_T10YFF</th>\n",
              "      <th>Feature_Level_SMB_MA12</th>\n",
              "      <th>Feature_Level_HML_MA12</th>\n",
              "      <th>Feature_Level_CMA_MA12</th>\n",
              "      <th>Feature_Level_RMW_MA12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>655</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-02-29</td>\n",
              "      <td>2019-02-28</td>\n",
              "      <td>2024-01-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0011075219197046644, 0.007125057683433329, ...</td>\n",
              "      <td>[0.0, 0.10999999999999993, 0.0, 0.889999999999...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.021494</td>\n",
              "      <td>-0.021050</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.21033</td>\n",
              "      <td>104.7</td>\n",
              "      <td>0.757643</td>\n",
              "      <td>-1.34</td>\n",
              "      <td>-0.002417</td>\n",
              "      <td>-0.008700</td>\n",
              "      <td>-0.014225</td>\n",
              "      <td>0.003442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>656</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-03-30</td>\n",
              "      <td>2019-03-30</td>\n",
              "      <td>2024-02-29</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.001060695344725992, 0.05569936251138364, 0....</td>\n",
              "      <td>[0.600000000000001, 0.0, 0.0, 0.39999999999999...</td>\n",
              "      <td>SMB</td>\n",
              "      <td>-0.001200</td>\n",
              "      <td>0.014250</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34301</td>\n",
              "      <td>104.2</td>\n",
              "      <td>0.800839</td>\n",
              "      <td>-1.08</td>\n",
              "      <td>-0.010833</td>\n",
              "      <td>-0.007425</td>\n",
              "      <td>-0.011375</td>\n",
              "      <td>0.006008</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>657</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-04-30</td>\n",
              "      <td>2019-04-30</td>\n",
              "      <td>2024-03-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.061560765599685034, 0.02688150024249609, 0....</td>\n",
              "      <td>[0.22222222222222177, 0.7777777777777771, 0.0,...</td>\n",
              "      <td>HML</td>\n",
              "      <td>-0.009711</td>\n",
              "      <td>-0.004725</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.39639</td>\n",
              "      <td>104.3</td>\n",
              "      <td>0.609385</td>\n",
              "      <td>-1.13</td>\n",
              "      <td>-0.012017</td>\n",
              "      <td>-0.009667</td>\n",
              "      <td>-0.012075</td>\n",
              "      <td>0.003500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>658</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-05-30</td>\n",
              "      <td>2019-05-30</td>\n",
              "      <td>2024-04-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0010915706488781143, 0.03134627046694975, 0...</td>\n",
              "      <td>[0.3233333333333335, 0.0, 0.17666666666666672,...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.011916</td>\n",
              "      <td>-0.002500</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.34885</td>\n",
              "      <td>104.1</td>\n",
              "      <td>0.898366</td>\n",
              "      <td>-0.64</td>\n",
              "      <td>-0.007225</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>-0.009092</td>\n",
              "      <td>0.002783</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>659</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>2019-06-30</td>\n",
              "      <td>2024-05-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.10971915890461427, 0.0020353477765108403, 0...</td>\n",
              "      <td>[0.0, 0.15399999999999978, 0.0, 0.845999999999...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.000783</td>\n",
              "      <td>-0.022375</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.29125</td>\n",
              "      <td>103.5</td>\n",
              "      <td>0.597877</td>\n",
              "      <td>-0.82</td>\n",
              "      <td>-0.007208</td>\n",
              "      <td>0.000850</td>\n",
              "      <td>-0.011717</td>\n",
              "      <td>0.002000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>2019-07-30</td>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.11669339034384454, 0.0, 0.2375168035242865,...</td>\n",
              "      <td>[0.39999999999999925, 0.19999999999999962, 0.0...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.045480</td>\n",
              "      <td>0.036675</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.03961</td>\n",
              "      <td>103.1</td>\n",
              "      <td>0.411006</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.006250</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>-0.008275</td>\n",
              "      <td>0.005992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>2019-08-30</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.1627590379670246, 0.0, 0.10815307820299522,...</td>\n",
              "      <td>[0.7999999999999985, 0.0, 0.0, 0.1999999999999...</td>\n",
              "      <td>SMB</td>\n",
              "      <td>-0.027500</td>\n",
              "      <td>-0.007675</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.00287</td>\n",
              "      <td>102.9</td>\n",
              "      <td>0.964565</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-0.011025</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>-0.008408</td>\n",
              "      <td>0.004525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>2019-09-30</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.18611193366893655, 0.0, 0.08024874148652647...</td>\n",
              "      <td>[0.16666666666666646, 0.0, 0.5, 0.333333333333...</td>\n",
              "      <td>CMA</td>\n",
              "      <td>-0.002867</td>\n",
              "      <td>-0.009575</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.13892</td>\n",
              "      <td>102.4</td>\n",
              "      <td>1.144704</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>-0.006492</td>\n",
              "      <td>0.004675</td>\n",
              "      <td>-0.008567</td>\n",
              "      <td>0.005183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>2019-10-30</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12433706540954628, 0.005727754861520321, 0....</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.013800</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.18019</td>\n",
              "      <td>102.1</td>\n",
              "      <td>0.814018</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>-0.006467</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>-0.005875</td>\n",
              "      <td>0.003042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-11-30</td>\n",
              "      <td>2019-11-30</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12783230340482263, 0.0, 0.0, 0.364692102455...</td>\n",
              "      <td>[0.1428571428571427, 0.0, 0.2857142857142854, ...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.014343</td>\n",
              "      <td>-0.000150</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22920</td>\n",
              "      <td>101.7</td>\n",
              "      <td>0.713907</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.005825</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>-0.005392</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã— 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44811ecc-1032-4b9a-a504-bef050bd1e52')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44811ecc-1032-4b9a-a504-bef050bd1e52 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44811ecc-1032-4b9a-a504-bef050bd1e52');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e8acfec8-f143-4182-b209-35a478b02313\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e8acfec8-f143-4182-b209-35a478b02313')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e8acfec8-f143-4182-b209-35a478b02313 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Cumulative returns 2000-01-30 - 2024-11-30 - ML strategy: 2.0755 / Equal weight: 1.1529\n",
            "\n",
            "Cumulative returns 1969-07-30 - 2024-11-30 - ML strategy: 18.7681 / Equal weight: 3.8781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##RF2"
      ],
      "metadata": {
        "id": "JuCQA8Oco0kP"
      },
      "id": "JuCQA8Oco0kP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 2 â€” second RF run under RF2\n",
        "if RF2 or Hybrid:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # RF2: Drop _MA12 and GARCH_1M\n",
        "    # -------------------\n",
        "    RF2_FEATURES = [f for f in FEATURES if not f.endswith('_MA12') and f != 'GARCH_1M']\n",
        "    #RF2_FEATURES = [f for f in FEATURES if f not in ['Cape']]\n",
        "\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train    = 60\n",
        "    min_obs_regime      = 50\n",
        "    min_obs_train       = 0\n",
        "    use_regime_split    = False\n",
        "    default_hyperparams = False\n",
        "\n",
        "    use_fixed_window    = True\n",
        "    rolling_window_size = 60\n",
        "\n",
        "    n_jobs = -1\n",
        "\n",
        "    # -------------------\n",
        "    # Hyperparameter Settings\n",
        "    # -------------------\n",
        "    if default_hyperparams:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': 'sqrt',\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 1,\n",
        "            'bootstrap': True,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "    else:\n",
        "        rf_params = {\n",
        "            'n_estimators': 100,\n",
        "            'max_depth': None,\n",
        "            'max_features': None,\n",
        "            'min_samples_split': 2,\n",
        "            'min_samples_leaf': 5,\n",
        "            'bootstrap': False,\n",
        "            'n_jobs': n_jobs\n",
        "        }\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results_rf2 = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Skip if insufficient data or overlapping dates\n",
        "        if len(train_window) < min_months_train:\n",
        "            continue\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            continue\n",
        "\n",
        "        # Prepare X_train / y_train using RF2_FEATURES\n",
        "        X_train = train_window[RF2_FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            continue\n",
        "\n",
        "        # Train RF\n",
        "        rf_model = RandomForestClassifier(**rf_params, random_state=42)\n",
        "        rf_model.fit(X_train, y_train)\n",
        "\n",
        "        # Prepare X_test\n",
        "        X_test = train_window[RF2_FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            continue\n",
        "\n",
        "        # Predict and map probabilities\n",
        "        probs = rf_model.predict_proba(X_test)[0]\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for cls, p in zip(rf_model.classes_, probs):\n",
        "            if cls in FACTORS:\n",
        "                full_probs[FACTORS.index(cls)] = p\n",
        "\n",
        "        # Calculate returns\n",
        "        allocated_return    = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Feature levels from RF2_FEATURES\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in RF2_FEATURES}\n",
        "\n",
        "        result = {\n",
        "            'Regime': 'NoRegime',\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_window['Date'].iloc[0],\n",
        "            'Train_End_Date': last_train_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': rf_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': rf_model.classes_[probs.argmax()],\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': rf_model.n_estimators,\n",
        "            'Average_Tree_Depth': np.mean([t.tree_.max_depth for t in rf_model.estimators_]),\n",
        "            'Max_Tree_Depth': np.max([t.tree_.max_depth for t in rf_model.estimators_]),\n",
        "            'Prediction_Horizon_Months': ((Predicted_month.year - last_train_date.year) * 12 +\n",
        "                                         (Predicted_month.month - last_train_date.month)),\n",
        "            **feature_levels\n",
        "        }\n",
        "        results_rf2.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build RF2 results DataFrame\n",
        "    # -------------------\n",
        "    results_df_rf2 = pd.DataFrame(results_rf2)\n",
        "    print(\"Final results_df_rf2 columns:\", results_df_rf2.columns.tolist())\n",
        "    display(results_df_rf2.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # Cumulative returns (2000 onward & total)\n",
        "    # -------------------\n",
        "    filtered2 = results_df_rf2[results_df_rf2['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered2.empty:\n",
        "        cum_alloc2 = (1 + filtered2['Allocated_Return']).prod() - 1\n",
        "        cum_eq2    = (1 + filtered2['Equal_Weight_Return']).prod() - 1\n",
        "        print(f\"Cumulative 2000â€“present â€” RF2: {cum_alloc2:.4f}  /  Equal: {cum_eq2:.4f}\")\n",
        "\n",
        "    if not results_df_rf2.empty:\n",
        "        cum_alloc_all2 = (1 + results_df_rf2['Allocated_Return']).prod() - 1\n",
        "        cum_eq_all2    = (1 + results_df_rf2['Equal_Weight_Return']).prod() - 1\n",
        "        print(f\"Total cumulative â€” RF2: {cum_alloc_all2:.4f}  /  Equal: {cum_eq_all2:.4f}\")\n"
      ],
      "metadata": {
        "id": "CMbhV42UouKY"
      },
      "id": "CMbhV42UouKY",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gradient boosting\n"
      ],
      "metadata": {
        "id": "MSWv9xFlDbMz"
      },
      "id": "MSWv9xFlDbMz"
    },
    {
      "cell_type": "code",
      "source": [
        "if GB:\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    # -------------------\n",
        "    # 1) Parameters\n",
        "    # -------------------\n",
        "    min_months_train = 60     # Minimum months of data needed (5 years for monthly data)\n",
        "    min_obs_regime = 50       # Minimum observations per regime if splitting\n",
        "    min_obs_train = 0         # Minimum total observations after dropping NAs\n",
        "    use_regime_split = False  # Toggle regime-based training or not\n",
        "    default_hyperparameters = False  # If True, override manually set hyperparameters\n",
        "\n",
        "    # Toggle for training window type:\n",
        "    use_fixed_window = True   # True for fixed (rolling) window, False for expanding window\n",
        "    rolling_window_size = 60  # When using a fixed window, use this many most recent rows\n",
        "\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "    results = []\n",
        "\n",
        "    # -------------------\n",
        "    # 2) Main Loop: Predict for each row in df_sorted\n",
        "    # -------------------\n",
        "    for i in range(1, len(df_sorted)):\n",
        "        test_row = df_sorted.iloc[i]\n",
        "        Predicted_month = test_row['Date']\n",
        "\n",
        "        # Build training window: either fixed-size (rolling) or expanding window\n",
        "        if use_fixed_window:\n",
        "            start_idx = max(0, i - rolling_window_size)\n",
        "            train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "        else:\n",
        "            train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "        # Check that we have enough training rows (i.e., months)\n",
        "        if len(train_window) < min_months_train:\n",
        "            print(f\"Test row date: {Predicted_month.date()} - Insufficient training rows ({len(train_window)} rows). Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Get first and last training dates\n",
        "        train_start_date = train_window['Date'].iloc[0]\n",
        "        train_end_date = train_window['Date'].iloc[-1]\n",
        "\n",
        "        # Regime-based checks (if enabled)\n",
        "        if use_regime_split:\n",
        "            regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "            insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "            if insufficient_regimes:\n",
        "                regime_str_list = [regime_short_mapping.get(r, str(r)) for r in insufficient_regimes]\n",
        "                regime_str = \", \".join(regime_str_list)\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active. Insufficient data in: {regime_str}. Skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Use only training data for the current regime\n",
        "            current_regime = test_row[REGIMES_COLUMN]\n",
        "            train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "            if len(train_window) < min_obs_regime:\n",
        "                regime_str = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "                print(f\"Test row date: {Predicted_month.date()}\")\n",
        "                print(f\"  Regime split active ({regime_str}). Only {len(train_window)} obs. Skipping.\")\n",
        "                continue\n",
        "            regime_used = regime_short_mapping.get(current_regime, str(current_regime))\n",
        "        else:\n",
        "            regime_used = 'NoRegime'\n",
        "\n",
        "        # Ensure the last training date is strictly before the test date\n",
        "        last_train_date = train_window['Date'].iloc[-1]\n",
        "        if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "            print(f\"Test row {Predicted_month.date()}: last training date not strictly before test month. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train = train_window[FEATURES].dropna()\n",
        "        y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "        if len(X_train) < min_obs_train:\n",
        "            print(f\"   -> After dropping NAs: {len(X_train)} < {min_obs_train}. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        # Convert y_train from strings to numeric codes and save mapping\n",
        "        y_train_cat = y_train.astype('category')\n",
        "        mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "        y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "        # -------------------\n",
        "        # Set hyperparameters based on default_hyperparameters flag\n",
        "        # -------------------\n",
        "        if default_hyperparameters:\n",
        "            xgb_params = {\n",
        "                'n_estimators': 100,\n",
        "                'max_depth': 3,\n",
        "                'learning_rate': 0.1,\n",
        "                'subsample': 1.0,\n",
        "                'colsample_bytree': 1.0,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss'\n",
        "            }\n",
        "        else:\n",
        "            # Use manually defined hyperparameters (from Optuna or otherwise)\n",
        "            xgb_params = {\n",
        "                'n_estimators': 500,\n",
        "                'max_depth': 15,\n",
        "                'learning_rate': 0.07,\n",
        "                'subsample': 1,\n",
        "                'colsample_bytree': 0.55,\n",
        "                'random_state': 42,\n",
        "                'eval_metric': 'mlogloss',\n",
        "                'min_child_weight': 2,\n",
        "                'gamma': 0.019\n",
        "            }\n",
        "\n",
        "        # Fit XGBoost gradient boosting classifier on numeric labels (full training, no early stopping)\n",
        "        xgb_model = XGBClassifier(**xgb_params)\n",
        "        xgb_model.fit(X_train, y_train_numeric)\n",
        "\n",
        "        # Prepare test data (using the last row in the training window)\n",
        "        X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "        if X_test.empty:\n",
        "            print(\"   -> Test features empty, skipping iteration.\")\n",
        "            continue\n",
        "\n",
        "        predicted_probabilities = xgb_model.predict_proba(X_test)[0]\n",
        "        # Get predicted numeric class and convert back to original factor name\n",
        "        predicted_numeric = xgb_model.classes_[predicted_probabilities.argmax()]\n",
        "        predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "        # Map predicted probabilities onto the full set of FACTORS\n",
        "        full_probs = np.zeros(len(FACTORS))\n",
        "        for code, prob in zip(xgb_model.classes_, predicted_probabilities):\n",
        "            factor_name = mapping[code]\n",
        "            try:\n",
        "                idx = FACTORS.index(factor_name)\n",
        "                full_probs[idx] = prob\n",
        "            except ValueError:\n",
        "                pass  # Skip if factor not found in FACTORS\n",
        "\n",
        "        # Compute allocated return and equal weight return using the test row's factor returns\n",
        "        allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "        equal_weight_return = np.mean(test_row[FACTORS].values)\n",
        "\n",
        "        # Tree depth statistics are not required for XGB; set to None\n",
        "        avg_depth = None\n",
        "        max_depth = None\n",
        "\n",
        "        # Calculate prediction horizon (months ahead)\n",
        "        months_ahead = (Predicted_month.year - last_train_date.year) * 12 + (Predicted_month.month - last_train_date.month)\n",
        "\n",
        "        # Store the actual feature levels used in X_test\n",
        "        feature_levels = {f\"Feature_Level_{f}\": X_test[f].iloc[0] for f in FEATURES}\n",
        "\n",
        "        print(f\"Test row date: {Predicted_month.date()} -> Model trained, prediction made (using: {train_start_date.date()} - {train_end_date.date()})\")\n",
        "\n",
        "        # Build the result dictionary for this iteration\n",
        "        result = {\n",
        "            'Regime': regime_used,\n",
        "            'Predicted_month': Predicted_month,\n",
        "            'Train_Start_Date': train_start_date,\n",
        "            'Train_End_Date': train_end_date,\n",
        "            'Train_Count': len(X_train),\n",
        "            'Feature_Importances': xgb_model.feature_importances_,\n",
        "            'Predicted_Probabilities': full_probs,\n",
        "            'Predicted_Winner': predicted_winner,\n",
        "            'Allocated_Return': allocated_return,\n",
        "            'Equal_Weight_Return': equal_weight_return,\n",
        "            'Actual_Winner': test_row['Winning Factor'],\n",
        "            'Num_Trees': xgb_model.n_estimators,\n",
        "            'Average_Tree_Depth': avg_depth,\n",
        "            'Max_Tree_Depth': max_depth,\n",
        "            'Prediction_Horizon_Months': months_ahead,\n",
        "            **feature_levels\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # -------------------\n",
        "    # 3) Build the final results DataFrame for GB\n",
        "    # -------------------\n",
        "    results_df_gb = pd.DataFrame(results)\n",
        "    print(\"Final results_df_gb columns:\", results_df_gb.columns.tolist())\n",
        "    display(results_df_gb.tail(10))\n",
        "\n",
        "    # -------------------\n",
        "    # 4) Calculate and Print Cumulative Returns (Filtered: from 1 Jan 2000 onwards)\n",
        "    # -------------------\n",
        "    filtered_results = results_df_gb[results_df_gb['Predicted_month'] >= pd.Timestamp(\"2000-01-01\")]\n",
        "    if not filtered_results.empty:\n",
        "        cum_return_allocated = (1 + filtered_results['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal = (1 + filtered_results['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_pred_month = filtered_results.iloc[0]['Predicted_month']\n",
        "        last_pred_month = filtered_results.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_pred_month.date(), last_pred_month.date(),\n",
        "            cum_return_allocated, cum_return_equal))\n",
        "    else:\n",
        "        print(\"No predictions from 1 Jan 2000 onwards.\")\n",
        "\n",
        "    # -------------------\n",
        "    # 5) Calculate and Print Cumulative Returns for Total Time\n",
        "    # -------------------\n",
        "    if not results_df_gb.empty:\n",
        "        cum_return_allocated_total = (1 + results_df_gb['Allocated_Return']).prod() - 1\n",
        "        cum_return_equal_total = (1 + results_df_gb['Equal_Weight_Return']).prod() - 1\n",
        "\n",
        "        first_total_month = results_df_gb.iloc[0]['Predicted_month']\n",
        "        last_total_month = results_df_gb.iloc[-1]['Predicted_month']\n",
        "\n",
        "        print(\"\\nCumulative returns {} - {} - ML strategy: {:.4f} / Equal weight: {:.4f}\".format(\n",
        "            first_total_month.date(), last_total_month.date(),\n",
        "            cum_return_allocated_total, cum_return_equal_total))\n",
        "    else:\n",
        "        print(\"No predictions available for total time.\")\n"
      ],
      "metadata": {
        "id": "9sWAd_BnDVlB"
      },
      "id": "9sWAd_BnDVlB",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Gradint boosting loop"
      ],
      "metadata": {
        "id": "cqpzfBYhagcZ"
      },
      "id": "cqpzfBYhagcZ"
    },
    {
      "cell_type": "code",
      "source": [
        "if gb_loop:\n",
        "    import optuna\n",
        "    import time\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    # Define global parameters required by the objective function:\n",
        "    min_months_train = 60       # Minimum training period (in months)\n",
        "    min_obs_regime = 50         # Minimum observations per regime (if regime splitting is used)\n",
        "    min_obs_train = 0           # Minimum training observations after dropna\n",
        "    use_regime_split = False    # Toggle regime-based training\n",
        "    use_fixed_window = True     # Use fixed (rolling) window if True; else, expanding window\n",
        "    rolling_window_size = 60    # Number of rows for the fixed window\n",
        "\n",
        "    # Define feature and factor lists, regime column name, and regime mapping:\n",
        "    FEATURES = ['CPI%', 'T10YFF', 'LEI%', 'Amihud', 'GARCH_1M']\n",
        "    FACTORS = ['SMB', 'HML', 'CMA', 'RMW']\n",
        "    REGIMES_COLUMN = 'Predicted_reg'\n",
        "    # Use your actual regime mapping; this is a dummy mapping for demonstration:\n",
        "    regime_short_mapping = {0: 'RegimeA', 1: 'RegimeB'}\n",
        "\n",
        "    # Ensure the sorted dataframe is defined globally\n",
        "    df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "    # Global list for storing trial results for CSV logging later\n",
        "    trial_results = []\n",
        "\n",
        "    def objective(trial):\n",
        "        # Declare all global variables needed inside the function\n",
        "        global df_sorted, use_fixed_window, rolling_window_size, min_months_train\n",
        "        global use_regime_split, min_obs_regime, min_obs_train, FEATURES, FACTORS, REGIMES_COLUMN, regime_short_mapping\n",
        "\n",
        "        start_time = time.time()  # Start time of the trial\n",
        "\n",
        "        # Define hyperparameters; note n_jobs=-1 uses all available cores\n",
        "        xgb_params = {\n",
        "            'n_estimators': trial.suggest_int('n_estimators', 100, 600),\n",
        "            'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
        "            'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
        "            'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
        "            'n_jobs': -1,\n",
        "            'random_state': 42,\n",
        "            'eval_metric': 'mlogloss'\n",
        "        }\n",
        "\n",
        "        cumulative_return_total = 0.0      # Cumulative return over entire period\n",
        "        cumulative_return_after2000 = 0.0    # Cumulative return for dates >= 2000-01-01\n",
        "\n",
        "        # Loop over test rows in df_sorted. Predictions are made as soon as training period is long enough.\n",
        "        for i in range(1, len(df_sorted)):\n",
        "            test_row = df_sorted.iloc[i]\n",
        "            Predicted_month = test_row['Date']\n",
        "\n",
        "            # Build training window based on the selected window type\n",
        "            if use_fixed_window:\n",
        "                start_idx = max(0, i - rolling_window_size)\n",
        "                train_window = df_sorted.iloc[start_idx:i].copy()\n",
        "                if len(train_window) < rolling_window_size:\n",
        "                    continue  # Skip if window is too short\n",
        "            else:\n",
        "                train_window = df_sorted.iloc[:i].copy()\n",
        "\n",
        "            # Check that the training period is long enough\n",
        "            start_date = train_window['Date'].iloc[0]\n",
        "            training_months = (Predicted_month.year - start_date.year) * 12 + (Predicted_month.month - start_date.month)\n",
        "            if training_months < min_months_train:\n",
        "                continue  # Skip if training period is too short\n",
        "\n",
        "            # Optional regime-based splitting if enabled\n",
        "            if use_regime_split:\n",
        "                regime_counts = train_window[REGIMES_COLUMN].value_counts()\n",
        "                insufficient_regimes = regime_counts[regime_counts < min_obs_regime].index.tolist()\n",
        "                if insufficient_regimes:\n",
        "                    continue\n",
        "                current_regime = test_row[REGIMES_COLUMN]\n",
        "                train_window = train_window[train_window[REGIMES_COLUMN] == current_regime]\n",
        "                if len(train_window) < min_obs_regime:\n",
        "                    continue\n",
        "\n",
        "            # Ensure the last training date is strictly before the test date\n",
        "            last_train_date = train_window['Date'].iloc[-1]\n",
        "            if (last_train_date.year == Predicted_month.year) and (last_train_date.month >= Predicted_month.month):\n",
        "                continue\n",
        "\n",
        "            # Prepare training data\n",
        "            X_train = train_window[FEATURES].dropna()\n",
        "            y_train = train_window['Winning Factor'].loc[X_train.index]\n",
        "            if len(X_train) < min_obs_train:\n",
        "                continue\n",
        "\n",
        "            # Convert categorical target to numeric codes\n",
        "            y_train_cat = y_train.astype('category')\n",
        "            mapping = dict(enumerate(y_train_cat.cat.categories))\n",
        "            y_train_numeric = y_train_cat.cat.codes\n",
        "\n",
        "            # Train the XGBoost model using the trial's hyperparameters\n",
        "            model = XGBClassifier(**xgb_params)\n",
        "            model.fit(X_train, y_train_numeric)\n",
        "\n",
        "            # Prepare test data: use the last row from the training window as test features\n",
        "            X_test = train_window[FEATURES].iloc[[-1]].dropna()\n",
        "            if X_test.empty:\n",
        "                continue\n",
        "\n",
        "            predicted_probabilities = model.predict_proba(X_test)[0]\n",
        "            predicted_numeric = model.classes_[predicted_probabilities.argmax()]\n",
        "            predicted_winner = mapping[predicted_numeric]\n",
        "\n",
        "            # Map predicted probabilities onto the full set of FACTORS\n",
        "            full_probs = np.zeros(len(FACTORS))\n",
        "            for code, prob in zip(model.classes_, predicted_probabilities):\n",
        "                factor_name = mapping[code]\n",
        "                try:\n",
        "                    idx = FACTORS.index(factor_name)\n",
        "                    full_probs[idx] = prob\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "            allocated_return = (full_probs * test_row[FACTORS].values).sum()\n",
        "            cumulative_return_total += allocated_return\n",
        "            if Predicted_month >= pd.Timestamp('2000-01-01'):\n",
        "                cumulative_return_after2000 += allocated_return\n",
        "\n",
        "        # Record trial runtime\n",
        "        elapsed = time.time() - start_time\n",
        "        minutes = int(elapsed // 60)\n",
        "        seconds = int(elapsed % 60)\n",
        "        runtime_str = f\"{minutes:02d}:{seconds:02d}\"\n",
        "\n",
        "        # Print trial summary\n",
        "        print(f\"\\nTrial {trial.number} finished in {runtime_str}\")\n",
        "        print(f\"Hyperparameters: {trial.params}\")\n",
        "        print(f\"Cumulative Return (after 2000): {cumulative_return_after2000:.4f}\")\n",
        "        print(f\"Cumulative Return (total): {cumulative_return_total:.4f}\\n\")\n",
        "\n",
        "        # Save trial details (formatting floats with a comma as the decimal separator)\n",
        "        trial_record = {\n",
        "            'Trial': trial.number,\n",
        "            'Runtime': runtime_str,\n",
        "            'CumulativeReturnAfter2000': str(f\"{cumulative_return_after2000:.4f}\").replace('.', ','),\n",
        "            'CumulativeReturnTotal': str(f\"{cumulative_return_total:.4f}\").replace('.', ','),\n",
        "            **trial.params\n",
        "        }\n",
        "        trial_results.append(trial_record)\n",
        "\n",
        "        # Return cumulative return after 2000 as the objective value\n",
        "        return cumulative_return_after2000\n",
        "\n",
        "    def print_trial_info(study, trial):\n",
        "        best_trial = study.best_trial\n",
        "        print(\"\\n=== Current Best Trial ===\")\n",
        "        print(f\"Trial {best_trial.number}: Value: {best_trial.value:.4f}\")\n",
        "        print(f\"Hyperparameters: {best_trial.params}\\n\")\n",
        "\n",
        "    # Create and run the Optuna study\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=30, callbacks=[print_trial_info])\n",
        "\n",
        "    # After the study, save all trial results to CSV (using ';' as separator)\n",
        "    results_df = pd.DataFrame(trial_results)\n",
        "    results_df.to_csv(\"optuna_trials_results.csv\", sep=\";\", index=False)\n",
        "    print(\"All trial results saved to optuna_trials_results.csv\")\n"
      ],
      "metadata": {
        "id": "K6JoZwTdQ8lL"
      },
      "id": "K6JoZwTdQ8lL",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hybrid / tÃ¤Ã¤ sÃ¤ilyttÃ¤Ã¤ random forestin dataframen mut averagee painot ja laskee allocated returns nistÃ¤"
      ],
      "metadata": {
        "id": "0HZuBVFOW8qu"
      },
      "id": "0HZuBVFOW8qu"
    },
    {
      "cell_type": "code",
      "source": [
        "if Hybrid:\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from functools import reduce\n",
        "\n",
        "    # names of the two resultâ€DataFrames in your namespace\n",
        "    MODEL_DF_NAMES = ['results_df_rf', 'results_df_rf2']\n",
        "\n",
        "    # 1) load raw factorâ€return sheet & rename its date column to match the models\n",
        "    df_returns = xls_file.parse(SHEET_NAME)\n",
        "    df_returns.rename(columns={'Date': 'Predicted_month'}, inplace=True)\n",
        "    # FACTORS should already be defined as your list of factorâ€return columns\n",
        "    df_factor = df_returns[['Predicted_month'] + FACTORS].copy()\n",
        "\n",
        "    # 2) extract each modelâ€™s month + prob vector\n",
        "    prob_dfs = []\n",
        "    for name in MODEL_DF_NAMES:\n",
        "        tmp = globals()[name][['Predicted_month', 'Predicted_Probabilities']].copy()\n",
        "        tmp.rename(\n",
        "            columns={'Predicted_Probabilities': f'Prob_{name}'},\n",
        "            inplace=True\n",
        "        )\n",
        "        prob_dfs.append(tmp)\n",
        "\n",
        "    # 3) innerâ€‘join on Predicted_month\n",
        "    df_probs = reduce(\n",
        "        lambda left, right: pd.merge(left, right, on='Predicted_month', how='inner'),\n",
        "        prob_dfs\n",
        "    )\n",
        "\n",
        "    # 4) bring in the Actual_Winner from your first RF run\n",
        "    df_probs = pd.merge(\n",
        "        df_probs,\n",
        "        results_df_rf[['Predicted_month', 'Actual_Winner']],\n",
        "        on='Predicted_month',\n",
        "        how='left'\n",
        "    )\n",
        "\n",
        "    # 5) compute the averaged (â€œhybridâ€) probabilities\n",
        "    df_probs['Hybrid_Predicted_Probabilities'] = df_probs.apply(\n",
        "        lambda row: np.mean([row[f'Prob_{n}'] for n in MODEL_DF_NAMES], axis=0),\n",
        "        axis=1\n",
        "    )\n",
        "    # rename to match the other DataFramesâ€™ column\n",
        "    df_probs.rename(\n",
        "        columns={'Hybrid_Predicted_Probabilities': 'Predicted_Probabilities'},\n",
        "        inplace=True\n",
        "    )\n",
        "\n",
        "    # 6) merge in the factor returns\n",
        "    df_hybrid = pd.merge(\n",
        "        df_probs,\n",
        "        df_factor,\n",
        "        on='Predicted_month',\n",
        "        how='inner'\n",
        "    )\n",
        "\n",
        "    # 7) compute the hybrid allocated return\n",
        "    df_hybrid['Allocated_Return'] = df_hybrid.apply(\n",
        "        lambda row: np.dot(row['Predicted_Probabilities'], row[FACTORS].values),\n",
        "        axis=1\n",
        "    )\n",
        "\n",
        "    # 8) keep only date, actual winner, predicted probabilities, and the allocated return\n",
        "    df_hybrid = df_hybrid[\n",
        "        ['Predicted_month', 'Actual_Winner', 'Predicted_Probabilities', 'Allocated_Return']\n",
        "    ]"
      ],
      "metadata": {
        "id": "JKYtkk2MWPC0"
      },
      "id": "JKYtkk2MWPC0",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model evaluation"
      ],
      "metadata": {
        "id": "A1BqWL_QXA8e"
      },
      "id": "A1BqWL_QXA8e"
    },
    {
      "cell_type": "code",
      "source": [
        "# CellÂ 3 â€” collect both RF and RF2 (and others) into your dict\n",
        "results_dfs = {}\n",
        "\n",
        "if RF:\n",
        "    results_dfs[\"Random Forest\"] = results_df_rf.copy()\n",
        "    print(\"Results from Random Forest added.\")\n",
        "\n",
        "if RF2:\n",
        "    results_dfs[\"Random Forest 2\"] = results_df_rf2.copy()\n",
        "    print(\"Results from Random ForestÂ 2 added.\")\n",
        "\n",
        "if GB:\n",
        "    results_dfs[\"Gradient Boosting\"] = results_df_gb.copy()\n",
        "    print(\"Results from Gradient Boosting added.\")\n",
        "\n",
        "if Hybrid:\n",
        "    results_dfs[\"Hybrid\"] = df_hybrid.copy()\n",
        "    print(\"Results from Hybrid Model added.\")\n",
        "\n",
        "if not results_dfs:\n",
        "    raise ValueError(\"No valid model was selected; set at least one of [RF, RF2, GB, Hybrid] to True.\")\n",
        "\n",
        "print(\"\\nAvailable model results:\")\n",
        "for name, df in results_dfs.items():\n",
        "    print(f\" â€¢ {name}: {df.shape[0]} rows Ã— {df.shape[1]} cols\")\n",
        "\n",
        "from IPython.display import display\n",
        "for name, df in results_dfs.items():\n",
        "    print(f\"\\n=== {name} (first 5 rows) ===\")\n",
        "    display(df.head())"
      ],
      "metadata": {
        "id": "FOzCGJrxXKUm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 525
        },
        "outputId": "2cac2b1a-189a-4dec-a85a-e5bb614428d6"
      },
      "id": "FOzCGJrxXKUm",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results from Random Forest added.\n",
            "\n",
            "Available model results:\n",
            " â€¢ Random Forest: 665 rows Ã— 23 cols\n",
            "\n",
            "=== Random Forest (first 5 rows) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     Regime Predicted_month Train_Start_Date Train_End_Date  Train_Count  \\\n",
              "0  NoRegime      1969-07-30       1964-07-30     1969-06-30           60   \n",
              "1  NoRegime      1969-08-30       1964-08-30     1969-07-30           60   \n",
              "2  NoRegime      1969-09-30       1964-09-30     1969-08-30           60   \n",
              "3  NoRegime      1969-10-30       1964-10-30     1969-09-30           60   \n",
              "4  NoRegime      1969-11-30       1964-11-30     1969-10-30           60   \n",
              "\n",
              "                                 Feature_Importances  \\\n",
              "0  [0.0, 0.10026943803953141, 0.37131138335885644...   \n",
              "1  [0.0, 0.0993752084023835, 0.41297909045887454,...   \n",
              "2  [0.09272943925233637, 0.0919433808521796, 0.38...   \n",
              "3  [0.03193463765061456, 0.0, 0.41235618653859585...   \n",
              "4  [0.025458538539021476, 0.08059555075022062, 0....   \n",
              "\n",
              "                             Predicted_Probabilities Predicted_Winner  \\\n",
              "0                          [0.0, 0.125, 0.125, 0.75]              RMW   \n",
              "1  [0.0, 0.0, 0.7777777777777771, 0.2222222222222...              CMA   \n",
              "2  [0.0, 0.0, 0.39999999999999925, 0.600000000000...              RMW   \n",
              "3  [0.0, 0.0, 0.16666666666666646, 0.833333333333...              RMW   \n",
              "4  [0.7999999999999985, 0.19999999999999962, 0.0,...              SMB   \n",
              "\n",
              "   Allocated_Return  Equal_Weight_Return  ... Max_Tree_Depth  \\\n",
              "0          0.014988             0.002950  ...              6   \n",
              "1         -0.028756            -0.014950  ...              6   \n",
              "2          0.017060             0.001475  ...              6   \n",
              "3         -0.003167            -0.003250  ...              4   \n",
              "4         -0.021980            -0.004700  ...              5   \n",
              "\n",
              "   Prediction_Horizon_Months  Feature_Level_CPI%  Feature_Level_LEI  \\\n",
              "0                          1             0.27548               40.7   \n",
              "1                          1             0.54945               40.5   \n",
              "2                          1             0.54645               40.3   \n",
              "3                          1             0.27174               40.2   \n",
              "4                          1             0.54201               40.3   \n",
              "\n",
              "   Feature_Level_EWMA_0.94  Feature_Level_T10YFF  Feature_Level_SMB_MA12  \\\n",
              "0                 0.633686                 -0.77                0.002433   \n",
              "1                 1.181224                 -2.09               -0.001875   \n",
              "2                 0.665889                 -2.92               -0.003542   \n",
              "3                 0.627873                 -0.99               -0.004850   \n",
              "4                 0.582997                 -2.06               -0.006117   \n",
              "\n",
              "   Feature_Level_HML_MA12  Feature_Level_CMA_MA12  Feature_Level_RMW_MA12  \n",
              "0                0.010083                0.010975               -0.009525  \n",
              "1                0.008575                0.007342               -0.004750  \n",
              "2                0.005083                0.005833               -0.000908  \n",
              "3                0.001083                0.002083                0.000617  \n",
              "4               -0.001892                0.000675                0.005100  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-60bb5233-a0bd-4587-837e-fa93e473c986\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Regime</th>\n",
              "      <th>Predicted_month</th>\n",
              "      <th>Train_Start_Date</th>\n",
              "      <th>Train_End_Date</th>\n",
              "      <th>Train_Count</th>\n",
              "      <th>Feature_Importances</th>\n",
              "      <th>Predicted_Probabilities</th>\n",
              "      <th>Predicted_Winner</th>\n",
              "      <th>Allocated_Return</th>\n",
              "      <th>Equal_Weight_Return</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Tree_Depth</th>\n",
              "      <th>Prediction_Horizon_Months</th>\n",
              "      <th>Feature_Level_CPI%</th>\n",
              "      <th>Feature_Level_LEI</th>\n",
              "      <th>Feature_Level_EWMA_0.94</th>\n",
              "      <th>Feature_Level_T10YFF</th>\n",
              "      <th>Feature_Level_SMB_MA12</th>\n",
              "      <th>Feature_Level_HML_MA12</th>\n",
              "      <th>Feature_Level_CMA_MA12</th>\n",
              "      <th>Feature_Level_RMW_MA12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-07-30</td>\n",
              "      <td>1964-07-30</td>\n",
              "      <td>1969-06-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.10026943803953141, 0.37131138335885644...</td>\n",
              "      <td>[0.0, 0.125, 0.125, 0.75]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.014988</td>\n",
              "      <td>0.002950</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.27548</td>\n",
              "      <td>40.7</td>\n",
              "      <td>0.633686</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>0.002433</td>\n",
              "      <td>0.010083</td>\n",
              "      <td>0.010975</td>\n",
              "      <td>-0.009525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-08-30</td>\n",
              "      <td>1964-08-30</td>\n",
              "      <td>1969-07-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.0993752084023835, 0.41297909045887454,...</td>\n",
              "      <td>[0.0, 0.0, 0.7777777777777771, 0.2222222222222...</td>\n",
              "      <td>CMA</td>\n",
              "      <td>-0.028756</td>\n",
              "      <td>-0.014950</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54945</td>\n",
              "      <td>40.5</td>\n",
              "      <td>1.181224</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>-0.001875</td>\n",
              "      <td>0.008575</td>\n",
              "      <td>0.007342</td>\n",
              "      <td>-0.004750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-09-30</td>\n",
              "      <td>1964-09-30</td>\n",
              "      <td>1969-08-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.09272943925233637, 0.0919433808521796, 0.38...</td>\n",
              "      <td>[0.0, 0.0, 0.39999999999999925, 0.600000000000...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.017060</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54645</td>\n",
              "      <td>40.3</td>\n",
              "      <td>0.665889</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>0.005083</td>\n",
              "      <td>0.005833</td>\n",
              "      <td>-0.000908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-10-30</td>\n",
              "      <td>1964-10-30</td>\n",
              "      <td>1969-09-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.03193463765061456, 0.0, 0.41235618653859585...</td>\n",
              "      <td>[0.0, 0.0, 0.16666666666666646, 0.833333333333...</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.003167</td>\n",
              "      <td>-0.003250</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.27174</td>\n",
              "      <td>40.2</td>\n",
              "      <td>0.627873</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.004850</td>\n",
              "      <td>0.001083</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.000617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-11-30</td>\n",
              "      <td>1964-11-30</td>\n",
              "      <td>1969-10-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.025458538539021476, 0.08059555075022062, 0....</td>\n",
              "      <td>[0.7999999999999985, 0.19999999999999962, 0.0,...</td>\n",
              "      <td>SMB</td>\n",
              "      <td>-0.021980</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54201</td>\n",
              "      <td>40.3</td>\n",
              "      <td>0.582997</td>\n",
              "      <td>-2.06</td>\n",
              "      <td>-0.006117</td>\n",
              "      <td>-0.001892</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-60bb5233-a0bd-4587-837e-fa93e473c986')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-60bb5233-a0bd-4587-837e-fa93e473c986 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-60bb5233-a0bd-4587-837e-fa93e473c986');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-51d7d8a6-87e2-4d06-8cb3-eab37931cf2c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-51d7d8a6-87e2-4d06-8cb3-eab37931cf2c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-51d7d8a6-87e2-4d06-8cb3-eab37931cf2c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Increase column width so no text is truncated\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Define the date range\n",
        "start_date = pd.to_datetime('1968-08-01')\n",
        "end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# Dictionary to store filtered results for each model using the new naming format.\n",
        "filtered_results_dfs = {}\n",
        "\n",
        "# Loop through each model's results dataframe in results_dfs and add numbering.\n",
        "for i, (model_name, df) in enumerate(results_dfs.items(), 1):\n",
        "    new_model_name = f\"ML{i}: {model_name}\"\n",
        "\n",
        "    # Convert 'Predicted_month' to datetime if not already\n",
        "    df['Predicted_month'] = pd.to_datetime(df['Predicted_month'])\n",
        "\n",
        "    # Filter the DataFrame within the specified date range and sort by date.\n",
        "    filtered_df = df[(df['Predicted_month'] >= start_date) & (df['Predicted_month'] <= end_date)].copy().sort_values('Predicted_month')\n",
        "\n",
        "    # Store the filtered dataframe in our new dictionary using the new model name.\n",
        "    filtered_results_dfs[new_model_name] = filtered_df\n",
        "\n",
        "    # Display the filtered results with a header showing the new model name.\n",
        "    print(f\"\\n=== Filtered Results for Model '{new_model_name}' ===\")\n",
        "    display(filtered_df)\n",
        "\n",
        "# Reset column width option to default after display.\n",
        "pd.reset_option('display.max_colwidth')\n"
      ],
      "metadata": {
        "id": "JmpzyRpGWP0n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20a5bddc-a0d5-47da-d974-c5ee3302ad28"
      },
      "id": "JmpzyRpGWP0n",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Filtered Results for Model 'ML1: Random Forest' ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "       Regime Predicted_month Train_Start_Date Train_End_Date  Train_Count  \\\n",
              "0    NoRegime      1969-07-30       1964-07-30     1969-06-30           60   \n",
              "1    NoRegime      1969-08-30       1964-08-30     1969-07-30           60   \n",
              "2    NoRegime      1969-09-30       1964-09-30     1969-08-30           60   \n",
              "3    NoRegime      1969-10-30       1964-10-30     1969-09-30           60   \n",
              "4    NoRegime      1969-11-30       1964-11-30     1969-10-30           60   \n",
              "..        ...             ...              ...            ...          ...   \n",
              "660  NoRegime      2024-07-30       2019-07-30     2024-06-30           60   \n",
              "661  NoRegime      2024-08-30       2019-08-30     2024-07-30           60   \n",
              "662  NoRegime      2024-09-30       2019-09-30     2024-08-30           60   \n",
              "663  NoRegime      2024-10-30       2019-10-30     2024-09-30           60   \n",
              "664  NoRegime      2024-11-30       2019-11-30     2024-10-30           60   \n",
              "\n",
              "                                                                                                                                                              Feature_Importances  \\\n",
              "0                                                         [0.0, 0.10026943803953141, 0.37131138335885644, 0.0, 0.22256621107596758, 0.1686689159148776, 0.0, 0.13718405161076683]   \n",
              "1                                                         [0.0, 0.0993752084023835, 0.41297909045887454, 0.0, 0.22341265790556697, 0.14225115312691736, 0.0, 0.12198189010625754]   \n",
              "2                                           [0.09272943925233637, 0.0919433808521796, 0.3870971743122873, 0.0, 0.2078985174451667, 0.11726103693813952, 0.0, 0.10307045119989047]   \n",
              "3                                       [0.03193463765061456, 0.0, 0.41235618653859585, 0.027203580220893888, 0.0, 0.22776283221210036, 0.21812890409546076, 0.08261385928233464]   \n",
              "4      [0.025458538539021476, 0.08059555075022062, 0.4994077154266129, 0.12331698454939533, 0.03874760123343152, 0.13026361635647973, 0.10154244347596411, 0.0006675496688741748]   \n",
              "..                                                                                                                                                                            ...   \n",
              "660                                                        [0.11669339034384454, 0.0, 0.2375168035242865, 0.26149904109158323, 0.05371459112773911, 0.3305761739125466, 0.0, 0.0]   \n",
              "661                                        [0.1627590379670246, 0.0, 0.10815307820299522, 0.22701024139802684, 0.10391771290273805, 0.29126737671320163, 0.0, 0.1068925528160137]   \n",
              "662                                       [0.18611193366893655, 0.0, 0.08024874148652647, 0.21630962127225797, 0.10528836941942882, 0.3030303030303035, 0.0, 0.10901103112254687]   \n",
              "663  [0.12433706540954628, 0.005727754861520321, 0.0064914555097230296, 0.379228522179751, 0.2730658381100342, 0.19816645290997914, 0.0064914555097230296, 0.0064914555097230296]   \n",
              "664                                                                       [0.12783230340482263, 0.0, 0.0, 0.3646921024559112, 0.26110529029253726, 0.24637030384672884, 0.0, 0.0]   \n",
              "\n",
              "                                                  Predicted_Probabilities  \\\n",
              "0                                               [0.0, 0.125, 0.125, 0.75]   \n",
              "1                     [0.0, 0.0, 0.7777777777777771, 0.22222222222222177]   \n",
              "2                      [0.0, 0.0, 0.39999999999999925, 0.600000000000001]   \n",
              "3                      [0.0, 0.0, 0.16666666666666646, 0.833333333333333]   \n",
              "4                     [0.7999999999999985, 0.19999999999999962, 0.0, 0.0]   \n",
              "..                                                                    ...   \n",
              "660  [0.39999999999999925, 0.19999999999999962, 0.0, 0.39999999999999925]   \n",
              "661                   [0.7999999999999985, 0.0, 0.0, 0.19999999999999962]   \n",
              "662                   [0.16666666666666646, 0.0, 0.5, 0.3333333333333329]   \n",
              "663                                                  [0.0, 0.0, 0.0, 1.0]   \n",
              "664     [0.1428571428571427, 0.0, 0.2857142857142854, 0.5714285714285708]   \n",
              "\n",
              "    Predicted_Winner  Allocated_Return  Equal_Weight_Return  ...  \\\n",
              "0                RMW          0.014988             0.002950  ...   \n",
              "1                CMA         -0.028756            -0.014950  ...   \n",
              "2                RMW          0.017060             0.001475  ...   \n",
              "3                RMW         -0.003167            -0.003250  ...   \n",
              "4                SMB         -0.021980            -0.004700  ...   \n",
              "..               ...               ...                  ...  ...   \n",
              "660              RMW          0.045480             0.036675  ...   \n",
              "661              SMB         -0.027500            -0.007675  ...   \n",
              "662              CMA         -0.002867            -0.009575  ...   \n",
              "663              RMW         -0.013800            -0.000850  ...   \n",
              "664              RMW         -0.014343            -0.000150  ...   \n",
              "\n",
              "    Max_Tree_Depth  Prediction_Horizon_Months  Feature_Level_CPI%  \\\n",
              "0                6                          1             0.27548   \n",
              "1                6                          1             0.54945   \n",
              "2                6                          1             0.54645   \n",
              "3                4                          1             0.27174   \n",
              "4                5                          1             0.54201   \n",
              "..             ...                        ...                 ...   \n",
              "660              7                          1             0.03961   \n",
              "661              7                          1            -0.00287   \n",
              "662              7                          1             0.13892   \n",
              "663              6                          1             0.18019   \n",
              "664              6                          1             0.22920   \n",
              "\n",
              "     Feature_Level_LEI  Feature_Level_EWMA_0.94  Feature_Level_T10YFF  \\\n",
              "0                 40.7                 0.633686                 -0.77   \n",
              "1                 40.5                 1.181224                 -2.09   \n",
              "2                 40.3                 0.665889                 -2.92   \n",
              "3                 40.2                 0.627873                 -0.99   \n",
              "4                 40.3                 0.582997                 -2.06   \n",
              "..                 ...                      ...                   ...   \n",
              "660              103.1                 0.411006                 -0.97   \n",
              "661              102.9                 0.964565                 -1.24   \n",
              "662              102.4                 1.144704                 -1.42   \n",
              "663              102.1                 0.814018                 -1.02   \n",
              "664              101.7                 0.713907                 -0.55   \n",
              "\n",
              "     Feature_Level_SMB_MA12  Feature_Level_HML_MA12  Feature_Level_CMA_MA12  \\\n",
              "0                  0.002433                0.010083                0.010975   \n",
              "1                 -0.001875                0.008575                0.007342   \n",
              "2                 -0.003542                0.005083                0.005833   \n",
              "3                 -0.004850                0.001083                0.002083   \n",
              "4                 -0.006117               -0.001892                0.000675   \n",
              "..                      ...                     ...                     ...   \n",
              "660               -0.006250                0.005908               -0.008275   \n",
              "661               -0.011025                0.003317               -0.008408   \n",
              "662               -0.006492                0.004675               -0.008567   \n",
              "663               -0.006467                0.004633               -0.005875   \n",
              "664               -0.005825                0.001267               -0.005392   \n",
              "\n",
              "     Feature_Level_RMW_MA12  \n",
              "0                 -0.009525  \n",
              "1                 -0.004750  \n",
              "2                 -0.000908  \n",
              "3                  0.000617  \n",
              "4                  0.005100  \n",
              "..                      ...  \n",
              "660                0.005992  \n",
              "661                0.004525  \n",
              "662                0.005183  \n",
              "663                0.003042  \n",
              "664                0.001533  \n",
              "\n",
              "[665 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e340ed03-f5e6-42ea-bc50-4cdd692560ab\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Regime</th>\n",
              "      <th>Predicted_month</th>\n",
              "      <th>Train_Start_Date</th>\n",
              "      <th>Train_End_Date</th>\n",
              "      <th>Train_Count</th>\n",
              "      <th>Feature_Importances</th>\n",
              "      <th>Predicted_Probabilities</th>\n",
              "      <th>Predicted_Winner</th>\n",
              "      <th>Allocated_Return</th>\n",
              "      <th>Equal_Weight_Return</th>\n",
              "      <th>...</th>\n",
              "      <th>Max_Tree_Depth</th>\n",
              "      <th>Prediction_Horizon_Months</th>\n",
              "      <th>Feature_Level_CPI%</th>\n",
              "      <th>Feature_Level_LEI</th>\n",
              "      <th>Feature_Level_EWMA_0.94</th>\n",
              "      <th>Feature_Level_T10YFF</th>\n",
              "      <th>Feature_Level_SMB_MA12</th>\n",
              "      <th>Feature_Level_HML_MA12</th>\n",
              "      <th>Feature_Level_CMA_MA12</th>\n",
              "      <th>Feature_Level_RMW_MA12</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-07-30</td>\n",
              "      <td>1964-07-30</td>\n",
              "      <td>1969-06-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.10026943803953141, 0.37131138335885644, 0.0, 0.22256621107596758, 0.1686689159148776, 0.0, 0.13718405161076683]</td>\n",
              "      <td>[0.0, 0.125, 0.125, 0.75]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.014988</td>\n",
              "      <td>0.002950</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.27548</td>\n",
              "      <td>40.7</td>\n",
              "      <td>0.633686</td>\n",
              "      <td>-0.77</td>\n",
              "      <td>0.002433</td>\n",
              "      <td>0.010083</td>\n",
              "      <td>0.010975</td>\n",
              "      <td>-0.009525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-08-30</td>\n",
              "      <td>1964-08-30</td>\n",
              "      <td>1969-07-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.0, 0.0993752084023835, 0.41297909045887454, 0.0, 0.22341265790556697, 0.14225115312691736, 0.0, 0.12198189010625754]</td>\n",
              "      <td>[0.0, 0.0, 0.7777777777777771, 0.22222222222222177]</td>\n",
              "      <td>CMA</td>\n",
              "      <td>-0.028756</td>\n",
              "      <td>-0.014950</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54945</td>\n",
              "      <td>40.5</td>\n",
              "      <td>1.181224</td>\n",
              "      <td>-2.09</td>\n",
              "      <td>-0.001875</td>\n",
              "      <td>0.008575</td>\n",
              "      <td>0.007342</td>\n",
              "      <td>-0.004750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-09-30</td>\n",
              "      <td>1964-09-30</td>\n",
              "      <td>1969-08-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.09272943925233637, 0.0919433808521796, 0.3870971743122873, 0.0, 0.2078985174451667, 0.11726103693813952, 0.0, 0.10307045119989047]</td>\n",
              "      <td>[0.0, 0.0, 0.39999999999999925, 0.600000000000001]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.017060</td>\n",
              "      <td>0.001475</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54645</td>\n",
              "      <td>40.3</td>\n",
              "      <td>0.665889</td>\n",
              "      <td>-2.92</td>\n",
              "      <td>-0.003542</td>\n",
              "      <td>0.005083</td>\n",
              "      <td>0.005833</td>\n",
              "      <td>-0.000908</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-10-30</td>\n",
              "      <td>1964-10-30</td>\n",
              "      <td>1969-09-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.03193463765061456, 0.0, 0.41235618653859585, 0.027203580220893888, 0.0, 0.22776283221210036, 0.21812890409546076, 0.08261385928233464]</td>\n",
              "      <td>[0.0, 0.0, 0.16666666666666646, 0.833333333333333]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.003167</td>\n",
              "      <td>-0.003250</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>0.27174</td>\n",
              "      <td>40.2</td>\n",
              "      <td>0.627873</td>\n",
              "      <td>-0.99</td>\n",
              "      <td>-0.004850</td>\n",
              "      <td>0.001083</td>\n",
              "      <td>0.002083</td>\n",
              "      <td>0.000617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>1969-11-30</td>\n",
              "      <td>1964-11-30</td>\n",
              "      <td>1969-10-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.025458538539021476, 0.08059555075022062, 0.4994077154266129, 0.12331698454939533, 0.03874760123343152, 0.13026361635647973, 0.10154244347596411, 0.0006675496688741748]</td>\n",
              "      <td>[0.7999999999999985, 0.19999999999999962, 0.0, 0.0]</td>\n",
              "      <td>SMB</td>\n",
              "      <td>-0.021980</td>\n",
              "      <td>-0.004700</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>0.54201</td>\n",
              "      <td>40.3</td>\n",
              "      <td>0.582997</td>\n",
              "      <td>-2.06</td>\n",
              "      <td>-0.006117</td>\n",
              "      <td>-0.001892</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.005100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>660</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>2019-07-30</td>\n",
              "      <td>2024-06-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.11669339034384454, 0.0, 0.2375168035242865, 0.26149904109158323, 0.05371459112773911, 0.3305761739125466, 0.0, 0.0]</td>\n",
              "      <td>[0.39999999999999925, 0.19999999999999962, 0.0, 0.39999999999999925]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>0.045480</td>\n",
              "      <td>0.036675</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.03961</td>\n",
              "      <td>103.1</td>\n",
              "      <td>0.411006</td>\n",
              "      <td>-0.97</td>\n",
              "      <td>-0.006250</td>\n",
              "      <td>0.005908</td>\n",
              "      <td>-0.008275</td>\n",
              "      <td>0.005992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>2019-08-30</td>\n",
              "      <td>2024-07-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.1627590379670246, 0.0, 0.10815307820299522, 0.22701024139802684, 0.10391771290273805, 0.29126737671320163, 0.0, 0.1068925528160137]</td>\n",
              "      <td>[0.7999999999999985, 0.0, 0.0, 0.19999999999999962]</td>\n",
              "      <td>SMB</td>\n",
              "      <td>-0.027500</td>\n",
              "      <td>-0.007675</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.00287</td>\n",
              "      <td>102.9</td>\n",
              "      <td>0.964565</td>\n",
              "      <td>-1.24</td>\n",
              "      <td>-0.011025</td>\n",
              "      <td>0.003317</td>\n",
              "      <td>-0.008408</td>\n",
              "      <td>0.004525</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>662</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>2019-09-30</td>\n",
              "      <td>2024-08-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.18611193366893655, 0.0, 0.08024874148652647, 0.21630962127225797, 0.10528836941942882, 0.3030303030303035, 0.0, 0.10901103112254687]</td>\n",
              "      <td>[0.16666666666666646, 0.0, 0.5, 0.3333333333333329]</td>\n",
              "      <td>CMA</td>\n",
              "      <td>-0.002867</td>\n",
              "      <td>-0.009575</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0.13892</td>\n",
              "      <td>102.4</td>\n",
              "      <td>1.144704</td>\n",
              "      <td>-1.42</td>\n",
              "      <td>-0.006492</td>\n",
              "      <td>0.004675</td>\n",
              "      <td>-0.008567</td>\n",
              "      <td>0.005183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>663</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>2019-10-30</td>\n",
              "      <td>2024-09-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12433706540954628, 0.005727754861520321, 0.0064914555097230296, 0.379228522179751, 0.2730658381100342, 0.19816645290997914, 0.0064914555097230296, 0.0064914555097230296]</td>\n",
              "      <td>[0.0, 0.0, 0.0, 1.0]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.013800</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.18019</td>\n",
              "      <td>102.1</td>\n",
              "      <td>0.814018</td>\n",
              "      <td>-1.02</td>\n",
              "      <td>-0.006467</td>\n",
              "      <td>0.004633</td>\n",
              "      <td>-0.005875</td>\n",
              "      <td>0.003042</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>664</th>\n",
              "      <td>NoRegime</td>\n",
              "      <td>2024-11-30</td>\n",
              "      <td>2019-11-30</td>\n",
              "      <td>2024-10-30</td>\n",
              "      <td>60</td>\n",
              "      <td>[0.12783230340482263, 0.0, 0.0, 0.3646921024559112, 0.26110529029253726, 0.24637030384672884, 0.0, 0.0]</td>\n",
              "      <td>[0.1428571428571427, 0.0, 0.2857142857142854, 0.5714285714285708]</td>\n",
              "      <td>RMW</td>\n",
              "      <td>-0.014343</td>\n",
              "      <td>-0.000150</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>0.22920</td>\n",
              "      <td>101.7</td>\n",
              "      <td>0.713907</td>\n",
              "      <td>-0.55</td>\n",
              "      <td>-0.005825</td>\n",
              "      <td>0.001267</td>\n",
              "      <td>-0.005392</td>\n",
              "      <td>0.001533</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>665 rows Ã— 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e340ed03-f5e6-42ea-bc50-4cdd692560ab')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e340ed03-f5e6-42ea-bc50-4cdd692560ab button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e340ed03-f5e6-42ea-bc50-4cdd692560ab');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-029a2fae-0894-42b6-b174-b7c529fe52ab\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-029a2fae-0894-42b6-b174-b7c529fe52ab')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-029a2fae-0894-42b6-b174-b7c529fe52ab button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_a34a3a28-db33-43f1-9b6b-d3a6a16dc655\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('filtered_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a34a3a28-db33-43f1-9b6b-d3a6a16dc655 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('filtered_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "filtered_df"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "88d0f495-1290-403b-86a2-7bd1c2c12814",
      "metadata": {
        "tags": [],
        "id": "88d0f495-1290-403b-86a2-7bd1c2c12814",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        },
        "outputId": "d6c8c954-1e4f-4ef1-bddf-4af903107d06"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h3>Overall Performance Metrics Summary (Side by Side)</h3><table border='1' cellpadding='5'><tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th><th>Samples</th></tr><tr><td>Random Forest</td><td>0.3203</td><td>0.3214</td><td>0.3203</td><td>0.3207</td><td>665</td></tr></table>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Use FACTORS directly as labels for the confusion matrix axes.\n",
        "labels = FACTORS\n",
        "\n",
        "# Determine how many models we have.\n",
        "num_models = len(results_dfs)\n",
        "\n",
        "# Create a figure with 1 row and num_models columns.\n",
        "fig, axes = plt.subplots(nrows=1, ncols=num_models, figsize=(8 * num_models, 8))\n",
        "if num_models == 1:\n",
        "    axes = [axes]  # Make it iterable\n",
        "\n",
        "# A dictionary to hold overall metrics for each model.\n",
        "overall_metrics_dict = {}\n",
        "\n",
        "# Iterate over the dictionary and plot the overall confusion matrix for each model.\n",
        "for i, (model_key, results_df) in enumerate(results_dfs.items()):\n",
        "    # Extract true and predicted winners.\n",
        "    all_true = results_df['Actual_Winner']\n",
        "    all_pred = results_df['Predicted_Winner']\n",
        "\n",
        "    # Compute the overall confusion matrix.\n",
        "    cm_total = confusion_matrix(all_true, all_pred, labels=labels)\n",
        "\n",
        "    # Plot the confusion matrix for this model.\n",
        "    sns.heatmap(cm_total, annot=True, fmt='d', cmap=\"Blues\",\n",
        "                xticklabels=labels, yticklabels=labels, ax=axes[i])\n",
        "    axes[i].set_xlabel(\"Predicted Labels\")\n",
        "    axes[i].set_ylabel(\"True Labels\")\n",
        "    axes[i].set_title(f\"{model_key} (n = {len(all_true)})\")\n",
        "\n",
        "    # Compute overall performance metrics.\n",
        "    overall_metrics_dict[model_key] = {\n",
        "         \"Accuracy\": accuracy_score(all_true, all_pred),\n",
        "         \"Precision\": precision_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"Recall\": recall_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"F1 Score\": f1_score(all_true, all_pred, average='weighted', zero_division=0),\n",
        "         \"Samples\": len(all_true)\n",
        "    }\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.show()\n",
        "\n",
        "# Build an HTML table summarizing overall performance metrics for all models.\n",
        "html_overall = \"<h3>Overall Performance Metrics Summary (Side by Side)</h3>\"\n",
        "html_overall += \"<table border='1' cellpadding='5'><tr><th>Model</th><th>Accuracy</th><th>Precision</th><th>Recall</th><th>F1 Score</th><th>Samples</th></tr>\"\n",
        "\n",
        "for model_key, metrics in overall_metrics_dict.items():\n",
        "    html_overall += f\"<tr><td>{model_key}</td><td>{metrics['Accuracy']:.4f}</td><td>{metrics['Precision']:.4f}</td>\"\n",
        "    html_overall += f\"<td>{metrics['Recall']:.4f}</td><td>{metrics['F1 Score']:.4f}</td><td>{metrics['Samples']}</td></tr>\"\n",
        "\n",
        "html_overall += \"</table>\"\n",
        "\n",
        "# Display the metrics table.\n",
        "display(HTML(html_overall))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance"
      ],
      "metadata": {
        "id": "-U5ovbRVPoPM"
      },
      "id": "-U5ovbRVPoPM"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5",
      "metadata": {
        "tags": [],
        "id": "6f3b40dd-d04a-475d-b328-7baabf2114f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "ee581c21-a6c5-4f3e-e007-6b9cc24d468e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assume FEATURES is defined (e.g., FEATURES = ['CPI%', 'T10YFF', 'LEI%', 'Amihud', 'GARCH_1M'])\n",
        "# and results_dfs is a dictionary with keys like \"RF\", \"GB\", (and maybe \"Hybrid\")\n",
        "# where each value is a DataFrame that has a column \"Feature_Importances\" (an array).\n",
        "\n",
        "# 1. Compute overall average feature importances for each model.\n",
        "model_importances = {}\n",
        "for model_key, df in results_dfs.items():\n",
        "    # Stack the arrays from the \"Feature_Importances\" column and average over predictions.\n",
        "    model_importances[model_key] = np.vstack(df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# 2. Create a DataFrame from the computed importances.\n",
        "# Rows: features, Columns: model keys.\n",
        "importance_df = pd.DataFrame(model_importances, index=FEATURES)\n",
        "\n",
        "# Optional: sort features by overall mean importance (averaged across models) so that\n",
        "# the most important features appear on top.\n",
        "importance_df['Mean'] = importance_df.mean(axis=1)\n",
        "importance_df = importance_df.sort_values(by='Mean', ascending=False)\n",
        "sorted_features = importance_df.index.tolist()\n",
        "importance_df = importance_df.drop(columns=['Mean'])\n",
        "\n",
        "# 3. Plot a grouped horizontal bar chart.\n",
        "models = importance_df.columns.tolist()\n",
        "n_models = len(models)\n",
        "n_features = len(sorted_features)\n",
        "y = np.arange(n_features)  # base positions for each feature group\n",
        "bar_height = 0.8 / n_models  # total group thickness is 0.8\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, max(4, n_features * 0.6)))\n",
        "for i, model in enumerate(models):\n",
        "    # Calculate an offset for each model in the group.\n",
        "    offset = (i - n_models/2) * bar_height + bar_height/2\n",
        "    ax.barh(y + offset, importance_df.loc[sorted_features, model], height=bar_height, label=model)\n",
        "\n",
        "ax.set_yticks(y)\n",
        "ax.set_yticklabels(sorted_features)\n",
        "ax.invert_yaxis()  # so the top feature is at the top\n",
        "ax.set_xlabel(\"Average Feature Importance\")\n",
        "ax.set_title(\"Comparison of Overall Average Feature Importances Across Models\")\n",
        "ax.legend(title=\"Model\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Allocation chart"
      ],
      "metadata": {
        "id": "bhRxmOoFPtEI"
      },
      "id": "bhRxmOoFPtEI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8",
      "metadata": {
        "tags": [],
        "id": "9714d2c9-98b2-4fd5-8ea1-db0c582091a8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Determine how many models to plot\n",
        "num_models = len(results_dfs)\n",
        "\n",
        "# Create subplots: one row per model, sharing the x-axis\n",
        "fig, axes = plt.subplots(nrows=num_models, ncols=1, figsize=(12, 6 * num_models), sharex=True)\n",
        "if num_models == 1:\n",
        "    axes = [axes]  # Ensure iterable if only one model\n",
        "\n",
        "# Loop through each model's result dataframe\n",
        "for ax, (model_key, df_model) in zip(axes, results_dfs.items()):\n",
        "    df_temp = df_model.copy()\n",
        "\n",
        "    # 1) Convert Predicted_month to datetime (robust handling)\n",
        "    df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "\n",
        "    # 2) Drop rows with invalid dates\n",
        "    df_temp = df_temp.dropna(subset=[\"Predicted_month\"])\n",
        "    df_temp = df_temp.sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "    # 3) Stack probabilities\n",
        "    full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "    probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "    probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "    # 4) Sort by date again (just in case)\n",
        "    probability_df = probability_df.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "    # 5) Plot stackplot\n",
        "    ax.stackplot(\n",
        "        probability_df[\"Date\"],\n",
        "        [probability_df[col] for col in FACTORS],\n",
        "        labels=FACTORS,\n",
        "        alpha=0.8\n",
        "    )\n",
        "\n",
        "    # âœ… Legend: top right, inside chart, solid white background\n",
        "    ax.legend(\n",
        "        loc=\"upper right\",\n",
        "        fontsize=\"small\",\n",
        "        title=\"Factors\",\n",
        "        frameon=True,\n",
        "        framealpha=1.0,      # fully opaque\n",
        "        facecolor='white',   # solid background\n",
        "        edgecolor='black'    # optional: to match style\n",
        "    )\n",
        "\n",
        "    # Set title, axes, and formatting\n",
        "    ax.set_title(f\"{model_key} Outperforming Probabilities\", fontsize=14)\n",
        "    ax.set_ylabel(\"Probability\", fontsize=12)\n",
        "    ax.set_ylim(0, 1)\n",
        "    ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "    ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Trim x-axis to data range\n",
        "    ax.set_xlim(probability_df[\"Date\"].min(), probability_df[\"Date\"].max())\n",
        "\n",
        "# Final x-axis label and formatting\n",
        "plt.xlabel(\"Date\", fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Factor weight analysis"
      ],
      "metadata": {
        "id": "PY247n9MPyWG"
      },
      "id": "PY247n9MPyWG"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toggle:\n",
        "combine_all_models = False  # Set to True to combine all models into the same charts; False for individual charts per model\n",
        "\n",
        "# Set the date range for viewing.\n",
        "start_date = pd.to_datetime(\"1968-07-30\")\n",
        "end_date   = pd.to_datetime(\"2024-11-30\")\n",
        "\n",
        "# Define static equal weight value.\n",
        "equal_weight = 1 / len(FACTORS)  # e.g., for 5 factors equal_weight = 0.20\n",
        "\n",
        "if combine_all_models:\n",
        "    # Combined charts: One set of subplots (one per factor) for all models.\n",
        "    n_factors = len(FACTORS)\n",
        "    fig, axs = plt.subplots(n_factors, 1, figsize=(12, 4 * n_factors), sharex=False)\n",
        "    if n_factors == 1:\n",
        "        axs = [axs]  # ensure axs is iterable\n",
        "\n",
        "    for i, factor in enumerate(FACTORS):\n",
        "        ax = axs[i]\n",
        "        min_dates = []\n",
        "        max_dates = []\n",
        "\n",
        "        # Loop through each model's results\n",
        "        for model_key, df_model in results_dfs.items():\n",
        "            df_temp = df_model.copy()\n",
        "            df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "            df_temp = df_temp.dropna(subset=[\"Predicted_month\"]).sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "            # Stack predicted probabilities into a DataFrame.\n",
        "            full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "            probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "            probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "            # Filter to desired date range.\n",
        "            mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "            filtered_df = probability_df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "            if filtered_df.empty:\n",
        "                continue\n",
        "\n",
        "            ax.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "                    label=f\"{factor}_{model_key}\", linewdth=0.6)\n",
        "\n",
        "            min_dates.append(filtered_df[\"Date\"].min())\n",
        "            max_dates.append(filtered_df[\"Date\"].max())\n",
        "\n",
        "        # Set x-axis limits to exactly the data span (if any data exist)\n",
        "        if min_dates and max_dates:\n",
        "            ax.set_xlim(min(min_dates), max(max_dates))\n",
        "\n",
        "        # Draw the static equal weight horizontal line.\n",
        "        ax.axhline(equal_weight, color='black', linestyle='--',\n",
        "                   label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "        ax.set_title(f\"{factor} Predicted Probabilities Across Models\", fontsize=14)\n",
        "        ax.set_ylabel(\"Probability\", fontsize=12)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_yticks([0, 0.25, 0.5, 0.75, 1.0])\n",
        "        ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        ax.legend(loc='best', fontsize='small')\n",
        "\n",
        "    plt.xlabel(\"Date\", fontsize=12)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    # Separate charts: Loop over each model and for each factor create individual charts.\n",
        "    for model_key, df_model in results_dfs.items():\n",
        "        for factor in FACTORS:\n",
        "            df_temp = df_model.copy()\n",
        "            df_temp[\"Predicted_month\"] = pd.to_datetime(df_temp[\"Predicted_month\"], errors='coerce')\n",
        "            df_temp = df_temp.dropna(subset=[\"Predicted_month\"]).sort_values(\"Predicted_month\").reset_index(drop=True)\n",
        "\n",
        "            full_probs = np.vstack(df_temp[\"Predicted_Probabilities\"].values)\n",
        "            probability_df = pd.DataFrame(full_probs, columns=FACTORS)\n",
        "            probability_df[\"Date\"] = df_temp[\"Predicted_month\"]\n",
        "\n",
        "            # Filter to desired date range.\n",
        "            mask = (probability_df[\"Date\"] >= start_date) & (probability_df[\"Date\"] <= end_date)\n",
        "            filtered_df = probability_df.loc[mask].reset_index(drop=True)\n",
        "\n",
        "            if filtered_df.empty:\n",
        "                continue\n",
        "\n",
        "            plt.figure(figsize=(12, 4))\n",
        "            plt.plot(filtered_df[\"Date\"], filtered_df[factor],\n",
        "                     label=f\"{factor} Predicted Probability\", color='blue', linewidth=0.6)\n",
        "\n",
        "            plt.axhline(equal_weight, color='black', linestyle='--',\n",
        "                        label=f\"Equal Weight ({equal_weight:.2%})\")\n",
        "\n",
        "            plt.fill_between(filtered_df[\"Date\"],\n",
        "                             filtered_df[factor],\n",
        "                             equal_weight,\n",
        "                             where=(filtered_df[factor] > equal_weight),\n",
        "                             interpolate=True, color='green', alpha=0.3, label='Overweight')\n",
        "            plt.fill_between(filtered_df[\"Date\"],\n",
        "                             filtered_df[factor],\n",
        "                             equal_weight,\n",
        "                             where=(filtered_df[factor] < equal_weight),\n",
        "                             interpolate=True, color='red', alpha=0.3, label='Underweight')\n",
        "\n",
        "            plt.title(f\"{model_key} - Over/Under Weight for {factor}\")\n",
        "            plt.xlabel(\"Date\")\n",
        "            plt.ylabel(\"Probability\")\n",
        "            plt.ylim(0, 1)\n",
        "            # Set x-axis limits to exactly where data exists.\n",
        "            plt.xlim(filtered_df[\"Date\"].min(), filtered_df[\"Date\"].max())\n",
        "            plt.legend(loc='best')\n",
        "            plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "            plt.xticks(rotation=45)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n"
      ],
      "metadata": {
        "id": "5My7U6ny7GAJ"
      },
      "id": "5My7U6ny7GAJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Total outperforming probabilities"
      ],
      "metadata": {
        "id": "6wkhRaZrP6aF"
      },
      "id": "6wkhRaZrP6aF"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 1. Compute average predicted probabilities per model\n",
        "# ----------------------------------------------------\n",
        "avg_probs_dict = {}\n",
        "avg_highest_factor_weight_dict = {}\n",
        "\n",
        "for model_key, df_model in results_dfs.items():\n",
        "    full_probs = np.vstack(df_model[\"Predicted_Probabilities\"].values)\n",
        "\n",
        "    # 1a. Compute the average probabilities across all rows\n",
        "    avg = full_probs.mean(axis=0)\n",
        "    avg_probs_dict[model_key] = pd.Series(avg, index=FACTORS)\n",
        "\n",
        "    # 1b. Compute the average of the highest weight factor\n",
        "    #     (for each time step, pick the max factor weight, then average those)\n",
        "    avg_highest_factor_weight_dict[model_key] = full_probs.max(axis=1).mean()\n",
        "\n",
        "# Create a DataFrame where rows = models, columns = factors\n",
        "avg_probs_df = pd.DataFrame(avg_probs_dict).T\n",
        "avg_probs_df.index.name = \"Model\"\n",
        "avg_probs_df = avg_probs_df.round(4)\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 2. Generate consistent factor colors from stackplot\n",
        "# ----------------------------------------------------\n",
        "# Use a dummy stackplot to extract the assigned factor colors\n",
        "_, ax_dummy = plt.subplots()\n",
        "dummy_data = np.random.rand(10, len(FACTORS))\n",
        "dummy_dates = pd.date_range(\"2000-01-01\", periods=10)\n",
        "stack = ax_dummy.stackplot(dummy_dates, dummy_data.T, labels=FACTORS, alpha=0.8)\n",
        "plt.close()  # We donâ€™t want to display this\n",
        "\n",
        "# Build color map: factor name â†’ RGBA color\n",
        "factor_colors = {factor: poly.get_facecolor()[0] for factor, poly in zip(FACTORS, stack)}\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 3. Display HTML Table of average probabilities\n",
        "# ----------------------------------------------------\n",
        "html_table = avg_probs_df.reset_index().to_html(index=False, classes=\"table table-striped table-bordered\", border=0)\n",
        "display(HTML(\"<h3>Average Outperforming Probabilities by Model</h3>\" + html_table))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 4. Print average of the highest factor weight by model\n",
        "# ----------------------------------------------------\n",
        "display(HTML(\"<h4>Average of the Highest Factor Weight by Model</h4>\"))\n",
        "for model_key, avg_highest in avg_highest_factor_weight_dict.items():\n",
        "    display(HTML(f\"<p><strong>{model_key}:</strong> {avg_highest:.4f}</p>\"))\n",
        "\n",
        "# ----------------------------------------------------\n",
        "# 5. Stacked Bar Chart with Consistent Colors and Labels\n",
        "# ----------------------------------------------------\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "bottom = np.zeros(len(avg_probs_df))\n",
        "x = np.arange(len(avg_probs_df))\n",
        "\n",
        "for factor in FACTORS:\n",
        "    values = avg_probs_df[factor].values\n",
        "    bars = ax.bar(x, values, bottom=bottom,\n",
        "                  label=factor,\n",
        "                  color=factor_colors[factor],\n",
        "                  edgecolor=\"white\",\n",
        "                  linewidth=0.5)\n",
        "\n",
        "    # Centered labels\n",
        "    for bar, val in zip(bars, values):\n",
        "        if val > 0.03:\n",
        "            ax.text(\n",
        "                bar.get_x() + bar.get_width() / 2,\n",
        "                bar.get_y() + bar.get_height() / 2,\n",
        "                f\"{val * 100:.1f}%\",\n",
        "                ha=\"center\", va=\"center\", fontsize=9, color=\"white\"\n",
        "            )\n",
        "\n",
        "    bottom += values\n",
        "\n",
        "# Final touches\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(avg_probs_df.index)\n",
        "ax.set_ylabel(\"Strategy average factor weights\")\n",
        "ax.set_title(\"Strategy average factor weights\")\n",
        "ax.legend(title=\"Factor\", loc=\"upper right\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7GvhTSes34Ci"
      },
      "id": "7GvhTSes34Ci",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Return data"
      ],
      "metadata": {
        "id": "M0sRKlsm6Yja"
      },
      "id": "M0sRKlsm6Yja"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 0: Define column orders based on your requirements\n",
        "# ------------------------------------------------------------------------------\n",
        "# Common columns that are identical across all models.\n",
        "common_cols = ['Predicted_month', 'Mkt', 'RF', 'Mkt-RF', 'Us_standard'] + FACTORS + ['Equal_Weight_Return', 'Actual_Winner']\n",
        "\n",
        "# Model-specific columns that will be renamed.\n",
        "model_specific_cols = ['Allocated_Return', 'Predicted_Winner']\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 1. Build a base common DataFrame from one model's merged results.\n",
        "# ------------------------------------------------------------------------------\n",
        "# Take the first model as the base to extract common columns.\n",
        "base_key, base_df = list(results_dfs.items())[0]\n",
        "base_df = base_df.copy()\n",
        "base_df['Predicted_month'] = pd.to_datetime(base_df['Predicted_month'], errors='coerce')\n",
        "\n",
        "# Merge with df_sorted (the master DataFrame sorted by date) on date.\n",
        "base_df_local = base_df.merge(df_sorted, left_on='Predicted_month', right_on='Date', how='left')\n",
        "common_df = base_df_local[[c for c in common_cols if c in base_df_local.columns]].copy()\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 2. Process each model individually to extract the model-specific columns.\n",
        "# ------------------------------------------------------------------------------\n",
        "# We'll assign a new display name using numbering such that each model gets:\n",
        "# \"ML{number}: {Model Name}\"\n",
        "model_dfs = []         # Will hold one DataFrame per model.\n",
        "new_model_names = []   # To store new model names.\n",
        "for i, (model_key, df_model) in enumerate(results_dfs.items(), 1):\n",
        "    new_model_name = f\"ML{i}: {model_key}\"  # New display name.\n",
        "    new_model_names.append(new_model_name)\n",
        "\n",
        "    df_temp = df_model.copy()\n",
        "    df_temp['Predicted_month'] = pd.to_datetime(df_temp['Predicted_month'], errors='coerce')\n",
        "\n",
        "    # Merge with df_sorted on 'Predicted_month' = 'Date'\n",
        "    df_temp_local = df_temp.merge(df_sorted, left_on='Predicted_month', right_on='Date', how='left')\n",
        "\n",
        "    # Keep only the 'Predicted_month' plus the model-specific columns.\n",
        "    subset_cols = ['Predicted_month'] + [col for col in model_specific_cols if col in df_temp_local.columns]\n",
        "    df_subset = df_temp_local[subset_cols].copy()\n",
        "\n",
        "    # Rename model-specific columns with the new model name.\n",
        "    rename_dict = {}\n",
        "    for col in model_specific_cols:\n",
        "        if col in df_subset.columns:\n",
        "            rename_dict[col] = f\"{new_model_name} {col}\"\n",
        "    df_subset.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "    model_dfs.append(df_subset)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 3. Merge each model-specific DataFrame with the common DataFrame.\n",
        "# ------------------------------------------------------------------------------\n",
        "combined_df = common_df.copy()\n",
        "for df_sub in model_dfs:\n",
        "    combined_df = combined_df.merge(df_sub, on='Predicted_month', how='left')\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 4. Reorder the columns to match the desired order.\n",
        "# ------------------------------------------------------------------------------\n",
        "benchmark_cols = ['Mkt', 'RF', 'Mkt-RF', 'Us_standard']\n",
        "common_order = ['Predicted_month'] + benchmark_cols + FACTORS\n",
        "# Model-specific allocated return columns.\n",
        "allocated_cols = [f\"{name} Allocated_Return\" for name in new_model_names]\n",
        "# Model-specific predicted winner columns.\n",
        "predicted_cols = [f\"{name} Predicted_Winner\" for name in new_model_names]\n",
        "\n",
        "final_order = common_order + ['Equal_Weight_Return'] + allocated_cols + ['Actual_Winner'] + predicted_cols\n",
        "final_order = [col for col in final_order if col in combined_df.columns]\n",
        "\n",
        "combined_df = combined_df[final_order].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "# ==============================================================================\n",
        "# Step 5. Display the final combined results table.\n",
        "# ------------------------------------------------------------------------------\n",
        "display(combined_df)\n",
        "print(\"\\nFirst date in 'Predicted_month':\", pd.to_datetime(combined_df['Predicted_month']).min())\n",
        "print(\"Last date in 'Predicted_month':\", pd.to_datetime(combined_df['Predicted_month']).max())\n"
      ],
      "metadata": {
        "id": "rA982P5EGhj4"
      },
      "id": "rA982P5EGhj4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import textwrap\n",
        "\n",
        "# Define your date range\n",
        "start_date = pd.to_datetime('2000-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "\n",
        "# Create a filtered copy of combined_df (so the original data isn't lost)\n",
        "filtered_df = combined_df.loc[\n",
        "    (combined_df['Predicted_month'] >= start_date) &\n",
        "    (combined_df['Predicted_month'] <= end_date)\n",
        "].copy()\n",
        "\n",
        "# Ensure \"Year\" column exists in filtered_df\n",
        "if 'Year' not in filtered_df.columns:\n",
        "    filtered_df['Year'] = filtered_df['Predicted_month'].dt.year\n",
        "\n",
        "# Identify ML return columns and create a name map to remove \"Allocated_Return\"\n",
        "ml_return_cols = [c for c in filtered_df.columns if 'Allocated_Return' in c]\n",
        "ml_name_map = {ml: ml.replace(\"Allocated_Return\", \"\").strip() for ml in ml_return_cols}\n",
        "\n",
        "# Helper: Compute annual metrics (RF-adjusted)\n",
        "def compute_annual_metrics(returns: pd.Series, rf: pd.Series):\n",
        "    returns = returns.dropna()\n",
        "    if returns.empty:\n",
        "        return np.nan, np.nan, np.nan\n",
        "    rf = rf.reindex(returns.index)\n",
        "    ann_ret = (1 + returns).prod() - 1\n",
        "    ann_rf  = (1 + rf).prod() - 1\n",
        "    ann_ex_ret = ann_ret - ann_rf\n",
        "    ann_vol = (returns - rf).std() * np.sqrt(12)\n",
        "    ann_sharpe = ann_ex_ret / ann_vol if ann_vol else np.nan\n",
        "    return ann_ret, ann_vol, ann_sharpe\n",
        "\n",
        "# Build df_metrics (raw values) and df_excess (excess over Equal Weight)\n",
        "metrics_rows, excess_rows = [], []\n",
        "for year, grp in filtered_df.groupby('Year'):\n",
        "    ew_ret, ew_vol, ew_sharpe = compute_annual_metrics(grp['Equal_Weight_Return'], grp['RF'])\n",
        "    row_m = {'Year': year,\n",
        "             'Equal_Weight Return': ew_ret,\n",
        "             'Equal_Weight Vol':    ew_vol,\n",
        "             'Equal_Weight Sharpe': ew_sharpe}\n",
        "    row_e = {'Year': year}\n",
        "\n",
        "    for ml in ml_return_cols:\n",
        "        ml_short = ml_name_map[ml]\n",
        "        ml_ret, ml_vol, ml_sharpe = compute_annual_metrics(grp[ml], grp['RF'])\n",
        "        row_m[f\"{ml_short} Return\"]  = ml_ret\n",
        "        row_m[f\"{ml_short} Vol\"]     = ml_vol\n",
        "        row_m[f\"{ml_short} Sharpe\"]  = ml_sharpe\n",
        "\n",
        "        row_e[f\"{ml_short} Excess Return\"]  = ml_ret - ew_ret\n",
        "        row_e[f\"{ml_short} Excess Vol\"]     = ml_vol - ew_vol\n",
        "        row_e[f\"{ml_short} Excess Sharpe\"]  = ml_sharpe - ew_sharpe\n",
        "\n",
        "    metrics_rows.append(row_m)\n",
        "    excess_rows.append(row_e)\n",
        "\n",
        "df_metrics = pd.DataFrame(metrics_rows).set_index('Year').sort_index()\n",
        "df_excess  = pd.DataFrame(excess_rows).set_index('Year').sort_index()\n",
        "\n",
        "# Function: Insert newline breaks for column names longer than max_width characters.\n",
        "def wrap_colname(colname, max_width=15):\n",
        "    lines = textwrap.wrap(colname, width=max_width)\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "# Apply wrapping to all column names\n",
        "df_metrics.columns = [wrap_colname(col) for col in df_metrics.columns]\n",
        "df_excess.columns  = [wrap_colname(col) for col in df_excess.columns]\n",
        "\n",
        "# Function: Style dataframe to enable multiline headers and format numbers with a max of 3 decimals.\n",
        "def style_with_wrapping_and_format(df):\n",
        "    styled = df.style.set_table_styles([\n",
        "        {\n",
        "            'selector': 'th',\n",
        "            'props': [\n",
        "                ('white-space', 'pre-wrap'),  # allow multiline\n",
        "                ('word-wrap', 'break-word')   # break long words\n",
        "            ]\n",
        "        }\n",
        "    ]).format(lambda x: f\"{x:.3f}\" if isinstance(x, float) else x)\n",
        "    return styled\n",
        "\n",
        "# Display the raw metrics and excess metrics with formatted output.\n",
        "display(style_with_wrapping_and_format(df_metrics))\n",
        "display(style_with_wrapping_and_format(df_excess))\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# Summary: Count how often each ML strategy \"beats\" Equal Weight.\n",
        "#  - Excess Return is \"better\" if > 0.\n",
        "#  - Excess Volatility is \"better\" if < 0.\n",
        "#  - Excess Sharpe is \"better\" if > 0.\n",
        "#\n",
        "# Average excess metrics are now calculated from all observations.\n",
        "# -----------------------------------------------------------------\n",
        "summary_rows = []\n",
        "total_years = len(df_excess)\n",
        "\n",
        "def w(ml_short, suffix):\n",
        "    return wrap_colname(f\"{ml_short} {suffix}\")\n",
        "\n",
        "for ml in ml_return_cols:\n",
        "    ml_short = ml_name_map[ml]\n",
        "    ret_series    = df_excess[w(ml_short, \"Excess Return\")]\n",
        "    vol_series    = df_excess[w(ml_short, \"Excess Vol\")]\n",
        "    sharpe_series = df_excess[w(ml_short, \"Excess Sharpe\")]\n",
        "\n",
        "    ret_pos_count    = (ret_series > 0).sum()\n",
        "    vol_neg_count    = (vol_series < 0).sum()\n",
        "    sharpe_pos_count = (sharpe_series > 0).sum()\n",
        "\n",
        "    # Average excess metrics now computed over all observations:\n",
        "    avg_ret    = ret_series.mean()\n",
        "    avg_vol    = vol_series.mean()\n",
        "    avg_sharpe = sharpe_series.mean()\n",
        "\n",
        "    summary_rows.append({\n",
        "        \"Strategy\": ml_short,\n",
        "        \"Excess Return (Positive) Count\": f\"{ret_pos_count}/{total_years}\",\n",
        "        \"Avg Excess Return\":   avg_ret,\n",
        "        \"Excess Vol (Negative) Count\":    f\"{vol_neg_count}/{total_years}\",\n",
        "        \"Avg Excess Vol\":      avg_vol,\n",
        "        \"Excess Sharpe (Positive) Count\": f\"{sharpe_pos_count}/{total_years}\",\n",
        "        \"Avg Excess Sharpe\":   avg_sharpe\n",
        "    })\n",
        "\n",
        "summary_df = pd.DataFrame(summary_rows)\n",
        "summary_df.columns = [wrap_colname(col) for col in summary_df.columns]\n",
        "\n",
        "display(style_with_wrapping_and_format(summary_df))\n"
      ],
      "metadata": {
        "id": "j8ieygX5yPei"
      },
      "id": "j8ieygX5yPei",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns table"
      ],
      "metadata": {
        "id": "E3Xi_BH1iwiv"
      },
      "id": "E3Xi_BH1iwiv"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# PRELIMINARY: Use the merged multi-model table (combined_df)\n",
        "# ---------------------------------------------------------------------\n",
        "start_date = pd.to_datetime('2000-01-01')\n",
        "end_date   = pd.to_datetime('2024-12-30')\n",
        "df_filtered = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                            (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Rename benchmark column if present (using the first element of BENCHMARK)\n",
        "rename_dict = {}\n",
        "if BENCHMARK[0] in df_filtered.columns:\n",
        "    rename_dict[BENCHMARK[0]] = 'Benchmark Return'\n",
        "df_filtered.rename(columns=rename_dict, inplace=True)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Remove RF from factors (we exclude it)\n",
        "factors_to_use = [fac for fac in FACTORS if fac.upper() != 'RF']\n",
        "\n",
        "# Define possible benchmark columns (for cumulative returns).\n",
        "possible_bench = [\"Benchmark Return\", \"Mkt\", \"Mkt-RF\", \"Us_standard\"]\n",
        "benchmark_cols = [col for col in possible_bench if col in df_filtered.columns]\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Calculate Equal-Weighted Returns based on factors_to_use.\n",
        "if all(f in df_filtered.columns for f in factors_to_use):\n",
        "    df_filtered['Equal Factor Weight Strategy Return'] = df_filtered[factors_to_use].mean(axis=1)\n",
        "    equal_ret_col_list = ['Equal Factor Weight Strategy Return']\n",
        "else:\n",
        "    equal_ret_col_list = []\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Predicted Winner Weighted Strategy Return:\n",
        "# Use the base model's predicted winner column using new naming.\n",
        "base_model_key = list(results_dfs.keys())[0]\n",
        "base_model_new = f\"ML1: {base_model_key}\"  # First model is ML1.\n",
        "base_model_pred_col = f\"{base_model_new} Predicted_Winner\"\n",
        "if base_model_pred_col in df_filtered.columns:\n",
        "    df_filtered['Predicted_Winner'] = df_filtered[base_model_pred_col]\n",
        "\n",
        "def calc_winner_strategy(row):\n",
        "    pred = row['Predicted_Winner']\n",
        "    if pred in factors_to_use:\n",
        "        other_factors = [f for f in factors_to_use if f != pred]\n",
        "        if other_factors:\n",
        "            return 0.5 * row[pred] + 0.5 * row[other_factors].mean()\n",
        "        else:\n",
        "            return row[pred]\n",
        "    else:\n",
        "        return row[factors_to_use].mean()\n",
        "\n",
        "df_filtered['Predicted Winner Weighted Strategy Return'] = df_filtered.apply(calc_winner_strategy, axis=1)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Compute cumulative returns for each return series.\n",
        "# We'll work on a copy for cumulative computations.\n",
        "cum = df_filtered.copy()\n",
        "\n",
        "# 1. For each model's allocated return (using new names).\n",
        "allocated_cum_cols = []\n",
        "new_model_names = [f\"ML{i}: {model_key}\" for i, model_key in enumerate(results_dfs.keys(), 1)]\n",
        "for name in new_model_names:\n",
        "    col_alloc = f\"{name} Allocated_Return\"\n",
        "    if col_alloc in cum.columns:\n",
        "        new_cum_col = col_alloc.replace(\"Allocated_Return\", \"Cumulative Allocated Return\")\n",
        "        cum[new_cum_col] = (1 + cum[col_alloc]).cumprod() - 1\n",
        "        allocated_cum_cols.append(new_cum_col)\n",
        "\n",
        "# 2. For equal factor weight returns.\n",
        "if 'Equal Factor Weight Strategy Return' in cum.columns:\n",
        "    cum['Equal Factor Weight Cumulative Return'] = (1 + cum['Equal Factor Weight Strategy Return']).cumprod() - 1\n",
        "    equal_cum_cols = ['Equal Factor Weight Cumulative Return']\n",
        "else:\n",
        "    equal_cum_cols = []\n",
        "\n",
        "# 3. For each benchmark column.\n",
        "bench_cum_cols = []\n",
        "# If \"Benchmark Return\" is available, use the actual benchmark name from BENCHMARK[0].\n",
        "if \"Benchmark Return\" in cum.columns:\n",
        "    new_bench_col = f\"{BENCHMARK[0]} Cumulative Return\"\n",
        "    cum[new_bench_col] = (1 + cum[\"Benchmark Return\"]).cumprod() - 1\n",
        "    bench_cum_cols.append(new_bench_col)\n",
        "# Process any other benchmark columns\n",
        "for col in benchmark_cols:\n",
        "    if col != \"Benchmark Return\":\n",
        "        new_col = col + \" Cumulative Return\"\n",
        "        cum[new_col] = (1 + cum[col]).cumprod() - 1\n",
        "        bench_cum_cols.append(new_col)\n",
        "\n",
        "# 4. For Predicted Winner Weighted Strategy Return.\n",
        "if 'Predicted Winner Weighted Strategy Return' in cum.columns:\n",
        "    cum['Predicted Winner Weighted Cumulative Return'] = (1 + cum['Predicted Winner Weighted Strategy Return']).cumprod() - 1\n",
        "\n",
        "# 5. For each factor in factors_to_use: compute cumulative returns.\n",
        "factor_cum_cols = []\n",
        "for fac in factors_to_use:\n",
        "    if fac in cum.columns:\n",
        "        new_name = fac + \" Cumulative\"\n",
        "        cum[new_name] = (1 + cum[fac]).cumprod() - 1\n",
        "        factor_cum_cols.append(new_name)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Build the final cumulative returns table.\n",
        "# Final order:\n",
        "#   a. Common columns: Predicted_month, then benchmark cumulative returns, then factor cumulative returns.\n",
        "#   b. Then Equal Factor Weight Cumulative Return.\n",
        "#   c. Then each model's Cumulative Allocated Return.\n",
        "#   d. Then Predicted Winner Weighted Cumulative Return.\n",
        "# ---------------------------------------------------------------------\n",
        "final_common_order = ['Predicted_month'] + bench_cum_cols + factor_cum_cols\n",
        "final_order = final_common_order + equal_cum_cols + allocated_cum_cols\n",
        "if 'Predicted Winner Weighted Cumulative Return' in cum.columns:\n",
        "    final_order.append('Predicted Winner Weighted Cumulative Return')\n",
        "\n",
        "final_order = [col for col in final_order if col in cum.columns]\n",
        "cumulative_table = cum[final_order].sort_values('Predicted_month').reset_index(drop=True)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Display the Final Cumulative Returns Table.\n",
        "# ---------------------------------------------------------------------\n",
        "print(\"Cumulative Returns Table:\")\n",
        "display(cumulative_table)\n",
        "print(\"\\nFirst date in 'Predicted_month':\", pd.to_datetime(cumulative_table['Predicted_month']).min())\n",
        "print(\"Last date in 'Predicted_month':\", pd.to_datetime(cumulative_table['Predicted_month']).max())\n"
      ],
      "metadata": {
        "id": "wm06N1gjGjVr"
      },
      "id": "wm06N1gjGjVr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Cumulative returns chart"
      ],
      "metadata": {
        "id": "W-uq2G9iPQwD"
      },
      "id": "W-uq2G9iPQwD"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Toggle settings\n",
        "show_50_50_strategy = False  # Toggle for the 50%/50% predicted winner weighted strategy line.\n",
        "show_benchmark = False       # Toggle for showing benchmark cumulative return(s).\n",
        "use_log_scale = False         # Toggle for logarithmic (True) or linear (False) y-axis scale.\n",
        "\n",
        "# Use the cumulative_table built in Cell 2.\n",
        "df_plot = cumulative_table.copy()\n",
        "\n",
        "# Determine the plotting date range.\n",
        "start_date = df_plot['Predicted_month'].min()\n",
        "end_date   = df_plot['Predicted_month'].max()\n",
        "print(f\"Updated plotting range: {start_date} to {end_date}\")\n",
        "print(\"Columns available for plotting:\", df_plot.columns.tolist())\n",
        "\n",
        "if start_date == end_date:\n",
        "    print(\"âš  Warning: The dataset might not have updated properly. Try rerunning the previous cell!\")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.clf()  # Clear the figure\n",
        "\n",
        "# --- Plot each model's cumulative allocated return ---\n",
        "for col in df_plot.columns:\n",
        "    if \"Cumulative Allocated Return\" in col:\n",
        "        plt.plot(df_plot['Predicted_month'], df_plot[col], label=col)\n",
        "\n",
        "# --- Plot equal factor weight cumulative return ---\n",
        "if 'Equal Factor Weight Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Equal Factor Weight Cumulative Return'],\n",
        "             label='Equal Factor Weight Cumulative Return')\n",
        "\n",
        "# --- Plot benchmark cumulative returns, if toggle is set ---\n",
        "if show_benchmark:\n",
        "    if 'Benchmark Cumulative Return' in df_plot.columns:\n",
        "        plt.plot(df_plot['Predicted_month'], df_plot['Benchmark Cumulative Return'],\n",
        "                 label='Benchmark Cumulative Return')\n",
        "    for col in df_plot.columns:\n",
        "        # Only plot columns with \"Mkt\" or \"Us_standard\" that do NOT include \"Mkt-RF\"\n",
        "        if ((\"Mkt\" in col or \"Us_standard\" in col) and\n",
        "            \"Cumulative Return\" in col and \"Mkt-RF\" not in col and col != \"Benchmark Cumulative Return\"):\n",
        "            plt.plot(df_plot['Predicted_month'], df_plot[col], label=col)\n",
        "\n",
        "# --- Plot Predicted Winner Weighted Cumulative Return if available and toggle is on ---\n",
        "if show_50_50_strategy and 'Predicted Winner Weighted Cumulative Return' in df_plot.columns:\n",
        "    plt.plot(df_plot['Predicted_month'], df_plot['Predicted Winner Weighted Cumulative Return'],\n",
        "             label='50%/50% Predicted Winner Strategy')\n",
        "\n",
        "# --- Format and display the chart ---\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Cumulative Returns')\n",
        "plt.title('Cumulative Returns Comparison')\n",
        "plt.xlim(start_date, end_date)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Toggle the y-axis scale\n",
        "if use_log_scale:\n",
        "    plt.yscale('log')\n",
        "else:\n",
        "    plt.yscale('linear')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "wqfmAhDk3KBx"
      },
      "id": "wqfmAhDk3KBx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PERFORMANCE METRICS"
      ],
      "metadata": {
        "id": "-YjFAG1ePN-9"
      },
      "id": "-YjFAG1ePN-9"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
        "\n",
        "def annualized_metrics(monthly_returns):\n",
        "    \"\"\"\n",
        "    Compute the annualized return, volatility, and Sharpe ratio from monthly returns.\n",
        "    Assumes monthly returns.\n",
        "    \"\"\"\n",
        "    # Fill missing values with 0 to avoid issues.\n",
        "    monthly_returns = monthly_returns.fillna(0)\n",
        "    mean_m = monthly_returns.mean()\n",
        "    std_m = monthly_returns.std()\n",
        "    ann_ret = mean_m * 12\n",
        "    ann_vol = std_m * np.sqrt(12)\n",
        "    sharpe = ann_ret / ann_vol if ann_vol != 0 else np.nan\n",
        "    return ann_ret, ann_vol, sharpe\n",
        "\n",
        "def max_drawdown(monthly_returns):\n",
        "    \"\"\"\n",
        "    Compute the maximum drawdown from a series of monthly returns.\n",
        "    \"\"\"\n",
        "    wealth = (1 + monthly_returns.fillna(0)).cumprod()\n",
        "    dd_series = wealth / wealth.cummax() - 1\n",
        "    return dd_series.min()\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Determine the raw return columns from combined_df.\n",
        "# We assume:\n",
        "# â€¢ \"Predicted_month\" is the date column.\n",
        "# â€¢ Raw (monthly) return strategy columns either contain \"Return\"\n",
        "#   (e.g. \"ML1: Random Forest Allocated_Return\")\n",
        "#   OR exactly match BENCHMARK[0] (e.g. \"Mkt\") or \"Benchmark Return\"\n",
        "# We exclude any column that contains \"Cumulative\" as well as non-return columns.\n",
        "# ---------------------------------------------------------------------\n",
        "all_columns = combined_df.columns.tolist()\n",
        "# Include columns that (a) have \"Return\" in them (but not \"Cumulative\"), OR\n",
        "# are exactly equal to BENCHMARK[0] or \"Benchmark Return\"\n",
        "strategy_cols = [\n",
        "    col for col in all_columns\n",
        "    if (\n",
        "         ((\"Return\" in col) or (col == BENCHMARK[0]) or (col == \"Benchmark Return\"))\n",
        "         and (\"Cumulative\" not in col)\n",
        "         and (col not in [\"Actual_Winner\", \"Predicted_month\"])\n",
        "       )\n",
        "]\n",
        "\n",
        "# Display the list of strategy columns for debugging.\n",
        "print(\"Strategy columns used for performance metrics:\")\n",
        "print(strategy_cols)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Loop through each selected strategy column and compute performance metrics.\n",
        "# ---------------------------------------------------------------------\n",
        "metrics_list = []\n",
        "for col in strategy_cols:\n",
        "    monthly_ret_series = combined_df[col]\n",
        "    ann_ret, ann_vol, sharpe = annualized_metrics(monthly_ret_series)\n",
        "    mdd = max_drawdown(monthly_ret_series)\n",
        "\n",
        "    metrics_list.append({\n",
        "         \"Strategy\": col,\n",
        "         \"Annualized Return\": f\"{ann_ret*100:.2f}%\",\n",
        "         \"Annualized Volatility\": f\"{ann_vol*100:.2f}%\",\n",
        "         \"Sharpe Ratio\": f\"{sharpe:.2f}\",\n",
        "         \"Max Drawdown\": f\"{mdd*100:.2f}%\"\n",
        "    })\n",
        "\n",
        "# Create a DataFrame with the performance metrics.\n",
        "metrics_df = pd.DataFrame(metrics_list).sort_values(\"Strategy\").reset_index(drop=True)\n",
        "\n",
        "# Optionally, sort by another metric (e.g., Sharpe Ratio) if desired.\n",
        "# metrics_df = metrics_df.sort_values(\"Sharpe Ratio\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display the performance metrics as an HTML table.\n",
        "display(HTML(metrics_df.to_html(index=False)))\n"
      ],
      "metadata": {
        "id": "1o0tq56K64bL"
      },
      "id": "1o0tq56K64bL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Drawdown chart"
      ],
      "metadata": {
        "id": "YXD3cJKFO_sE"
      },
      "id": "YXD3cJKFO_sE"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as mticker\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 1. TOGGLE OPTIONS\n",
        "# -------------------------------------------------------------------------\n",
        "show_benchmark_drawdown = True           # Toggle benchmark drawdown\n",
        "show_equal_weight_drawdown = True        # Toggle Equal Weight (single) drawdown\n",
        "show_winner_weighted_drawdown = False     # Toggle Winner Weighted drawdown\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 2. COPY cumulative_table (assumed computed previously)\n",
        "# -------------------------------------------------------------------------\n",
        "drawdown_df = cumulative_table.copy()\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 3. CALCULATE DRAWDOWNS USING WEALTH INDEX (Wealth = 1 + Cumulative Return)\n",
        "# -------------------------------------------------------------------------\n",
        "\n",
        "# a) For each ML model's cumulative allocated return column:\n",
        "ml_alloc_cols = [col for col in drawdown_df.columns if \"Cumulative Allocated Return\" in col]\n",
        "for col in ml_alloc_cols:\n",
        "    wealth = 1 + drawdown_df[col]\n",
        "    drawdown_name = col.replace(\"Cumulative Allocated Return\", \"Drawdown\")\n",
        "    drawdown_df[drawdown_name] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# b) For Benchmark:\n",
        "# Use the benchmark name from BENCHMARK[0]\n",
        "benchmark_name = BENCHMARK[0]  # for example, \"Mkt\"\n",
        "benchmark_cum_col = f\"{benchmark_name} Cumulative Return\"\n",
        "if benchmark_cum_col in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[benchmark_cum_col]\n",
        "    drawdown_df[f\"{benchmark_name} Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "elif \"Mkt Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Mkt Cumulative Return\"]\n",
        "    drawdown_df[\"Mkt Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "else:\n",
        "    print(\"WARNING: No benchmark cumulative return column found.\")\n",
        "\n",
        "# c) For Equal Weight (single version):\n",
        "if \"Equal Factor Weight Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Equal Factor Weight Cumulative Return\"]\n",
        "    drawdown_df[\"Equal Weight Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# d) For Predicted Winner Weighted:\n",
        "if \"Predicted Winner Weighted Cumulative Return\" in drawdown_df.columns:\n",
        "    wealth = 1 + drawdown_df[\"Predicted Winner Weighted Cumulative Return\"]\n",
        "    drawdown_df[\"Winner Weighted Drawdown\"] = wealth / wealth.cummax() - 1\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 4. FILTER BY DATE RANGE\n",
        "# -------------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "drawdown_df[\"Predicted_month\"] = pd.to_datetime(drawdown_df[\"Predicted_month\"])\n",
        "plot_df = drawdown_df[(drawdown_df[\"Predicted_month\"] >= start_date) &\n",
        "                      (drawdown_df[\"Predicted_month\"] <= end_date)]\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 5. PLOT THE DRAWDOWNS\n",
        "# -------------------------------------------------------------------------\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.clf()  # Clear any existing figure\n",
        "\n",
        "# a) Plot each ML model's drawdown (those columns that start with \"ML\" and contain \"Drawdown\")\n",
        "for col in plot_df.columns:\n",
        "    if col.startswith(\"ML\") and \"Drawdown\" in col:\n",
        "        plt.plot(plot_df[\"Predicted_month\"], plot_df[col], label=col)\n",
        "\n",
        "# b) Plot Benchmark Drawdown if toggled on\n",
        "if show_benchmark_drawdown:\n",
        "    # Look for the benchmark drawdown column with the actual benchmark name (e.g., \"Mkt Drawdown\")\n",
        "    bench_drawdown_col = f\"{benchmark_name} Drawdown\"\n",
        "    if bench_drawdown_col in plot_df.columns:\n",
        "        plt.plot(plot_df[\"Predicted_month\"], plot_df[bench_drawdown_col], label=bench_drawdown_col)\n",
        "\n",
        "# c) Plot Equal Weight Drawdown if available\n",
        "if show_equal_weight_drawdown and \"Equal Weight Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Equal Weight Drawdown\"], label=\"Equal Weight Drawdown\")\n",
        "\n",
        "# d) Plot Winner Weighted Drawdown if toggled on\n",
        "if show_winner_weighted_drawdown and \"Winner Weighted Drawdown\" in plot_df.columns:\n",
        "    plt.plot(plot_df[\"Predicted_month\"], plot_df[\"Winner Weighted Drawdown\"], label=\"Winner Weighted Drawdown\")\n",
        "\n",
        "# Format the y-axis as percentages.\n",
        "plt.gca().yaxis.set_major_formatter(\n",
        "    mticker.FuncFormatter(lambda val, _: f\"{val*100:.0f}%\")\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# 6. ENSURE THE X-AXIS SPANS EXACTLY THE DEFINED DATE RANGE\n",
        "# -------------------------------------------------------------------------\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Drawdown\")\n",
        "plt.title(\"Drawdowns of Strategies\")\n",
        "plt.xlim(start_date, end_date)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FK-XFzpv4SXo"
      },
      "id": "FK-XFzpv4SXo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Regression"
      ],
      "metadata": {
        "id": "S244JpMyoSVe"
      },
      "id": "S244JpMyoSVe"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# CONFIGURATION\n",
        "# -----------------------------------------------------------------------------\n",
        "#CHATIN MUKAAN TÃ„Ã„ ON TRUE OLI LONG SHORT TAI LONG ONLY ILMEISESTI\n",
        "subtract_rf = True\n",
        "\n",
        "# Define the regression date range\n",
        "reg_start_date = pd.to_datetime('1973-08-01')\n",
        "reg_end_date   = pd.to_datetime('2025-01-01')\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 1. LOAD FAMA-FRENCH DATA\n",
        "# -----------------------------------------------------------------------------\n",
        "xls_file = pd.ExcelFile(\"/content/Gradu/THE_2ND_latest.xlsx\")\n",
        "df_factors = xls_file.parse(\"FF5\", dtype=str)  # read as string to easily clean decimals\n",
        "df_factors[\"Date\"] = pd.to_datetime(df_factors[\"Date\"])\n",
        "\n",
        "# Define which columns to use for the regression\n",
        "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
        "\n",
        "# We also need 'RF' for subtracting from portfolio returns\n",
        "all_ff5_cols = factors + ['RF']  # Mkt-RF, SMB, HML, RMW, CMA, and the risk-free rate (RF)\n",
        "\n",
        "# Convert all factor columns from string to float (assuming the Excel data is in %)\n",
        "for col in all_ff5_cols:\n",
        "    df_factors[col] = (\n",
        "        df_factors[col]\n",
        "        .str.replace(\",\", \".\", regex=False)\n",
        "        .astype(float)\n",
        "        .div(100)  # Convert percent to decimal if your Excel data is e.g. \"0.2\" as 0.2%\n",
        "    )\n",
        "\n",
        "# Sort by date\n",
        "df_factors = df_factors.sort_values(\"Date\").reset_index(drop=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. PREPARE MODEL NAMES / STORAGE\n",
        "# -----------------------------------------------------------------------------\n",
        "# This assumes you already have some dictionary 'results_dfs' that holds your models.\n",
        "# We'll construct multi-model labels from it. Adjust as needed.\n",
        "new_model_names = [f\"ML{i}: {model_key}\" for i, model_key in enumerate(results_dfs.keys(), 1)]\n",
        "\n",
        "# We'll store final regression summary metrics in a list of dicts\n",
        "regression_summary_list = []\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. REGRESSION LOOP FOR EACH MODEL\n",
        "# -----------------------------------------------------------------------------\n",
        "for new_model_name in new_model_names:\n",
        "    # Build the column name for the model's allocated return\n",
        "    model_alloc_col = f\"{new_model_name} Allocated_Return\"\n",
        "    if model_alloc_col not in combined_df.columns:\n",
        "        print(f\"Column '{model_alloc_col}' not found for {new_model_name}. Skipping regression.\")\n",
        "        continue\n",
        "\n",
        "    # 3a. Filter your combined_df to the date range, rename the return column\n",
        "    df_model_reg = combined_df.loc[\n",
        "        (pd.to_datetime(combined_df['Predicted_month']) >= reg_start_date) &\n",
        "        (pd.to_datetime(combined_df['Predicted_month']) <= reg_end_date),\n",
        "        ['Predicted_month', model_alloc_col]\n",
        "    ].copy().sort_values('Predicted_month')\n",
        "    df_model_reg = df_model_reg.rename(columns={model_alloc_col: \"Allocated_Return\"})\n",
        "\n",
        "    # 3b. Merge in the Fama-French factors + RF\n",
        "    df_model_reg['Predicted_month'] = pd.to_datetime(df_model_reg['Predicted_month'])\n",
        "    merged_reg = pd.merge(\n",
        "        df_model_reg,\n",
        "        df_factors[['Date'] + all_ff5_cols],\n",
        "        left_on=\"Predicted_month\",\n",
        "        right_on=\"Date\",\n",
        "        how=\"inner\"\n",
        "    )\n",
        "    merged_reg.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "    # 3c. Subtract RF from portfolio returns if needed\n",
        "    if subtract_rf and 'RF' in merged_reg.columns:\n",
        "        merged_reg['Adj_Allocated_Return'] = merged_reg['Allocated_Return'] - merged_reg['RF']\n",
        "    else:\n",
        "        merged_reg['Adj_Allocated_Return'] = merged_reg['Allocated_Return']\n",
        "\n",
        "    # 3d. Build regression (Y = Adj_Allocated_Return, X = Mkt-RF, SMB, HML, RMW, CMA)\n",
        "    X = merged_reg[factors]  # exclude 'RF' from the regressors\n",
        "    X = sm.add_constant(X)\n",
        "    y = merged_reg['Adj_Allocated_Return']\n",
        "\n",
        "    model_reg = sm.OLS(y, X, missing='drop').fit()\n",
        "\n",
        "    # 3e. Print summary\n",
        "    print(\"\\n=== Regression Results for\", new_model_name, \"vs. FF5 Factors ===\")\n",
        "    print(model_reg.summary())\n",
        "    print(f\"\\nAlpha (Monthly, {new_model_name}): {model_reg.params.get('const', np.nan):.4f}\")\n",
        "    if 'Mkt-RF' in model_reg.params:\n",
        "        print(f\"Market Beta ({new_model_name}): {model_reg.params['Mkt-RF']:.4f}\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "    # 3f. Collect summary stats\n",
        "    regression_summary_list.append({\n",
        "        \"Model\": new_model_name,\n",
        "        \"Alpha\": model_reg.params.get('const', np.nan),\n",
        "        \"Market Beta\": model_reg.params.get('Mkt-RF', np.nan),\n",
        "        \"R-squared\": model_reg.rsquared,\n",
        "        \"Adj. R-squared\": model_reg.rsquared_adj,\n",
        "        \"p-value\": model_reg.f_pvalue\n",
        "    })\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. REGRESSION FOR EQUAL-WEIGHT STRATEGY\n",
        "# -----------------------------------------------------------------------------\n",
        "equal_df = combined_df.loc[\n",
        "    (pd.to_datetime(combined_df['Predicted_month']) >= reg_start_date) &\n",
        "    (pd.to_datetime(combined_df['Predicted_month']) <= reg_end_date),\n",
        "    ['Predicted_month', 'Equal_Weight_Return']\n",
        "].copy().sort_values('Predicted_month')\n",
        "equal_df = equal_df.rename(columns={'Equal_Weight_Return': 'Equal_Weight_Strategy_Return'})\n",
        "\n",
        "equal_df['Predicted_month'] = pd.to_datetime(equal_df['Predicted_month'])\n",
        "equal_merged = pd.merge(\n",
        "    equal_df,\n",
        "    df_factors[['Date'] + all_ff5_cols],  # also include RF for subtracting\n",
        "    left_on=\"Predicted_month\",\n",
        "    right_on=\"Date\",\n",
        "    how=\"inner\"\n",
        ")\n",
        "equal_merged.drop(columns=[\"Date\"], inplace=True)\n",
        "\n",
        "if subtract_rf and 'RF' in equal_merged.columns:\n",
        "    print(\"Equal-Weight Strategy: Adjusting returns by subtracting RF.\")\n",
        "    equal_merged['Adj_Equal_Weight_Return'] = (\n",
        "        equal_merged['Equal_Weight_Strategy_Return'] - equal_merged['RF']\n",
        "    )\n",
        "else:\n",
        "    equal_merged['Adj_Equal_Weight_Return'] = equal_merged['Equal_Weight_Strategy_Return']\n",
        "\n",
        "X_eq = equal_merged[factors]  # Mkt-RF, SMB, HML, RMW, CMA\n",
        "X_eq = sm.add_constant(X_eq)\n",
        "y_eq = equal_merged['Adj_Equal_Weight_Return']\n",
        "\n",
        "model_eq = sm.OLS(y_eq, X_eq, missing='drop').fit()\n",
        "\n",
        "print(\"\\n=== Regression Results for Equal-Weight Strategy vs. FF5 Factors ===\")\n",
        "print(model_eq.summary())\n",
        "print(f\"\\nAlpha (Monthly, Equal-Weight Strategy): {model_eq.params.get('const', np.nan):.4f}\")\n",
        "print(f\"Market Beta (Equal-Weight Strategy):   {model_eq.params.get('Mkt-RF', np.nan):.4f}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. BUILD A SUMMARY TABLE\n",
        "# -----------------------------------------------------------------------------\n",
        "regression_summary_df = pd.DataFrame(regression_summary_list)\n",
        "\n",
        "equal_summary = pd.DataFrame([{\n",
        "    \"Model\": \"Equal-Weight Strategy\",\n",
        "    \"Alpha\": model_eq.params.get(\"const\", np.nan),\n",
        "    \"Market Beta\": model_eq.params.get(\"Mkt-RF\", np.nan),\n",
        "    \"R-squared\": model_eq.rsquared,\n",
        "    \"Adj. R-squared\": model_eq.rsquared_adj,\n",
        "    \"p-value\": model_eq.f_pvalue\n",
        "}])\n",
        "\n",
        "regression_summary_df = pd.concat([regression_summary_df, equal_summary], ignore_index=True)\n",
        "regression_summary_df = regression_summary_df.round(3)\n",
        "\n",
        "print(\"\\nSummary of Regression Results:\")\n",
        "display(regression_summary_df)\n"
      ],
      "metadata": {
        "id": "MOb-rnOzH_4W"
      },
      "id": "MOb-rnOzH_4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "print(\"\\n=== PERFORMANCE METRICS ===\")\n",
        "\n",
        "def annual_sharpe(series):\n",
        "    \"\"\"\n",
        "    Calculate the annual Sharpe ratio from a monthly return Series.\n",
        "    - Annual return = (product(1 + monthly returns)) - 1\n",
        "    - Annual volatility = std(monthly returns) * sqrt(12)\n",
        "    - Sharpe = annual return / annual volatility (assuming risk-free rate = 0)\n",
        "    \"\"\"\n",
        "    annual_return = (1 + series).prod() - 1\n",
        "    annual_vol = series.std() * np.sqrt(12)\n",
        "    if annual_vol == 0:\n",
        "        return np.nan\n",
        "    return annual_return / annual_vol\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 1) Define the date range and filter combined_df accordingly.\n",
        "# ---------------------------------------------------------------------\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "# Filter the merged DataFrame for the desired date range.\n",
        "df_filtered = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                            (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Create a \"Year\" column from Predicted_month.\n",
        "df_filtered[\"Year\"] = df_filtered[\"Predicted_month\"].dt.year\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 2) Identify Strategy Columns for Regression Metrics:\n",
        "#    - ML strategy columns: Expected to have names like \"ML1: Random Forest Allocated_Return\", etc.\n",
        "#    - Benchmark: \"Benchmark Return\"\n",
        "#    - Equal Weight: \"Equal_Weight_Return\"\n",
        "# ---------------------------------------------------------------------\n",
        "ml_strategy_cols = [col for col in df_filtered.columns\n",
        "                    if col.startswith(\"ML\") and \"Allocated_Return\" in col and \"Cumulative\" not in col]\n",
        "\n",
        "bench_col = \"Benchmark Return\" if \"Benchmark Return\" in df_filtered.columns else None\n",
        "eq_col    = \"Equal_Weight_Return\" if \"Equal_Weight_Return\" in df_filtered.columns else None\n",
        "\n",
        "print(\"Strategy columns used for annual Sharpe calculation:\")\n",
        "print(\"ML Strategy Columns:\", ml_strategy_cols)\n",
        "if bench_col:\n",
        "    print(\"Benchmark Column:\", bench_col)\n",
        "if eq_col:\n",
        "    print(\"Equal Weight Column:\", eq_col)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 3) Compute Annual Sharpe Ratios for Each Strategy\n",
        "# ---------------------------------------------------------------------\n",
        "annual_sharpe_dict = {}\n",
        "\n",
        "# Compute for each ML model column.\n",
        "for col in ml_strategy_cols:\n",
        "    annual_sharpe_dict[col] = df_filtered.groupby(\"Year\")[col].apply(annual_sharpe)\n",
        "\n",
        "# Compute for benchmark (if available).\n",
        "if bench_col is not None:\n",
        "    annual_sharpe_dict[bench_col] = df_filtered.groupby(\"Year\")[bench_col].apply(annual_sharpe)\n",
        "\n",
        "# Compute for equal weight (if available).\n",
        "if eq_col is not None:\n",
        "    annual_sharpe_dict[eq_col] = df_filtered.groupby(\"Year\")[eq_col].apply(annual_sharpe)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 4) Combine the Results into One DataFrame and Round to 3 Decimals\n",
        "# ---------------------------------------------------------------------\n",
        "annual_sharpe_table = pd.DataFrame(annual_sharpe_dict)\n",
        "annual_sharpe_table = annual_sharpe_table.round(3)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# 5) Display the Annual Sharpe Ratios as a Neatly Formatted HTML Table\n",
        "# ---------------------------------------------------------------------\n",
        "display(HTML(\"<h3>Annual Sharpe Ratios by Year and Strategy</h3>\" + annual_sharpe_table.to_html(index=True)))\n"
      ],
      "metadata": {
        "id": "E0cAJzowyqZn"
      },
      "id": "E0cAJzowyqZn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# ============================================================================\n",
        "# 1) Prepare Annual Return Data\n",
        "# ============================================================================\n",
        "\n",
        "# Define the date range for annual analysis.\n",
        "start_date = pd.to_datetime(\"2000-01-01\")\n",
        "end_date   = pd.to_datetime(\"2024-12-31\")\n",
        "\n",
        "# Filter the merged DataFrame (combined_df) for the desired date range.\n",
        "# (combined_df comes from your earlier multiâ€‘model merging steps.)\n",
        "df_annual = combined_df[(combined_df['Predicted_month'] >= start_date) &\n",
        "                          (combined_df['Predicted_month'] <= end_date)].copy()\n",
        "\n",
        "# Create a \"Year\" column.\n",
        "df_annual['Year'] = df_annual['Predicted_month'].dt.year\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Identify columns:\n",
        "#   â€¢ ML strategy columns: they contain \"Allocated_Return\" (e.g., \"ML1: Random Forest Allocated_Return\")\n",
        "#   â€¢ Benchmark: assume the column is \"Benchmark Return\" (or use what was defined earlier).\n",
        "#   â€¢ Equal Weight: assume \"Equal_Weight_Return\"\n",
        "#   â€¢ Factor columns: using the global FACTORS (e.g., ['SMB', 'HML', 'CMA', 'RMW'])\n",
        "# ---------------------------------------------------------------------\n",
        "ml_cols = [col for col in df_annual.columns if (\"Allocated_Return\" in col) and (\"Cumulative\" not in col)]\n",
        "benchmark_col = \"Benchmark Return\" if \"Benchmark Return\" in df_annual.columns else None\n",
        "equal_weight_col = \"Equal_Weight_Return\" if \"Equal_Weight_Return\" in df_annual.columns else None\n",
        "factor_cols = [fac for fac in FACTORS if fac in df_annual.columns]\n",
        "\n",
        "print(\"ML Strategy Columns:\", ml_cols)\n",
        "print(\"Benchmark Column:\", benchmark_col)\n",
        "print(\"Equal Weight Column:\", equal_weight_col)\n",
        "print(\"Factor Columns:\", factor_cols)\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Function to compute annual return as the compounded return over the year.\n",
        "# ---------------------------------------------------------------------\n",
        "def annual_return(group, col):\n",
        "    \"\"\"Compound return over the group: product(1 + r) - 1.\"\"\"\n",
        "    return (1 + group[col]).prod() - 1\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Compute annual returns for each strategy.\n",
        "# We'll build a dictionary where keys are strategy names and values are Series indexed by Year.\n",
        "annual_returns = {}\n",
        "\n",
        "# For each ML model column.\n",
        "for col in ml_cols:\n",
        "    annual_returns[col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, col))\n",
        "\n",
        "# For benchmark.\n",
        "if benchmark_col is not None:\n",
        "    annual_returns[benchmark_col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, benchmark_col))\n",
        "\n",
        "# For equal weight strategy.\n",
        "if equal_weight_col is not None:\n",
        "    annual_returns[equal_weight_col] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, equal_weight_col))\n",
        "\n",
        "# For each factor.\n",
        "for fac in factor_cols:\n",
        "    annual_returns[fac] = df_annual.groupby(\"Year\").apply(lambda grp: annual_return(grp, fac))\n",
        "\n",
        "# Combine the computed annual returns into one DataFrame.\n",
        "annual_returns_df = pd.DataFrame(annual_returns)\n",
        "annual_returns_df = annual_returns_df.round(3)\n",
        "\n",
        "# Display the Annual Returns Table.\n",
        "display(HTML(\"<h3>Annual Returns Table</h3>\" + annual_returns_df.to_html()))\n",
        "\n",
        "# ============================================================================\n",
        "# 2) Compute Excess Returns and Plot by Factor\n",
        "# ============================================================================\n",
        "\n",
        "# For each factor, compute excess return for each ML model relative to that factor.\n",
        "# We define excess return as: (ML Annual Return) - (Factor Annual Return)\n",
        "excess_returns = {}\n",
        "for fac in factor_cols:\n",
        "    # Create a DataFrame of excess returns for all ML models for factor 'fac'\n",
        "    excess_df = pd.DataFrame()\n",
        "    for ml in ml_cols:\n",
        "        excess_df[ml] = annual_returns_df[ml] - annual_returns_df[fac]\n",
        "    excess_returns[fac] = excess_df\n",
        "\n",
        "# Now plot excess returns by factor.\n",
        "n_factors = len(factor_cols)\n",
        "if n_factors > 0:\n",
        "    # Create one subplot per factor.\n",
        "    fig, axes = plt.subplots(1, n_factors, figsize=(6 * n_factors, 5), sharex=True)\n",
        "    if n_factors == 1:\n",
        "        axes = [axes]\n",
        "    for i, fac in enumerate(factor_cols):\n",
        "        ax = axes[i]\n",
        "        # Plot a line for each ML model excess return for this factor.\n",
        "        for ml in ml_cols:\n",
        "            ax.plot(excess_returns[fac].index, excess_returns[fac][ml],\n",
        "                    marker='o', label=f'{ml} - {fac}')\n",
        "        ax.set_title(f'Excess Return: ML - {fac}')\n",
        "        ax.set_xlabel(\"Year\")\n",
        "        ax.set_ylabel(\"Excess Return\")\n",
        "        ax.grid(True, linestyle='--', alpha=0.7)\n",
        "        ax.legend(fontsize='small')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# ============================================================================\n",
        "# 3) (Optional) Compute and Plot Annual Returns for Benchmark and Equal Weight\n",
        "# ============================================================================\n",
        "\n",
        "# For convenience, let's display the annual returns for benchmark and equal weight strategies.\n",
        "if benchmark_col is not None:\n",
        "    print(\"\\nAnnual Returns - Benchmark:\")\n",
        "    display(annual_returns_df[[benchmark_col]])\n",
        "if equal_weight_col is not None:\n",
        "    print(\"\\nAnnual Returns - Equal Weight Strategy:\")\n",
        "    display(annual_returns_df[[equal_weight_col]])\n"
      ],
      "metadata": {
        "id": "Lo1wj3nU3ceD"
      },
      "id": "Lo1wj3nU3ceD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Return comparison by period"
      ],
      "metadata": {
        "id": "tC8mxG1Ynm-j"
      },
      "id": "tC8mxG1Ynm-j"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 1) Create a \"Year\" Column in the Merged DataFrame\n",
        "# -------------------------------------------------\n",
        "combined_df['Year'] = pd.to_datetime(combined_df['Predicted_month']).dt.year\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 2) Define a Function to Calculate Annual Return\n",
        "# -------------------------------------------------\n",
        "def annual_return(series):\n",
        "    \"\"\"\n",
        "    Compute the annual compounded return from a monthly return series.\n",
        "    \"\"\"\n",
        "    return (1 + series).prod() - 1\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 3) Compute Annual Returns for Each Strategy:\n",
        "#    a) For each ML model's allocated return.\n",
        "#    b) For Benchmark Return.\n",
        "#    c) For Equal Weight Return.\n",
        "# -------------------------------------------------\n",
        "# a) For ML models: we assume these columns start with \"ML\" and contain \"Allocated_Return\"\n",
        "ml_cols = [col for col in combined_df.columns\n",
        "           if col.startswith(\"ML\") and \"Allocated_Return\" in col and \"Cumulative\" not in col]\n",
        "annual_returns_ml = {col: combined_df.groupby(\"Year\")[col].apply(annual_return) for col in ml_cols}\n",
        "\n",
        "# b) For Benchmark:\n",
        "annual_return_bench = None\n",
        "if \"Benchmark Return\" in combined_df.columns:\n",
        "    annual_return_bench = combined_df.groupby(\"Year\")[\"Benchmark Return\"].apply(annual_return)\n",
        "\n",
        "# c) For Equal Weight:\n",
        "annual_return_eq = None\n",
        "if \"Equal_Weight_Return\" in combined_df.columns:\n",
        "    annual_return_eq = combined_df.groupby(\"Year\")[\"Equal_Weight_Return\"].apply(annual_return)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 4) Compute Annual Returns for Each Factor in FACTORS:\n",
        "# -------------------------------------------------\n",
        "annual_returns_factors = {}\n",
        "for factor in FACTORS:\n",
        "    if factor in combined_df.columns:\n",
        "        annual_returns_factors[factor] = combined_df.groupby(\"Year\")[factor].apply(annual_return)\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 5) Compute Excess Returns for each ML model relative to each factor.\n",
        "#    Excess = (ML Model Annual Return) - (Factor Annual Return)\n",
        "# -------------------------------------------------\n",
        "excess_returns = {}\n",
        "for ml_col, ml_series in annual_returns_ml.items():\n",
        "    df_excess = pd.DataFrame(index=ml_series.index)\n",
        "    for factor, factor_series in annual_returns_factors.items():\n",
        "        df_excess[f\"Excess ({ml_col} - {factor})\"] = ml_series - factor_series\n",
        "    excess_returns[ml_col] = df_excess\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 6) Build a Summary Table of Annual Returns for all Strategies\n",
        "# -------------------------------------------------\n",
        "years = sorted(combined_df['Year'].unique())\n",
        "annual_summary = pd.DataFrame(index=years)\n",
        "\n",
        "# Add ML model returns.\n",
        "for ml_col, series in annual_returns_ml.items():\n",
        "    annual_summary[ml_col] = series\n",
        "\n",
        "# Add benchmark return if available.\n",
        "if annual_return_bench is not None:\n",
        "    annual_summary[\"Benchmark Return\"] = annual_return_bench\n",
        "\n",
        "# Add equal weight return if available.\n",
        "if annual_return_eq is not None:\n",
        "    annual_summary[\"Equal_Weight_Return\"] = annual_return_eq\n",
        "\n",
        "# -------------------------------------------------\n",
        "# 7) Display the Summary Table and (optionally) Excess Returns for the First ML Model\n",
        "# -------------------------------------------------\n",
        "print(\"Annual Returns Summary:\")\n",
        "display(annual_summary.round(3))\n",
        "\n",
        "# Optionally, display excess returns for the first ML model.\n",
        "first_ml_col = ml_cols[0] if ml_cols else None\n",
        "if first_ml_col is not None:\n",
        "    print(f\"\\nExcess Returns for {first_ml_col}:\")\n",
        "    display(excess_returns[first_ml_col].round(3))\n"
      ],
      "metadata": {
        "id": "_KfEgK83EKLT"
      },
      "id": "_KfEgK83EKLT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corr Heat map & regiimi sharpet\n"
      ],
      "metadata": {
        "id": "WnQxAZGgWuKt"
      },
      "id": "WnQxAZGgWuKt"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = xls_file.parse(SHEET_NAME)\n",
        "df = df[[\"Date\"] + FACTORS]\n",
        "\n",
        "# Calculate correlations\n",
        "correlation_matrix = df[FACTORS].corr()\n",
        "\n",
        "# Show regular correlation table\n",
        "print(correlation_matrix)\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
        "plt.title(\"Correlation Heatmap of Factors\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "trQUVO-yLcAS"
      },
      "id": "trQUVO-yLcAS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "VXaJ_itz6opp"
      },
      "id": "VXaJ_itz6opp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import subprocess\n",
        "import pandas as pd\n",
        "\n",
        "# --- 1) Clone or pull your repo ---\n",
        "repo_url  = \"https://github.com/Elkkujou/Gradu.git\"\n",
        "repo_name = \"Gradu\"\n",
        "\n",
        "if os.path.exists(repo_name):\n",
        "    subprocess.run([\"git\", \"-C\", repo_name, \"pull\"], check=True)\n",
        "else:\n",
        "    subprocess.run([\"git\", \"clone\", repo_url], check=True)\n",
        "\n",
        "# --- 2) Load the Excel sheet into data_ff5 ---\n",
        "xlsx_path = os.path.join(repo_name, \"THE_2ND_latest.xlsx\")\n",
        "xls_file  = pd.ExcelFile(xlsx_path)\n",
        "\n",
        "SHEET_NAME  = \"ajodata_FF5\"\n",
        "data_ff5    = xls_file.parse(SHEET_NAME)\n",
        "\n",
        "# --- 3) Parse the date column (assumed to be the first column) ---\n",
        "date_col = data_ff5.columns[0]\n",
        "data_ff5[date_col] = pd.to_datetime(data_ff5[date_col])\n",
        "\n",
        "# --- 4) Compute zâ€‘scores for your four features ---\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "z_cols   = [f + '_z' for f in FEATURES]\n",
        "\n",
        "# Crossâ€‘sample zâ€‘score: (x â€“ mean) / std\n",
        "data_ff5[z_cols] = data_ff5[FEATURES].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# --- 5) Quick check ---\n",
        "print(data_ff5.loc[:, FEATURES + z_cols].head(10))"
      ],
      "metadata": {
        "id": "Nbkz9QHj6yph"
      },
      "id": "Nbkz9QHj6yph",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1) Make sure your Date column is datetime and set as index\n",
        "data_ff5['Date'] = pd.to_datetime(data_ff5.iloc[:, 0])\n",
        "data_ff5.set_index('Date', inplace=True)\n",
        "\n",
        "# 2) Define features and their zâ€‘score column names\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "z_cols   = [f + '_z' for f in FEATURES]\n",
        "\n",
        "# 3) (Reâ€‘)compute zâ€‘scores if you havenâ€™t yet\n",
        "data_ff5[z_cols] = data_ff5[FEATURES].apply(lambda x: (x - x.mean()) / x.std())\n",
        "\n",
        "# 4) Plot each in its own figure\n",
        "for feat, zc in zip(FEATURES, z_cols):\n",
        "    median_val = data_ff5[zc].median()\n",
        "\n",
        "    plt.figure()                          # new figure for each chart\n",
        "    plt.plot(data_ff5.index, data_ff5[zc], label=f'{feat} Zâ€‘Score')\n",
        "    plt.axhline(0,        linewidth=1,   label='Zero')\n",
        "    plt.axhline(median_val, linestyle='--', linewidth=1, label=f'Median = {median_val:.2f}')\n",
        "\n",
        "    plt.title(f'{feat} Zâ€‘Score Over Time')\n",
        "    plt.xlabel('Date')\n",
        "    plt.ylabel('Zâ€‘Score')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BqHRQ1Fk7ujB"
      },
      "id": "BqHRQ1Fk7ujB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Define your features and factors\n",
        "FEATURES = ['CPI%', 'T10YFF', 'CFNAI', 'Cape']\n",
        "ZCOLS    = [f + '_z' for f in FEATURES]\n",
        "FACTORS  = ['SMB', 'HML', 'CMA', 'RMW']\n",
        "\n",
        "# 2) Prepare an empty DataFrame to hold Sharpe ratios\n",
        "cols = []\n",
        "for feat in FEATURES:\n",
        "    cols += [f\"{feat} > 0 SR\", f\"{feat} < 0 SR\"]\n",
        "sharpe_df = pd.DataFrame(index=FACTORS, columns=cols, dtype=float)\n",
        "\n",
        "# 3) Compute annualized Sharpe = âˆš12 * mean(return) / std(return)\n",
        "for feat, zcol in zip(FEATURES, ZCOLS):\n",
        "    for fac in FACTORS:\n",
        "        mask_pos = data_ff5[zcol] > 0\n",
        "        mask_neg = ~mask_pos\n",
        "\n",
        "        r_pos = data_ff5.loc[mask_pos, fac]\n",
        "        r_neg = data_ff5.loc[mask_neg, fac]\n",
        "\n",
        "        # avoid division by zero\n",
        "        sr_pos = np.sqrt(12) * r_pos.mean() / r_pos.std() if r_pos.std() != 0 else np.nan\n",
        "        sr_neg = np.sqrt(12) * r_neg.mean() / r_neg.std() if r_neg.std() != 0 else np.nan\n",
        "\n",
        "        sharpe_df.loc[fac, f\"{feat} > 0 SR\"] = sr_pos\n",
        "        sharpe_df.loc[fac, f\"{feat} < 0 SR\"] = sr_neg\n",
        "\n",
        "# 4) Round and display\n",
        "sharpe_df = sharpe_df.round(3)\n",
        "print(sharpe_df)"
      ],
      "metadata": {
        "id": "2HuD8lXC8Z2X"
      },
      "id": "2HuD8lXC8Z2X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Regime-wise Correlation Heatmaps\n",
        "#\n",
        "# For the selected return columns, compute and plot the correlation matrix\n",
        "# for each market regime as a heatmap.\n",
        "\n",
        "# %%\n",
        "# Use the global FACTORS instead of redefining returns_columns\n",
        "unique_regimes = df[REGIMES_COLUMN].unique()\n",
        "for regime in unique_regimes:\n",
        "    regime_data = df[df[REGIMES_COLUMN] == regime][FACTORS]\n",
        "    corr = regime_data.corr()\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
        "    plt.title(f\"Return Correlation Heatmap - {regime}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "4GbzDKk2FZYH"
      },
      "id": "4GbzDKk2FZYH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Plot Sharpe Ratios by Market Regime\n",
        "#\n",
        "# Compute and visualize Sharpe ratios for selected factors across each regime,\n",
        "# as well as the unconditional (all-data) values, using a bar chart.\n",
        "# The numeric regime codes are converted back to their original names using the regime_mapping,\n",
        "# and then further shortened using regime_short_mapping.\n",
        "\n",
        "# %%\n",
        "# Define factors and regime columns (using global variables if already defined)\n",
        "factors_columns = FACTORS\n",
        "regimes_column = REGIMES_COLUMN   # Assumes REGIMES_COLUMN was defined earlier\n",
        "\n",
        "# Use the previously created regime_short_mapping to convert numeric codes back to short names.\n",
        "# (If a code is not in regime_short_mapping, it will default to \"Regime <code>\")\n",
        "regime_short_names = {reg: regime_short_mapping.get(reg, f\"Regime {reg}\")\n",
        "                      for reg in df[regimes_column].unique()}\n",
        "\n",
        "sharpe_ratios = {\n",
        "    regime_short_names[regime]: (\n",
        "        df[df[regimes_column] == regime][factors_columns].mean() /\n",
        "        df[df[regimes_column] == regime][factors_columns].std()\n",
        "    )\n",
        "    for regime in df[regimes_column].unique()\n",
        "}\n",
        "\n",
        "# Calculate the \"Unconditional\" Sharpe ratios (using all data)\n",
        "sharpe_ratios[\"Unconditional\"] = df[factors_columns].mean() / df[factors_columns].std()\n",
        "\n",
        "# Convert the dictionary to a DataFrame and set column names\n",
        "sharpe_ratios_df = pd.DataFrame(sharpe_ratios).T\n",
        "sharpe_ratios_df.columns = factors_columns\n",
        "\n",
        "# Plot the Sharpe ratios using the same styling as before.\n",
        "plt.figure(figsize=(14, 8))\n",
        "sharpe_ratios_df.plot(\n",
        "    kind=\"bar\",\n",
        "    grid=True,\n",
        "    colormap=\"viridis\",\n",
        "    title=\"Sharpe Ratios by Regime and Unconditional\",\n",
        "    figsize=(14, 8)\n",
        ")\n",
        "plt.ylabel(\"Sharpe Ratio\", fontsize=12)\n",
        "plt.xlabel(\"Market Regimes\", fontsize=12)\n",
        "plt.xticks(rotation=45, fontsize=10)\n",
        "plt.yticks(fontsize=10)\n",
        "plt.legend(title=\"Factors\", fontsize=10, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "GCkBikW6A2o0"
      },
      "id": "GCkBikW6A2o0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Feature importance by period"
      ],
      "metadata": {
        "id": "SwsjcsWyoH2Z"
      },
      "id": "SwsjcsWyoH2Z"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ======= USER-DEFINED DATE RANGE =======\n",
        "# Adjust these dates to view feature importances for a specific period\n",
        "start_date = pd.to_datetime('2020-01-01')\n",
        "end_date   = pd.to_datetime('2022-12-31')\n",
        "\n",
        "# ======= Filter the Data =======\n",
        "# Filter the results_df for the specified date range based on the 'Predicted_month' column\n",
        "filtered_results_df = results_df[\n",
        "    (results_df['Predicted_month'] >= start_date) &\n",
        "    (results_df['Predicted_month'] <= end_date)\n",
        "]\n",
        "\n",
        "# ======= Get Unique Regimes and Feature Count =======\n",
        "existing_regimes = filtered_results_df['Regime'].unique()\n",
        "n_regimes = len(existing_regimes)\n",
        "n_features = len(filtered_results_df['Feature_Importances'].iloc[0])  # Assumes each entry is a vector\n",
        "\n",
        "# ======= Robust Feature Naming =======\n",
        "try:\n",
        "    # Validate if the predefined FEATURES list matches the actual feature count\n",
        "    if len(FEATURES) != n_features:\n",
        "        print(f\"âš ï¸ Warning: FEATURES list length ({len(FEATURES)}) doesn't match model features ({n_features}).\")\n",
        "        print(\"Using auto-generated feature names instead.\")\n",
        "        raise ValueError\n",
        "    feature_names = FEATURES\n",
        "except (NameError, ValueError):\n",
        "    # Generate default feature names if there's a mismatch or if FEATURES is undefined\n",
        "    feature_names = [f'Feature {i+1}' for i in range(n_features)]\n",
        "    print(f\"Using auto-generated feature names for {n_features} features.\")\n",
        "\n",
        "# ======= Compute Overall Average Feature Importances =======\n",
        "overall_avg_fi = np.vstack(filtered_results_df['Feature_Importances'].values).mean(axis=0)\n",
        "\n",
        "# ======= Compute Regime-Specific Average Feature Importances =======\n",
        "regime_avg_fi = {}\n",
        "for regime_name in existing_regimes:\n",
        "    regime_df = filtered_results_df[filtered_results_df['Regime'] == regime_name]\n",
        "    regime_fi_array = np.vstack(regime_df['Feature_Importances'].values)\n",
        "    regime_avg_fi[regime_name] = regime_fi_array.mean(axis=0)\n",
        "\n",
        "# ======= Sort Features by Overall Importance (Descending) =======\n",
        "sorted_idx = overall_avg_fi.argsort()[::-1]\n",
        "sorted_idx = sorted_idx[sorted_idx < len(feature_names)]  # Ensure index bounds\n",
        "sorted_features = [feature_names[i] for i in sorted_idx]\n",
        "\n",
        "# ======= Plotting =======\n",
        "if n_regimes > 1:\n",
        "    total_plots = 1 + n_regimes  # One overall plot plus one for each regime\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, axs = plt.subplots(\n",
        "        total_plots,\n",
        "        1,\n",
        "        figsize=(19.5, total_plots * row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    if total_plots == 1:\n",
        "        axs = [axs]\n",
        "\n",
        "    # --- Overall Feature Importances ---\n",
        "    axs[0].barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    axs[0].set_yticks(np.arange(n_features))\n",
        "    axs[0].set_yticklabels(sorted_features)\n",
        "    axs[0].set_title(\"Overall Average Feature Importances\", pad=12)\n",
        "    axs[0].set_xlabel(\"Average Importance\")\n",
        "    axs[0].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # --- Regime-Specific Feature Importances ---\n",
        "    for idx, (regime_name, avg_fi) in enumerate(regime_avg_fi.items(), start=1):\n",
        "        sorted_regime_fi = avg_fi[sorted_idx]\n",
        "        axs[idx].barh(\n",
        "            np.arange(n_features),\n",
        "            sorted_regime_fi,\n",
        "            color='salmon',\n",
        "            edgecolor='black'\n",
        "        )\n",
        "        axs[idx].set_yticks(np.arange(n_features))\n",
        "        axs[idx].set_yticklabels(sorted_features)\n",
        "        axs[idx].set_title(f\"Feature Importances: {regime_name} Regime\", pad=12)\n",
        "        axs[idx].set_xlabel(\"Average Importance\")\n",
        "        axs[idx].grid(axis='x', linestyle='--', alpha=0.7)\n",
        "else:\n",
        "    # If zero or one regime, show only the overall chart\n",
        "    row_height = max(0.3 * n_features, 4)\n",
        "    fig, ax = plt.subplots(\n",
        "        1, 1, figsize=(19.5, row_height),\n",
        "        gridspec_kw={'hspace': 0.4}\n",
        "    )\n",
        "    ax.barh(\n",
        "        np.arange(n_features),\n",
        "        overall_avg_fi[sorted_idx],\n",
        "        color='steelblue',\n",
        "        edgecolor='black'\n",
        "    )\n",
        "    ax.set_yticks(np.arange(n_features))\n",
        "    ax.set_yticklabels(sorted_features)\n",
        "    ax.set_title(\"Overall Average Feature Importances (No Multiple Regimes)\", pad=12)\n",
        "    ax.set_xlabel(\"Average Importance\")\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "\n",
        "plt.tight_layout(pad=4.0)\n",
        "plt.subplots_adjust(left=0.3)  # Extra space for feature labels\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XgeSIiAu-M2b"
      },
      "id": "XgeSIiAu-M2b",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}